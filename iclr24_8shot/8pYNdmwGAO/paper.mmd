# EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding

Anonymous authors

Paper under double-blind review

###### Abstract

Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the _wild-type_, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNN can capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to \(6.4\%\) better than state-of-the-art methods and attains \(36\times\) inference speedup in comparison with large pre-trained models. The code and models are available at [https://anonymous.4open.science/r/EvolMPNN](https://anonymous.4open.science/r/EvolMPNN).

## 1 Introduction

_Can we predict important properties of a protein by directly observing only the effect of a few mutations on such properties?_ This basic biological question (Wells, 1990; Fowler & Fields, 2014) has recently engaged the machine learning community due to the current availability of benchmark data (Rao et al., 2019; Dallago et al., 2021; Xu et al., 2022). Proteins are sequences of amino-acids (residues), which are the cornerstone of life and influence a number of metabolic processes, including diseases (Pauling et al., 1951; Ideker & Sharan, 2008). For this reason, protein engineering stands at the forefront of modern biotechnology, offering a remarkable toolkit to manipulate and optimise existing proteins for a wide range of applications, from drug development to personalised therapy (Ulmer, 1983; Carter, 2011; Alley et al., 2019).

One fundamental process in protein engineering progressively mutates an initial protein, called the _wild-type_, to study the effect on the protein's properties (Siezen et al., 1991). These mutations form a family of _homologous proteins_ as in Figure 1. This process is appealing due to its cheaper cost compared to other methods and reduced time and risk (Wang et al., 2012; Engqvist & Rabe, 2019).

Yet, the way mutations affect the protein's properties is not completely understood (Bryant et al., 2021; Sarkisyan et al., 2016; Wu et al., 2016), as it depends on a number of chemical reactions and bonds among residues. For this reason, machine learning offers a viable alternative to model complex interactions among residues. Initial approaches employed _feature engineering_ to capture protein's evolution (Saravanan & Gautham, 2015; Feng & Zhang, 2000); yet, a manual approach is expensive and does not offer enough versatility. Advances in NLP and CV inspired the design of deep _protein sequence encoders_(Hochreiter & Schmidhuber, 1997; Yu et al., 2017; Vaswani et al., 2017) and general purpose Protein Language Models (PLMs) that are pre-trained on large scale datasets of sequences. Notable PLMs include ProtBert (Brandes et al., 2022), AlphaFold (Jumper et al., 2021), TAPE Transformer (Rao et al., 2019) and ESM (Rives et al., 2021). These models mainly rely on Multiple Sequence Alignments (MSAs) (Meier et al., 2021) to search on large databases of protein evolution. Nevertheless, this search process is insensitive to subtle yet crucial mutations and introduces additional computational burdens (Pearson, 2013; Chatzou et al., 2016).

To overcome the limitations of previous models, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to predict the mutational effect on homologous proteins. Our fundamental assumption is that there are inherent correlations between protein properties and the sequence differences among them, as shown in Figure 1-(b). EvolMPNN integrates both protein sequence and evolutionary information by identifying where and which mutations occur on the target protein sequence, compared with known protein sequences and predicts the mutational effect on the target protein property. To avoid the costly _quadratic_ pairwise comparison among proteins, we devise a theoretically grounded (see Section 4.6) _linear_ sampling strategy to compute differences only among the proteins and a fixed number of anchor proteins (Section 4.2). We additionally introduce two extensions of our model, EvolGNN and EvolFormer, to include available data on the relation among proteins (Section 4.5). The theoretical computation complexity of proposed methods are provided to guarantee their efficiency and practicality. We apply the proposed methods to three benchmark homologous protein property prediction datasets with nine splits. Empirical evaluation results (Section 5.1) show up to \(6.7\%\) Spearman's \(\rho\) correlation improvement over the best performing baseline models, reducing the inference time by \(36\times\) compared with pre-trained PLMs.

## 2 Preliminary and Problem

In the protein engineering process, we first receive a _set of proteins_\(\mathcal{M}=\{\mathcal{P}_{i}\}_{i=1,2,\ldots,M}\) in which each protein can be associated with a label vector \(\mathbf{Y}_{i}\in\mathbb{R}^{\theta}\) that describes its biomedical properties, _e.g._, fitness, stability, fluorescence, solubility, etc. Each protein \(\mathcal{P}_{i}\) is a linear chain of _amino-acids_\(\mathcal{P}_{i}=\{r_{j}\}_{j=1,2,\ldots,N}\). While a protein sequence folds into specific 3D conformation to perform some biomedical functions, each amino-acid is considered as a _residue_. Residues are connected to one another by peptide bonds and can interact with each other by different chemical bounds (Pauling et al., 1951). In short, the function of a protein is mainly determined by the chemical interactions between residues. Since the 3D structure is missing in benchmark datasets (Rao et al., 2019; Dallago et al., 2021; Xu et al., 2022), we assume no 3D protein information in this paper.

**Homologous Protein Family.** A set of protein sequences (\(\mathcal{M}\)) is a _homologous protein family_ if there exists an ancestral protein \(\mathcal{P}_{\text{WT}}\), called _wild-type_, such that any \(\mathcal{P}_{i}\in\mathcal{M}\) is obtained by mutating \(\mathcal{P}_{\text{WT}}\) through substitution, deletion, insertion and truncation of residues (Ochoterena et al., 2019). As shown in Figure 1-(a), a homologous protein family can be organised together by representing their evolutionary relationships and Figure 1-(b) illustrates the detailed evolutionary patterns.

**Research Problem.** Protein engineering based on homologous proteins is a promising and essential direction for designing novel proteins of desired properties (Huang et al., 2014; Langan et al., 2019). Understanding the relation between protein sequence and property is one essential step. Practically, biologists perform experiments in the lab to label the property \(\hat{\mathbf{Y}}_{\text{TRAIN}}\) of a set of protein \(\mathcal{M}_{\text{TRAIN}}\subset\mathcal{M}\) and the follow-up task is predicting \(\hat{\mathbf{Y}}_{\text{TEST}}\) of the rest proteins \(\mathcal{M}_{\text{TEST}}\subset\mathcal{M}\). However, homologous proteins typically have similarities in their amino-acid sequences, structures, and functions due to their shared ancestry. Accurately predicting the homologous protein property by distinguishing these subtle yet crucial differences is still an open challenge.

Figure 1: Protein property prediction on homologous protein family. (a) An example homologous protein family with labelled nearby mutants with few mutations. The task is to predict the label of unknown mutants with more mutations. (b) The evolutionary pattern for (a); \((p_{1},m_{1})\) indicates the mutation \(m_{1}\) at the \(p_{1}\)-th position of the protein sequence.

## 3 Related Work

**Feature Engineering.** Besides conducting manual experiments in labs to measure protein properties, the basic solution is to design different feature engineering methods based on relevant biological knowledge, to extract useful information from protein sequence (Klein et al., 1985; Feng and Zhang, 2000; Wang et al., 2017). Dallago et al. (2021) introduce using Levenshtein distance (Li and Liu, 2007) and BLOSUM62-score (Eddy, 2004) relative to wild-type to design protein sequence features. In another benchmark work, Xu et al. (2022) adopt another two typical protein sequence feature descriptors, _i.e_., Dipeptide Deviation from Expected Mean (DDE) (Saravanan and Gautham, 2015) and Moran correlation (Moran) (Feng and Zhang, 2000). For more engineering methods, refer to the comprehensive review (Lee et al., 2007).

**Protein Representation Learning.** In the last decades, empowered by the outstanding achievements of machine learning and deep learning, protein representation learning has revolutionised protein property prediction research. Early work along this line adopts the idea of word2vec (Mikolov et al., 2013) to protein sequences (Xu et al., 2018; Mejia-Guerra and Buckler, 2019). To increase model capacity, deeper _protein sequence encoders_ were proposed by the Computer Vision (CV) and Nature Language Processing (NLP) communities (Hochreiter and Schmidhuber, 1997; Yu et al., 2017; Vaswani et al., 2017). The latest works develop _Protein Language Models_ (PLMs), which focus on employing deep sequence encoder models for protein sequences and are pre-trained on million- or billion-scale sequences. Well-known works include ProtBert (Brandes et al., 2022), AlphaFold (Jumper et al., 2021), TAPE Transformer (Rao et al., 2019) and ESM (Rives et al., 2021). However, most existing work does not pay enough attention to these subtle yet crucial differences in homologous proteins. Rives et al. (2021); Jumper et al. (2021) explore protein Multiple Sequence Alignments (MSAs) (Rao et al., 2021a; Meier et al., 2021) to capture the mutational effect. Nevertheless, the MSA searching process introduces additional computational burden and is insensitive to subtle but crucial sequence differences (Pearson, 2013). Chatzou et al. (2016) indicate the shortcomings of MSAs on easily neglecting the presence of minor mutations, which can propagate errors to downstream protein sequence representation learning tasks. This paper also lies in this direction, we propose a novel solution for the challenging homologous protein property prediction tasks.

## 4 Framework

EvolMPNN is a novel framework that integrates both protein sequence information and evolution information by means of residues. As a result, EvolMPNN accurately predicts the mutational effect on homologous protein families. First, in Section 4.1, we introduce _embedding initialisation_ for protein sequence and residues and the update module for residue embedding (Section 4.2). The _evolution encoding_ in Section 4.3 is the cornerstone of the model that ameliorates protein embeddings.

Figure 2: Our EvolMPNN framework encodes protein mutations via a sapient combination of residue evolution and sequence encoding.

We conclude in Section 4.4 with the generation of _final proteins embeddings and model optimisation_. We complement our model with a theoretical analysis to motivate our methodology and a discussion of the computation complexity (Section 4.6). We additionally propose extended versions of EvolMPNN that deal with available protein-protein interactions (Section 4.5).

### Embedding Initialisation

**Protein Sequence Embedding.** Given a _set of proteins_\(\mathcal{M}=\{\mathcal{P}_{i}\}_{i=1,2,\ldots,M}\), we first adopt a (parameter-frozen) PLM model (Rao et al., 2021b; Meier et al., 2021)1 as protein sequence encoder to initialise the protein sequence embedding (\(\mathbf{H}\)) for every protein \(\mathcal{P}_{i}\), which include _macro_ (_i.e._, protein sequence) level information as the primary embedding.

Footnote 1: We do not fine-tune PLM in this paper for efficiency consideration.

\[\mathbf{H}=\text{\sc PlmEncoder}(\{\mathcal{P}_{i}\}_{i=1,2,\ldots,M}), \tag{1}\]

where the obtained protein embedding \(\mathbf{H}\in\mathbb{R}^{M\times d}\) and \(\mathbf{H}_{i}\) corresponds to each protein \(\mathcal{P}_{i}\). Different encoders can extract information on various aspects, however, existing PLM models that rely on MSAs are not sensitive enough to capture the evolution pattern information in homologous protein families (Pearson, 2013). Chatzou et al. (2016) systematically indicate the shortcomings of MSAs on easily neglecting the presence of minor mutations, which can propagate errors to downstream protein sequence representation learning tasks.

**Residue Embedding.** In order to properly capture the evolution information in homologous proteins, we delve into the residue level for _micro_ clues. We adopt two residue embedding initialisation approaches, _i.e._, one-hot encoding (\(\Phi^{\text{OH}}\)) and pre-trained PLM encoder (\(\Phi^{\text{PLM}}\)), to generate protein's initial residue embeddings \(\hat{\mathbf{X}}_{i}=\{\mathbf{x}_{j}^{\mathsf{c}}\}_{j=1,2,\ldots,N}\), where \(\mathbf{x}_{j}^{\mathsf{c}}\in\mathbb{R}^{d}\). In particular, \(\Phi^{\text{OH}}\) assigns each protein residue2 with a binary feature vector \(\mathbf{x}_{j}^{\mathsf{c}}\), where \(\mathbf{x}_{jb}^{\mathsf{c}}=1\) indicates the appearance of the \(b\)-th residue at \(\mathcal{P}_{i}\)'s \(j\)-th position. By stacking \(N\) residues' feature vectors into a matrix, we can obtain \(\mathbf{X}_{i}\in\mathbb{R}^{N\times d}\). On the other hand, following the benchmark implementations (Zhu et al., 2022), PlmEncoder can export residue embeddings similar to Eq. 1. Formally, \(\Phi^{\text{PLM}}\) initialises protein residue embeddings as \(\mathbf{X}_{i}=\text{\sc PlmEncoder}(\{r_{j}\}_{j=1,2,\ldots,N})\).

Footnote 2: There are \(20\) different amino-acid residues commonly found in proteins

**Position Embedding.** Another essential component of existing PLM is the positional encoding, which was first proposed by Vaswani et al. (2017). This positional encoding effectively captures the relative structural information between entities and integrates it with the model (Ying et al., 2021). In our case, correctly recording the position of each residue in the protein sequence plays an essential role in identifying each protein's corresponding mutations. Because the same mutation that occurs at different positions may lead to disparate influences on protein property. Therefore, after initialising residue embeddings, we further apply positional embedding on each protein's residues. We adopt a methodology that reminisices (Ruoss et al., 2023) that demonstrates the paramount importance of assigning each residue with a unique position embedding. As such, we randomly initialise a set of \(d\) position embeddings \(\Phi^{\text{Pos}}\in\mathbb{R}^{N\times d}\). We denote the residue embedding empowered by position embedding as \(\hat{\mathbf{X}}_{i}=\mathbf{X}_{i}\odot\Phi^{\text{Pos}}\).

### Residue Embedding Update

The 3D protein folding depends on the strength of different chemical bonds between residues to maintain a stable 3D structure. Previous studies carefully designed residue contact maps to model the residue-residue interactions to learn effective residue embeddings (Rao et al., 2021b; Gao et al., 2023). In this paper, we adopt the residue-residue interaction to update residue embeddings but eschew the requirement of manually designing the contact map. Instead, we assume the existence of an implicit fully connected residue contact map of each protein \(\mathcal{P}_{i}\) and implement the Transformer model (Vaswani et al., 2017; Wu et al., 2022) to adaptively update residue embeddings. Denote \(\mathbf{R}_{i}^{(\ell)}\) as the input to the \((\ell+1)\)-th layer, with the first \(\mathbf{R}_{i}^{(0)}=\hat{\mathbf{X}}_{i}\) be the input encoding. The \((\ell+1)\)-thlayer of residue embedding update module can be formally defined as follows:

\[\begin{split}\mathbf{Att}_{i}^{h}(\mathbf{R}_{i}^{(\ell)})& =\textsc{Softmax}(\frac{\mathbf{R}_{i}^{(\ell)}\mathbf{W}_{Q}^{\ell,h}( \mathbf{R}_{i}^{(\ell)}\mathbf{W}_{K}^{\ell,h})^{\text{T}}}{\sqrt{d}}),\\ \tilde{\mathbf{R}}_{i}^{(\ell)}&=\mathbf{R}_{i}^{( \ell)}+\sum_{h=1}^{H}\mathbf{Att}_{i}^{h}(\mathbf{R}_{i}^{(\ell)})\mathbf{R}_ {i}^{(\ell)}\mathbf{W}_{V}^{\ell,h}\mathbf{W}_{O}^{\ell,h},\\ \mathbf{R}_{i}^{(\ell+1)}&=\hat{\mathbf{R}}_{i}^{( \ell)}+\text{ELU}(\hat{\mathbf{R}}_{i}^{(\ell)}\mathbf{W}_{1}^{\ell})\mathbf{ W}_{2}^{\ell},\end{split} \tag{2}\]

where \(\mathbf{W}_{O}^{\ell,h}\in\mathbb{R}^{d_{H}\times d}\), \(\mathbf{W}_{Q}^{h,h}\), \(\mathbf{W}_{K}^{h,h}\), \(\mathbf{W}_{V}^{l,h}\in\mathbb{R}^{d\times d_{H}}\), \(\mathbf{W}_{1}^{\ell}\in\mathbb{R}^{d\times r}\), \(\mathbf{W}_{2}^{\ell}\in\mathbb{R}^{d_{d}\times d}\), \(H\) is the number of attention heads, \(d_{H}\) is the dimension of each head, \(d_{t}\) is the dimension of the hidden layer, ELU (Clevert et al., 2015) is an activation function, and \(\mathbf{Att}_{i}^{h}(\mathbf{R}_{i}^{(\ell)})\) refers to as the attention matrix. After each Transformer layer, we add a normalisation layer _i.e._, LayerNorm (Ba et al., 2016), to reduce the over-fitting problem proposed by Vaswani et al. (2017). After stacking \(L_{r}\) layers, we obtain the final residue embeddings as \(\mathbf{R}_{i}=\mathbf{R}_{i}^{(L_{r})}\).

### Evolution Encoding

In homologous protein families, all proteins are mutants derived from a common wild-type protein \(\mathcal{P}_{\text{WT}}\) with different numbers and types of mutations. In this paper, we propose to capture the evolutionary information via the following assumption.

**Assumption 1** (Protein Property Relevance).: _Assume there is a homologous protein family \(\mathcal{M}\) and a function \(\textsc{F}_{\textsc{Diff}}\) can accurately distinguish the mutations on mutant \(\mathcal{P}_{i}\) compared with any \(\mathcal{P}_{j}\) as \(\textsc{F}_{\textsc{Diff}}(\mathcal{P}_{i},\mathcal{P}_{j})\). For any target protein \(\mathcal{P}_{i}\), its property \(\mathbf{Y}_{i}\) can be predicted by considering 1) its sequence information \(\mathcal{P}_{i}\); 2) \(\textsc{F}_{\textsc{Diff}}(\mathcal{P}_{i},\mathcal{P}_{j})\) and the property of \(\mathcal{P}_{j}\), i.e., \(\mathbf{Y}_{j}\). Shortly, we assume there exists a function \(f\) that maps \(\mathbf{Y}_{i}\leftarrow f(\textsc{F}_{\textsc{Diff}}(\mathcal{P}_{i},\mathcal{ P}_{j}),\mathbf{Y}_{j})\)._

Motivated by Assumption 1, we take both protein sequence and the mutants difference \(\textsc{F}_{\textsc{Diff}}(\mathcal{P}_{i},\mathcal{P}_{j})\) to accurately predict the protein property. To encode the protein sequence, we employ established tools described in Section 4.1. Here instead, we describe the evolution encoding to realise the function of \(\textsc{F}_{\textsc{Diff}}(\mathcal{P}_{i},\mathcal{P}_{j})\).

The naive solution to extract evolutionary patterns in a homologous family is constructing a complete phylogenetic tree (Fitch and Margoliash, 1967) based on the mutation distance between each protein pair. Yet, finding the most parsimonious phylogenetic tree is NP-hard (Sankoff, 1975).

To address the aforementioned problems, we propose an _anchor-based protein evolution encoding_ method. Specifically, denote \(\mathbf{H}_{i}^{(\ell)}\) as the input to the \((\ell+1)\)-th block and define \(\mathbf{H}_{i}^{(0)}=\mathbf{H}_{i}\). The evolution localisation encoding of the \((\ell+1)\)-th layer contains the following key components: _(i)_\(k\) anchor protein \(\{\mathcal{P}_{S_{i}}\}_{i=1,2,\ldots,k}\) selection. _(ii)_ Evolutionary information encoding function \(\textsc{F}_{\textsc{Diff}}\) that computes the difference between residues of each protein and those of the anchor protein, and target protein's evolutionary information is generated by summarising the obtained differences:

\[\mathbf{d}_{ij}=\textsc{Combine}(\mathbf{R}_{i}-\mathbf{R}_{S_{j}}), \tag{3}\]

where Combine can be implemented as differentiable operators, such as, Concatenate, Max Pool MEAN Pool and SUM Pool; here we use the MEAN Pool to obtain \(\mathbf{d}_{ij}\in\mathbb{R}^{d}\). _(iii)_ Message computation function \(\textsc{F}_{\textsc{Message}}\) that combines protein sequence feature information of two proteins with their evolutionary differences. We empirically find that the simple element-wise product between \(\mathbf{H}_{j}^{(\ell)}\) and \(\mathbf{d}_{ij}\) attains good results

\[\textsc{F}_{\textsc{Message}}(i,j,\mathbf{H}_{j}^{(\ell)},\mathbf{d}_{ij})= \mathbf{H}_{j}^{(\ell)}\odot\mathbf{d}_{ij}, \tag{4}\]

_(iv)_ Aggregate messages from \(k\) anchors and combine them with protein's embedding as follow:

\[\hat{\mathbf{H}}_{i}^{(\ell)}=\textsc{Combine}(\{\textsc{F}_{\textsc{Message }}(i,j,\mathbf{H}_{j}^{(\ell)},\mathbf{d}_{ij})\}_{j=1,2,\ldots,k}), \tag{5}\]

\[\mathbf{H}_{i}^{(\ell+1)}=\textsc{Concat}(\mathbf{H}_{i}^{(\ell)},\hat{\mathbf{ H}}_{i}^{(\ell)})\mathbf{W}^{\ell}, \tag{6}\]

where \(\mathbf{W}^{\ell}\in\mathbb{R}^{2d\times d}\) transform concatenated vectors to the hidden dimension. After stacking \(L_{p}\) layers, we obtain the final protein sequence embedding \(\mathbf{Z}_{i}^{p}=\mathbf{H}_{i}^{(L_{p})}\).

### Final Embedding and Optimisation

After obtaining protein \(\mathcal{P}_{i}\)'s residue embeddings \(\mathbf{R}_{i}\) and sequence embedding \(\mathbf{Z}_{i}^{\text{P}}\), we summarise its residue embeddings as a vector \(\mathbf{Z}_{i}^{\text{R}}=\text{MEAN Pool}(\mathbf{R}_{i})\). The final protein embedding summarises the protein sequence information and evolution information as the comprehensive embedding \(\mathbf{Z}_{i}=\text{Concat}(\mathbf{Z}_{i}^{\text{P}},\mathbf{Z}_{i}^{\text{ R}})\) and the final prediction is computed as \(\hat{\mathbf{Y}}_{i}=\mathbf{Z}_{i}\mathbf{W}^{\text{Final}}\) where \(\mathbf{W}^{\text{Final}}\in\mathbb{R}^{d\times\theta}\), \(\theta\) is the number of properties to predict. Afterwards, we adopt a simple and common strategy, similar to (Xu et al., 2022), to solve the protein property prediction tasks. Specifically, we adopt the MSELoss (\(\mathcal{L}\)) to measure the correctness of model predictions on training samples against ground truth labels. The objective of learning the target task is to optimise model parameters to minimise the loss \(\mathcal{L}\) on this task. The framework of EvolMPNN is summarised in Algorithm 1 in Appendix A.

### Extensions on Observed Graph

EvolMPNN does not leverage any information from explicit geometry among proteins, where each protein only communicates with randomly sampled anchors (Section 4.3). However, it is often possible to have useful structured data \(G=(\mathcal{M},\mathbf{A})\) that represents the relation between protein-protein by incorporating specific domain knowledge (Zhong et al., 2023).3 Therefore, here we introduce EvolGNN, an extension of EvolMPNN on the possibly observed protein interactions.

Footnote 3: Available contact map describes residue-residue interactions can be easily integrated as relational bias of Transformer (Wu et al., 2022) as we used in Section 4.2.

**EvolGNN.** We compute the evolution information as Eq. 3. The evolution information can be easily integrated into the pipeline of message-passing neural networks, as an additional structural coefficient (Wijesinghe & Wang, 2022):

\[\mathbf{m}_{a}^{(\ell)} =\text{Aggregate}^{\mathcal{N}}(\{\mathbf{A}_{ij},\underbrace{ \mathrm{d}_{ij}}_{\text{Evol. info.}},\mathbf{H}_{j}^{(\ell-1)}\,|\,j\in \mathcal{N}(i)\}),\] \[\mathbf{m}_{i}^{(\ell)} =\text{Aggregate}^{\mathcal{I}}(\{\mathbf{A}_{ij},\underbrace{ \mathrm{d}_{ij}}_{\text{Evol. info.}}\,|\,j\in\mathcal{N}(i)\})\,\mathbf{H}_{i}^ {(\ell-1)}, \tag{7}\] \[\mathbf{H}_{i}^{(\ell)} =\text{Combine}(\mathbf{m}_{a}^{(\ell)},\mathbf{m}_{i}^{(\ell)}),\]

where \(\text{Aggregate}^{\mathcal{N}}(\cdot)\) and \(\text{Aggregate}^{\mathcal{I}}(\cdot)\) are two parameterised functions. \(\mathbf{m}_{a}^{(\ell)}\) is a message aggregated from the neighbours \(\mathcal{N}(i)\) of protein \(\mathcal{P}_{i}\) and their structure (\(\mathbf{A}_{ij}\)) and evolution (\(\mathbf{d}_{ij}\)) coefficients. \(\mathbf{m}_{i}^{(\ell)}\) is an updated message from protein \(\mathcal{P}_{i}\) after performing an element-wise multiplication between \(\text{Aggregate}^{\mathcal{I}}(\cdot)\) and \(\mathbf{H}_{i}^{(\ell-1)}\) to account for structural and evolution effects from its neighbours. After, \(\mathbf{m}_{a}^{(\ell)}\) and \(\mathbf{m}_{i}^{(\ell)}\) are combined together to obtain the update embedding \(\mathbf{H}_{i}^{(\ell)}\).

**EvolFormer.** Another extension relies on pure Transformer structure, which means the evolution information of \(\mathcal{M}\) can be captured by every protein. The evolution information can be integrated into the pipeline of Transformer, as additional information to compute the attention matrix:

\[\mathbf{Att}^{h}(\mathbf{H}^{(\ell)})=\text{Softmax}(\frac{\mathbf{H}^{(\ell) }\mathbf{W}_{Q}^{\ell,h}(\mathbf{H}^{(\ell)}\mathbf{W}_{K}^{\ell,h})^{\text{T }}}{\sqrt{d}}+\underbrace{\text{MEAN Pool}(\{\mathbf{R}_{i}\}_{i=1,2,\dots,M})}_{ \text{Evol. info.}}), \tag{8}\]

Other follow-up information aggregation and feature vector update operations are the same as the basic Transformer pipeline, as described in Eq. 2.

### Theoretical Analysis

**Anchor Selection.** Inspired by (You et al., 2019), we adopt Bourgain's Theorem (Bourgain, 1985) to guide the random anchor number (\(k\)) of the evolution encoding layer. Briefly, support by a constructive proof (Theorem 2 (Linial et al., 1995)) of Bourgain Theorem (Theorem 1), only \(k=O(\log^{2}M)\) anchors are needed to ensure the resulting embeddings are guaranteed to have low distortion (Definition 1), in a given metric space \((\mathcal{M},\mathrm{F}_{\text{DIST}})\). EvolMPNN can be viewed as a generalisation of the embedding method of Theorem 2, where \(\mathrm{F}_{\text{DIST}}(\cdot)\) is generalised via message passing functions(Eq 3-Eq. 6). Therefore, Theorem 2 offers a theoretical guide that \(O(\log^{2}M)\) anchors are needed to guarantee low distortion embedding. Following this principle, EvolMPNN choose \(k=\log^{2}M\) random anchors, denoted as \(\{S_{j}\}_{j=1,2,\ldots,\log^{2}M}\), and we sample each protein in \(\mathcal{M}\) independently with probability \(\frac{1}{2^{j}}\). Detailed discussion and proof refer to Appendix B.

**Complexity Analysis.** The computation costs of EvolMPNN, EvolGNN, and EvolFormer come from residue encoding and evolution encoding, since the protein sequence and residue feature initialisation have no trainable parameters. The residue encoder introduces the complexity of \(O(MN)\) following an efficient implementation of NodeFormer (Wu et al., 2022). In the evolution encoding, EvolMPNN performs communication between each protein and \(\log^{2}M\) anchors, which introduces the complexity of \(O(M\log^{2}M)\); EvolGNN performs communication between each protein and \(K\) neighbours with \(O(KM)\) complexity; EvolFormer performs communication between all protein pairs, which introduces the complexity of \(O(M)\), following the efficient implement, NodeFormer. In the end, we obtain the total computation complexity of EvolMPNN - \(O((N+\log^{2}M)M)\), EvolGNN - \(O((N+K)M)\) and EvolFormer - \(O((N+1)M)\).

## 5 Experimental Study

In this section, we empirically study the performance of EvolMPNN. We validate our model on three benchmark homologous protein family datasets and evaluate the methods on nine data splits to consider comprehensive practical use cases. Our experiments comprise a comprehensive set of state-of-the-art methods from different categories. We additionally demonstrate the effectiveness of two extensions of our model, EvolGNN and EvolFormer, with different input features. We conclude our analysis studying the influence of some hyper-parameters and investigating the performance of EvolMPNN on high mutational mutants.

**Datasets and Splits.** We perform experiments on benchmark datasets of several important protein engineering tasks, including AAV, GB1 and Fluorescence, and generate three splits on each dataset. Data statistics are summarised in Table 1. The split \(\lambda\)-vs-Rest indicates that we train models on wild-type protein and mutants of no more than \(\lambda\) mutations, while the rest are assigned to test. The split Low-vs-High indicates that we train models on sequences with target value scores equal to or below wild-type, while the rest are assigned to test. For more details refer to Appendix C.

**Baselines.** As baseline models, we consider methods in four categories. First, we selected four _feature engineer_ methods, _i.e._, Levenshtein (Dallago et al., 2021), BLOSUM62 (Dallago et al., 2021), DDE (Saravanan & Gautham, 2015) and Moran (Feng & Zhang, 2000). Second, we select four _protein sequence encoder_ models, _i.e._, LSTM (Hochreiter & Schmidhuber, 1997), Transformer (Rao et al., 2019), CNN (Rao et al., 2019) and ResNet (Yu et al., 2017). Third, we select four _pre-trained PLM models_, _i.e._, ProtBert (Elnaggar et al., 2022), ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021) and ESM-2 (Lin et al., 2023). In the end, we select four _GNN-based methods_ which can utilise available graph structure, _i.e._, GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018), GraphTransformer (Shi et al., 2021) and NodeFormer (Wu et al., 2022).

**Implementation.** We follow the PEER benchmark settings4, including train and test pipeline, model optimisation and evaluation method (evaluation is Spearman's \(\rho\) metric), adopted in (Xu et al., 2022) to make sure the comparison fairness. For the baselines, including feature engineer, protein sequence

\begin{table}
\begin{tabular}{l|l|l|l|l|l} \hline \hline
**Landscape** & **Split** & **\# Total** & **\#Train** & **\#Valid** & **\#Test** \\ \hline AAV (Bryant et al., 2021) & 2-vs-Rest (Dallago et al., 2021) & 82,583 & 28,626 & 3,181 & 50,776 \\  & 7-vs-Rest (Dallago et al., 2021) & 82,583 & 63,001 & 7,001 & 12,581 \\  & Low-vs-High (Dallago et al., 2021) & 82,583 & 42,791 & 4,755 & 35,037 \\ \hline GB1 (Wu et al., 2016) & 2-vs-Rest (Dallago et al., 2021) & 8,733 & 381 & 43 & 8,309 \\  & 3-vs-Rest (Dallago et al., 2021) & 8,733 & 2,691 & 299 & 5,743 \\  & Low-vs-High (Dallago et al., 2021) & 8,733 & 4,580 & 509 & 3,644 \\ \hline Fluorescence (Sarisyan et al., 2016) & 2-vs-Rest & 54,025 & 12,712 & 1,413 & 39,900 \\  & 3-vs-Rest (Xu et al., 2022) & 54,025 & 21,446 & 5,362 & 27,217 \\  & Low-vs-High & 54,025 & 44,082 & 4,899 & 5,044 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Datasets splits, and corresponding statistics; if the split comes from a benchmark paper, we report the corresponding citation.

encoder and pre-trained PLM, we adopt the implementation provided by benchmark Torchdrug (Zhu et al., 2022) and the configurations reported in (Xu et al., 2022). For the GNN-based baselines, which require predefined graph structure and protein features, we construct \(K\)-NN graphs (Eppstein et al., 1997), with \(K=\{5,10,15\}\), and report the best performance. As features, we use the trained sequence encoder, which achieves better performance, used also in our method. In addition, we adopt ESM-1b as the residue encoder on GB1 dataset and adopt One-Hot encoding on AAV and Fluorescence datasets to speed up the training process. All experiments are conducted on two NVIDIA GeForce RTX 3090 GPUs with 24 GB memory, and we report the mean performance of three runs with different random seeds. We present more details in Appendix D. Note that we do not report results that take more than 48 hours due to our limited computation resources.

### Effectiveness

**EvolMPNN outperforms all baselines on 8 of 9 splits.** Table 2 summarises performance comparison on AAV, GB1 and Fluorescence datasets. EvolMPNN achieves new state-of-the-art performance on most splits of three datasets, with up to \(6.7\%\) improvements to baseline methods. This result vindicates the effectiveness of our proposed design to capture evolution information for homologous protein property prediction. Notably, GNN-based methods that utilise manually constructed graph structure do not enter top-2 on 9 splits and two Transformer structure models, _i.e._, GraphTransformer and NodeFormer, often outperform such methods. It can be understood since homology graph construction is a challenging biomedical task (Pearson, 2013), the simple \(K\)-NN graph construction is not an effective solution.

**Large-scale PLM models are dominated by simple models.** Surprisingly, we find that smaller models, such as CNN and ResNet, can outperform large ESM variaants pre-trained on million- and billion-scale sequences. For instance, ESM-1v has about 650 million parameters and is pre-trained on around 138 million UniRef90 sequences (Meier et al., 2021). Yet, CNN outperforms ESM-1v on three splits of Fluorescence dataset. This indicates the necessity of designs targeting specifically the crucial homologous protein engineering task.

**Our proposed extension models outperform all baselines on GB1 dataset.** We performed additional experiments on GB1 datasets to investigate the performance of two extended models, _i.e._, EvolGNN and EvolFormer and study the influence of different residue embedding initialisation methods. The results summarised in Table 3 evince that EvolMPNN outperforms the other two variants in three splits, and all our proposed models outperform the best baseline. This result confirms the effectiveness of encoding evolution information for homologous protein property prediction. Besides, the models adopting the PLM encoding \(\Phi^{\text{PLM}}\) achieve better performance than

\begin{table}
\begin{tabular}{l|l|l l l l|l l l|l l} \hline \multirow{2}{*}{**Category**} & \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{**AAV**} & \multicolumn{3}{c|}{**GB1**} & \multicolumn{3}{c}{**Fluorescence**} \\  & & 2-vs-R. & 7-vs-R. & L-vs-H. & 2-vs-R. & 3-vs-R. & L-vs-H. & 2-vs-R. & 7-vs-R. & L-vs-H. \\ \hline \multirow{3}{*}{**Feature**} & Levenshtein & 0.578 & 0.550 & 0.251 & 0.156 & -0.069 & -0.108 & 0.466 & 0.054 & 0.011 \\  & BLOSUM62 & NA & NA & NA & 0.128 & 0.005 & -0.127 & NA & NA & NA \\ \cline{2-10}  & DDE & 0.694\({}^{\dagger}\) & 0.636 & 0.158 & 0.445\({}^{\dagger}\) & 0.816 & 0.306 & 0.690 & 0.638\({}^{\dagger}\) & 0.159 \\  & Moran & 0.437\({}^{\dagger}\) & 0.398 & 0.069\({}^{\dagger}\) & 0.589 & 0.193 & 0.445 & 0.400\({}^{\dagger}\) & 0.046 \\ \hline \multirow{3}{*}{**Protein Seq.**} & LSTM & 0.125\({}^{\dagger}\) & 0.608 & 0.308 & -0.002\({}^{\dagger}\) & -0.002 & -0.007 & 0.256 & 0.494\({}^{\dagger}\) & 0.207 \\  & Transformer & 0.681\({}^{\dagger}\) & 0.748 & 0.304 & 0.271 & 0.877 & 0.474 & 0.250 & 0.643\({}^{\dagger}\) & 0.161 \\  & CNN & 0.746\({}^{\dagger}\) & 0.730 & 0.406 & 0.502\({}^{\dagger}\) & 0.857 & 0.515 & 0.805 & 0.682\({}^{\dagger}\) & 0.249 \\  & ResNet & 0.739\({}^{\dagger}\) & 0.733 & 0.223 & 0.133\({}^{\dagger}\) & 0.542 & 0.396 & 0.594 & 0.636\({}^{\dagger}\) & 0.243 \\ \hline \multirow{3}{*}{**Pre-trained**} & ProHert & 0.794\({}^{\dagger}\) & - & - & 0.634\({}^{\dagger}\) & 0.866 & 0.308 & 0.451 & 0.679\({}^{\dagger}\) & - \\  & ProHert & 0.209\({}^{\dagger}\) & 0.507 & 0.277 & 0.123\({}^{\dagger}\) & 0.619 & 0.164 & 0.403 & 0.339\({}^{\dagger}\) & 0.161 \\  & ESM-1b & 0.821\({}^{\dagger}\) & - & - & 0.704\({}^{\dagger}\) & 0.878 & 0.386 & 0.804 & 0.679\({}^{\dagger}\) & - \\ \cline{2-10}  & ESM-1b & 0.454\({}^{\dagger}\) & 0.573 & 0.241 & 0.337\({}^{\dagger}\) & 0.605 & 0.178 & 0.528 & 0.430\({}^{\dagger}\) & 0.091 \\  & ESM-1b\({}^{\dagger}\) & 0.533 & 0.580 & 0.171 & 0.359 & 0.632 & 0.180 & 0.562 & 0.563 & 0.070 \\  & ESM-2\({}^{\ast}\) & 0.475 & 0.581 & 0.199 & 0.422 & 0.632 & 0.189 & 0.501 & 0.511 & 0.084 \\ \hline \multirow{3}{*}{**GNN-based**} & GCN & 0.824 & 0.730 & 0.361 & 0.745 & 0.865 & 0.466 & 0.755 & 0.677 & 0.198 \\  & GAT & 0.821 & 0.741 & 0.369 & 0.725 & 0.873 & 0.508 & 0.768 & 0.667 & 0.208 \\ \cline{1-1}  & GraphTransf. & 0.827 & 0.749 & 0.389 & 0.753 & 0.876 & 0.548 & 0.780 & 0.678 & 0.231 \\ \cline{1-1}  & NodeFormer & 0.827 & 0.741 & 0.393 & 0.757 & 0.877 & 0.543 & 0.794 & 0.677 & 0.213 \\ \hline
**Ours** & EvolMPNN & **0.835** & **0.757** & **0.433** & **0.768** & **0.881** & **0.584** & **0.809** & **0.684** & 0.228 \\ \hline \end{tabular}
\end{table}
Table 2: Quality in terms Spearman’s \(\rho\) correlation with target value. NA indicates a non-applicable setting. * Used as a feature extractor with pre-trained weights frozen. \(\dagger\) Results reported in (Dallago et al., 2021; Xu et al., 2022). - Can not complete the training process within 48 hours on our devices. Top-2 performances of each split are marked as **bold** and underline.

those using the one-hot encoding \(\Phi^{\text{OH}}\). From this experiment, we conclude that residue information provided by PLM helps to capture protein's evolution information.

### Analysis of Performance

**The performance of EvolMPNN comes from its superior predictions on high mutational mutants.** For the Low-vs-High split of GB1 dataset, we group the test proteins into 4 groups depending on their number of mutations. Next, we compute three models, including EvolMPNN, ESM-1b (fine-tuned PLM model) and CNN (best baseline), prediction performances on each protein group and present the results in Figure 4. EvolMPNN outperforms two baselines in all 4 protein groups. Notably, by demonstrating EvolMPNN's clear edge in groups of no less than 3 mutations, we confirm the generalisation effectiveness from low mutational mutants to high mutational mutants. **As per inference time**, EvolMPNN and CNN require similar inference time (\(\approx 5\) secs), \(36\times\) faster than ESM-1b (\(\approx 3\) mins).

**Influence of hyper-parameter settings on EvolMPNN.** We present in Figure 4 a group of experiments to study the influence of some hyper-parameters on EvolMPNN, including the number of attention heads, embedding dimension and the number of layers of residue encoder and evolution encoder. EvolMPNN exhibits stable performance on different hyper-parameter settings.

## 6 Conclusion and Future Work

We propose Evolution-aware Message Passing Neural Network (EvolMPNN), that integrates both protein sequence information and evolution information by means of residues to predict the mutational effect on homologous proteins. Empirical and theoretical studies show that EvolMPNN and its extended variants (EvolGNN and EvolFormer) achieve outstanding performance on several benchmark datasets while retaining reasonable computation complexity. In future work, we intend to incorporate 3D protein structure information towards general-purpose homologous protein models.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**Split**} \\  & 2-vs-R. & 3-vs-R. & L.-vs-H. \\ \hline Best Baseline & 0.757 & 0.878 & 0.548 \\ \hline EvolMPNN (\(\Phi^{\text{OH}}\)) & 0.766 & 0.877 & 0.553 \\ EvolGNN (\(\Phi^{\text{OH}}\)) & 0.764 & 0.866 & 0.536 \\ EvolFormer (\(\Phi^{\text{OH}}\)) & 0.764 & 0.868 & 0.537 \\ \hline EvolMPNN (\(\Phi^{\text{PLM}}\)) & **0.768** & **0.881** & **0.584** \\ EvolGNN (\(\Phi^{\text{PLM}}\)) & 0.767 & 0.879 & 0.581 \\ EvolFormer (\(\Phi^{\text{PLM}}\)) & 0.766 & 0.879 & 0.575 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on GB1 datasets (metric: Spearman’s \(\rho\)) of our proposed methods, with different residue embeddings. Top-2 performances of each split marked as **bold** and underline.

## References

* Aley et al. (2019) Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church. Unified rational protein engineering with sequence-based deep representation learning. _Nature methods_, 16(12):1315-1322, 2019.
* Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _CoRR_, abs/1607.06450, 2016.
* Barnes et al. (2019) Christopher Barnes, Olivia Scheideler, and David Schaffer. Engineering the aav capsid to evade immune responses. _Current opinion in biotechnology_, 60:99-103, 2019.
* Bourgain (1985) Jean Bourgain. On lipschitz embedding of finite metric spaces in hilbert space. _Israel Journal of Mathematics_, 52:46-52, 1985.
* Brandes et al. (2022) Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: a universal deep-learning model of protein sequence and function. _Bioinformatics_, 38(8):2102-2110, 2022.
* Bryant et al. (2021) Drew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M Church, Lucy J Colwell, and Eric D Kelsic. Deep diversification of an aav capsid protein by machine learning. _Nature Biotechnology_, 39(6):691-696, 2021.
* Buning et al. (2015) Hildegard Buning, Anke Huber, Liang Zhang, Nadja Meumann, and Ulrich Hacker. Engineering the aav capsid to optimize vector-host-interactions. _Current Opinion in Pharmacology_, 24:94-104, 2015.
* Carter (2011) Paul J Carter. Introduction to current and future protein therapeutics: a protein engineering perspective. _Experimental cell research_, 317(9):1261-1269, 2011.
* Chatzou et al. (2016) Maria Chatzou, Cedrik Magis, Jia-Ming Chang, Carsten Kemena, Giovanni Bussotti, Ionas Erb, and Cedric Notredame. Multiple sequence alignment modeling: methods and applications. _Briefings in Bioinformatics_, 17(6):1009-1023, 2016.
* Clevert et al. (2015) Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). _CoRR_, abs/1511.07289, 2015.
* Dallago et al. (2021) Christian Dallago, Jody Mou, Kadina E. Johnston, Bruce J. Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, and Kevin Yang. FLIP: benchmark tasks in fitness landscape inference for proteins. In _Proceedings of the 2021 Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* Eddy (2004) Sean R Eddy. Where did the blosum62 alignment score matrix come from? _Nature Biotechnology_, 22(8):1035-1036, 2004.
* Elnaggar et al. (2022) Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Toward understanding the language of life through self-supervised learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 44(10):7112-7127, 2022.
* Engqvist and Rabe (2019) Martin KM Engqvist and Kersten S Rabe. Applications of protein engineering and directed evolution in plant research. _Plant Physiology_, 179(3):907-917, 2019.
* Eppstein et al. (1997) David Eppstein, Michael S Paterson, and F Frances Yao. On nearest-neighbor graphs. _Discrete & Computational Geometry_, 17:263-282, 1997.
* Feng and Zhang (2000) Zhi-Ping Feng and Chun-Ting Zhang. Prediction of membrane protein types based on the hydrophobic index of amino acids. _Journal of Protein Chemistry_, 19:269-275, 2000.
* Fitch and Margoliash (1967) Walter M Fitch and Emanuel Margoliash. Construction of phylogenetic trees: a method based on mutation distances as estimated from cytochrome c sequences is of general applicability. _Science_, 155(3760):279-284, 1967.
* Fowler and Fields (2014) Douglas M Fowler and Stanley Fields. Deep mutational scanning: a new style of protein science. _Nature methods_, 11(8):801-807, 2014.

* Gao et al. (2023) Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. Hierarchical graph learning for protein-protein interaction. _Nature Communications_, 14(1):1093, 2023.
* Hochreiter & Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural Computation_, 1997.
* Huang et al. (2014) Po-Ssu Huang, Gustav Oberdorfer, Chunfu Xu, Xue Y Pei, Brent L Nannenga, Joseph M Rogers, Frank DiMaio, Tamir Gonen, Ben Luisi, and David Baker. High thermodynamic stability of parametrically designed helical bundles. _science_, 346(6208):481-485, 2014.
* Ideker & Sharan (2008) Trey Ideker and Roded Sharan. Protein networks in disease. _Genome Research_, 18(4):644-652, 2008.
* Jumper et al. (2021) John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Kipf & Welling (2017) Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _Proceedings of the 2017 International Conference on Learning Representations (ICLR)_, 2017.
* Klein et al. (1985) Petr Klein, Minoru Kanehisa, and Charles DeLisi. The detection and classification of membrane-spanning proteins. _Biochimica et Biophysica Acta (BBA)-Biomembranes_, 815(3):468-476, 1985.
* Langan et al. (2019) Robert A Langan, Scott E Boyken, Andrew H Ng, Jennifer A Samson, Galen Dods, Alexandra M Westbrook, Taylor H Nguyen, Marc J Lajoie, Zibo Chen, Stephanie Berger, et al. De novo design of bioactive protein switches. _Nature_, 572(7768):205-210, 2019.
* Lee et al. (2007) David Lee, Oliver Redfern, and Christine Orengo. Predicting protein function from sequence and structure. _Nature Reviews Molecular Cell Biology_, 8(12):995-1005, 2007.
* Li & Liu (2007) Yujian Li and Bi Liu. A normalized levenshtein distance metric. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 29(6):1091-1095, 2007.
* Lin et al. (2023) Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023.
* Linial et al. (1995) Nathan Linial, Eran London, and Yuri Rabinovich. The geometry of graphs and some of its algorithmic applications. _Combinatorica_, 15:215-245, 1995.
* Meier et al. (2021) Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. In _Proceedings of the 2021 Annual Conference on Neural Information Processing Systems (NeurIPS)_, pp. 29287-29303, 2021.
* Mejia-Guerra & Buckler (2019) Maria Katherine Mejia-Guerra and Edward S Buckler. A k-mer grammar analysis to uncover maize regulatory architecture. _BMC Plant Biology_, 19(1):1-17, 2019.
* Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In _Proceedings of the 2013 Annual Conference on Neural Information Processing Systems (NIPS)_, pp. 3111-3119, 2013.
* Ochoterena et al. (2019) Helga Ochoterena, Alexander Vrijdaghs, Erik Smets, and Regine Claassen-Bockhoff. The search for common origin: homology revisited. _Systematic Biology_, 68(5):767-780, 2019.
* Pauling et al. (1951) Linus Pauling, Robert B Corey, and Herman R Branson. The structure of proteins: two hydrogen-bonded helical configurations of the polypeptide chain. _Proceedings of the National Academy of Sciences_, 37(4):205-211, 1951.
* Pearson (2013) William R Pearson. An introduction to sequence similarity ("homology") searching. _Current Protocols in Bioinformatics_, 42(1):3-1, 2013.
* Pearson (2014)* Rao et al. (2019) Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John F. Canny, Pieter Abbeel, and Yun S. Song. Evaluating protein transfer learning with TAPE. In _Proceedings of the 2019 Annual Conference on Neural Information Processing Systems (NeurIPS)_, pp. 9686-9698, 2019.
* Rao et al. (2021a) Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. MSA transformer. In _Proceedings of the 2021 International Conference on Machine Learning (ICML)_, volume 139, pp. 8844-8856. PMLR, 2021a.
* Rao et al. (2021b) Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer protein language models are unsupervised structure learners. In _Proceedings of the 2021 International Conference on Learning Representations (ICLR)_, 2021b.
* Rives et al. (2021) Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _Proceedings of the National Academy of Sciences_, 118(15):e2016239118, 2021.
* Ruoss et al. (2023) Anian Ruoss, Gregoire Deletang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. In _Proceedings of the 2023 Annual Meeting of the Association for Computational Linguistics (ACL)_, pp. 1889-1903. ACL, 2023.
* Sankoff (1975) David Sankoff. Minimal mutation trees of sequences. _SIAM Journal on Applied Mathematics_, 28(1):35-42, 1975.
* Saravanan & Gautham (2015) Vijayakumar Saravanan and Namasivayam Gautham. Harnessing computational biology for exact linear b-cell epitope prediction: a novel amino acid composition-based feature descriptor. _Omics: a journal of integrative biology_, 19(10):648-658, 2015.
* Sarkisyan et al. (2016) Karen S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. _Nature_, 533(7603):397-401, 2016.
* Sauer-Eriksson et al. (1995) A Elisabeth Sauer-Eriksson, Gerard J Kleywegt, Mathias Uhlen, and T Alwyn Jones. Crystal structure of the c2 fragment of streptococcal protein g in complex with the fc domain of human igg. _Structure_, 3(3):265-278, 1995.
* Shi et al. (2021) Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. In _Proceedings of the 2021 International Joint Conferences on Artifical Intelligence (IJCAI)_, pp. 1548-1554, 2021.
* Siezen et al. (1991) Roland J Siezen, William M de Vos, Jack AM Leunissen, and Bauke W Dijkstra. Homology modelling and protein engineering strategy of subtilases, the family of subtilisin-like serine proteinases. _Protein Engineering, Design and Selection_, 4(7):719-737, 1991.
* Sjobring et al. (1991) U Sjobring, L Bjorck, and W Kastern. Streptococcus protein g. gene structure and protein binding properties. _Journal of Biological Chemistry_, 266(1):399-405, 1991.
* Tsien (1998) Roger Y Tsien. The green fluorescent protein. _Annual Review of Biochemistry_, 67(1):509-544, 1998.
* Ulmer (1983) Kevin M Ulmer. Protein engineering. _Science_, 219(4585):666-671, 1983.
* Vandenberghe et al. (2009) LH Vandenberghe, JM Wilson, and G Gao. Tailoring the aav vector capsid for gene therapy. _Gene Therapy_, 16(3):311-319, 2009.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proceedings of the 2017 Annual Conference on Neural Information Processing Systems (NIPS)_, pp. 5998-6008, 2017.

* Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _Proceedings of the 2018 International Conference on Learning Representations (ICLR)_, 2018.
* Wang et al. (2017) Jiawei Wang, Bingjiao Yang, Jerico Revote, Andre Leier, Tatiana T Marquez-Lago, Geoffrey Webb, Jiangning Song, Kuo-Chen Chou, and Trevor Lithgow. Possum: a bioinformatics toolkit for generating numerical sequence feature descriptors based on pssm profiles. _Bioinformatics_, 33(17):2756-2758, 2017.
* Wang et al. (2012) Meng Wang, Tong Si, and Huimin Zhao. Biocatalyst development by directed evolution. _Bioresource Technology_, 115:117-125, 2012.
* Wells (1990) James A Wells. Additivity of mutational effects in proteins. _Biochemistry_, 29(37):8509-8517, 1990.
* Wijesinghe & Wang (2022) Asiri Wijesinghe and Qing Wang. A new perspective on "how graph neural networks go beyond weisfeiler-lehman?". In _Proceedings of the 2022 International Conference on Learning Representations (ICLR)_, 2022.
* Wu et al. (2016) Nicholas C Wu, Lei Dai, C Anders Olson, James O Lloyd-Smith, and Ren Sun. Adaptation in protein fitness landscapes is facilitated by indirect paths. _Elife_, 5:e16965, 2016.
* Wu et al. (2022) Qitian Wu, Wentao Zhao, Zenan Li, David P. Wipf, and Junchi Yan. Nodeformer: A scalable graph structure learning transformer for node classification. In _Proceedings of the 2022 Annual Conference on Neural Information Processing Systems (NeurIPS)_, pp. 27387-27401, 2022.
* Xu et al. (2022) Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Chang Ma, Runcheng Liu, and Jian Tang. PEER: A comprehensive and multi-task benchmark for protein sequence understanding. In _Proceedings of the 2022 Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* Xu et al. (2018) Ying Xu, Jiangning Song, Campbell Wilson, and James C Whisstock. Phoscontext2vec: a distributed representation of residue-level sequence contexts and its application to general and kinase-specific phosphorylation site prediction. _Scientific Reports_, 8(1):8240, 2018.
* Ying et al. (2021) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Graph random neural networks for semi-supervised learning on graphs. In _Proceedings of the 2021 Annual Conference on Neural Information Processing Systems (NeurIPS)_, pp. 28877-28888, 2021.
* You et al. (2019) Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In _Proceedings of the 2019 International Conference on Machine Learning (ICML)_, volume 97, pp. 7134-7143. PMLR, 2019.
* Yu et al. (2017) Fisher Yu, Vladlen Koltun, and Thomas A. Funkhouser. Dilated residual networks. In _Proceedings of the 2017 Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 636-644. IEEE, 2017.
* Zhong et al. (2023) Zhiqiang Zhong, Anastasia Barkova, and Davide Mottin. Knowledge-augmented graph machine learning for drug discovery: A survey from precision to interpretability. _CoRR_, abs/2302.08261, 2023.
* Zhu et al. (2022) Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, Chang Ma, Runcheng Liu, Louis-Pascal A. C. Xhonneux, Meng Qu, and Jian Tang. Torchdrug: A powerful and flexible machine learning platform for drug discovery. _CoRR_, abs/2202.08320, 2022.

## Appendix A Algorithm

```
0: Protein set \(\mathcal{M}=\{\mathcal{P}_{i}\}_{i=1,2,\ldots,M}\) and each protein sequence \(\mathcal{P}_{i}\) contains a residue set \(\{r_{j}\}_{j=1,2,\ldots,N}\); Message computation function \(\textsc{F}_{\textsc{Message}}\) that outputs an \(d\) dimensional message; Combine\((\cdot)\) and Concat\((\cdot)\) operators. Output: Protein embeddings \(\{\mathbf{Z}_{i}\}_{i=1,2,\ldots,M}\)
1\(\mathbf{H}_{i}\leftarrow\textsc{PlmCodec}(\mathcal{P}_{i})\)\(\mathbf{X}_{i}\leftarrow\Phi^{\textsc{OH}}(\{r_{j}\}_{j=1,2,\ldots,N})\)\(\Phi^{\textsc{PLM}}(\{r_{j}\}_{j=1,2,\ldots,N})\)\(\hat{\mathbf{X}}_{i}\leftarrow\mathbf{X}_{i}\odot\Phi^{\textsc{Pos}}\)\(\mathbf{R}_{i}^{(0)}\leftarrow\hat{\mathbf{X}}_{i}\)for\(\ell=1,2,\ldots,L_{r}\)do
2fori=1,2,\(\ldots\),Ndo
3\(\mathbf{R}_{i}^{(\ell)}\leftarrow\textsc{NodeFormer}(\mathbf{R}_{i}^{(\ell-1)})\)
4 end for
5
6 end for
7\(\mathbf{R}_{i}\leftarrow\mathbf{R}_{i}^{(L_{r})}\)\(\mathbf{H}_{i}^{(0)}\leftarrow\mathbf{H}_{i}\)for\(\ell=1,2,\ldots,L_{p}\)do
8\(\{S_{j}\}_{j=1,2,\ldots,k}\sim\mathcal{M}\)for\(i=1,2,\ldots,M\)do
9forj=1,2,\(\ldots\),kdo
10\(\mathbf{d}_{ij}=\textsc{Combine}(\mathbf{R}_{i}-\mathbf{R}_{S_{j}})\)
11 end for
12\(\hat{\mathbf{H}}_{i}^{(\ell)}=\textsc{Combine}(\{\textsc{F}_{\textsc{Message}}(i,j, \mathbf{H}_{j}^{(\ell)},\mathbf{d}_{ij})\}_{j=1,2,\ldots,k})\)\(\mathbf{H}_{i}^{(\ell+1)}=\textsc{Concat}(\mathbf{H}_{i}^{(\ell)},\hat{\mathbf{H}}_{i}^{( \ell)})\mathbf{W}^{\ell}\)
13 end for
14
15 end for
16\(\mathbf{Z}_{i}^{\textsc{PL}}=\mathbf{H}_{i}^{(L_{p})}\)\(\mathbf{Z}_{i}^{\textsc{R}}=\textsc{MEAN Pool}(\mathbf{R}_{i})\)\(\mathbf{Z}_{i}=\textsc{Concat}(\mathbf{Z}_{i}^{\textsc{P}},\mathbf{Z}_{i}^{ \textsc{R}})\)
```

**Algorithm 1**The framework of EvolMPNN

We summarise the process of Evolution-aware Message Passing Neural Network (EvolMPNN) in Algorithm 1. Given a protein set \(\mathcal{M}=\{\mathcal{P}_{i}\}_{i=1,2,\ldots,M}\) and each protein sequence \(\mathcal{P}_{i}\) contains a residue set \(\{r_{j}\}_{j=1,2,\ldots,N}\). For each protein \(\mathcal{P}_{i}\), we first initialise protein sequence (\(\mathbf{H}_{i}\)) and residue embeddings (\(\mathbf{X}_{i}\)) (line 1-2). After, the residue embeddings are empowered with positional encoding (\(\Phi_{\textsc{Pos}}\)) to get \(\hat{\mathbf{X}}_{i}\) (line 3). Such a design will help us to record the position of a mutation occurring in the protein sequence in the following steps. Then, we update residue embedding based on a contact map, which records the chemical reactions between residues after folding into a 3D structure (line 4-10). Furthermore, we aggregate evolution-aware embeddings by means of updated residue embeddings (line 11- line 17) and integrate them with protein sequence embeddings to empower them with evolution-aware semantics (line 18-21). Finally, we merge protein sequence and residue embeddings as the final protein embeddings, which contain comprehensive information and make predictions based on them (line 22- line 23).

## Appendix B Theoretical Analysis

Inspired by (You et al., 2019), we adopt Bourgain's Theorem (Bourgain, 1985) to guide the random anchor number (\(k\)) of the evolution encoding layer, such that the resulting embeddings are guaranteed to have low distortion. Specifically, distortion measures the faithfulness of the embeddings in preserving distances (in our case, is the differences between protein sequences on a homology network) when mapping from one metric space to another metric space, which can be defined as:

**Definition 1** (Distortion).: _Given two metric space \((\mathcal{M},\mathrm{F}_{\textsc{Dist}})\) and \((\mathcal{Z},\mathrm{F}^{\prime}_{\textsc{Dist}})\) and a function \(f:\mathcal{M}\to\mathcal{Z}\), \(f\) is said to have distortion \(\alpha\), if \(\forall\mathcal{P}_{i},\mathcal{P}_{j}\in\mathcal{M}\), \(\frac{1}{\alpha}\mathrm{F}_{\textsc{Dist}}(\mathcal{P}_{i},\mathcal{P}_{j}) \leq\mathrm{F}^{\prime}_{\textsc{Dist}}(f(\mathcal{P}_{i}),f(\mathcal{P}_{j}))\leq \mathrm{F}_{\textsc{Dist}}(\mathcal{P}_{i},\mathcal{P}_{j})\)._

**Theorem 1** (Bourgain Theorem).: _Given any finite metric space \((\mathcal{M},\mathrm{F}_{\textsc{Dist}})\), with \(|\mathcal{M}|=M\), there exists an embedding of \((\mathcal{M},\mathrm{F}_{\textsc{Dist}})\) into \(\mathbb{R}^{k}\) under any \(l_{p}\) metric, where \(k=O(\log^{2}M)\), and the distortion of the embedding is \(O(\log M)\)._

Theorem 1 states the Bourgain Theorem (Bourgain, 1985), which shows the existence of a low distortion embedding that maps from any metric space to the \(l_{p}\) metric space.

**Theorem 2** (Constructive Proof of Bourgain Theorem).: _For metric space \((\mathcal{M},\mathrm{F}_{\textsc{Dist}})\), given \(k=\log^{2}M\) random sets \(\{S_{j}\}_{j=1,2,\ldots,\log^{2}M}\subset\mathcal{M}\), \(S_{j}\) is chosen by including each point in \(\mathcal{M}\) independently with probability \(\frac{1}{2^{j}}\). An embedding method for \(\mathcal{P}_{i}\in\mathcal{M}\) is defined as:_

\[f(\mathcal{P}_{i})=(\frac{\mathrm{F}_{\textsc{Dist}}(\mathcal{P}_{i},S_{1})}{k },\frac{\mathrm{F}_{\textsc{Dist}}(\mathcal{P}_{i},S_{2})}{k},\ldots,\frac{ \mathrm{F}_{\textsc{Dist}}(\mathcal{P}_{i},S_{\log^{2}M})}{k}), \tag{9}\]

_Then, \(f\) is an embedding method that satisfies Theorem 1._

**Anchor Selection.**EvolMPNN can be viewed as a generalisation of the embedding method of Theorem 2 (Linial et al., 1995), where \(\mathrm{F}_{\textsc{Dist}}(\cdot)\) is generalised via message passing functions (Eq 3-Eq. 6). Therefore, Theorem 2 offers a theoretical guide that \(O(\log^{2}M)\) anchors are needed to guarantee low distortion embedding. Following this principle, EvolMPNN choose \(k=\log^{2}M\) random anchors, denoted as \(\{S_{j}\}_{j=1,2,\ldots,\log^{2}M}\), and we sample each protein in \(\mathcal{M}\) independently with probability \(\frac{1}{2^{j}}\).

## Appendix C Datasets

Adeno-associated virus (AAV) capsid proteins are responsible for helping the virus carrying viral DNA into a target cell (Vandenberghe et al., 2009); there is great interest in engineering versions of these proteins for gene therapy (Bryant et al., 2021; Buning et al., 2015; Barnes et al., 2019). (Bryant et al., 2021) produces mutants on a 28 amino-acid window from position 561 to 588 of VP-1 and measures the fitness of resulting variants with between 1 and 39 mutations. We adopt three splits from the benchmark (Dallago et al., 2021), including 2-vs-Rest, 7-vs-Rest and Low-vs-High.

GB1 is the binding domain of protein G, an immunoglobulin binding protein found in Streptococcal bacteria (Sauer-Eriksson et al., 1995; Sjobring et al., 1991). Wu et al. (2016) measure the fitness of generated mutations. We adopt three splits from the benchmark (Dallago et al., 2021), including 2-vs-Rest, 3-vs-Rest and Low-vs-High.

The green fluorescent protein is an important marker protein, enabling scientists to see the presence of the particular protein in an organic structure by its green fluorescence (Tsien, 1998). Sarkisyan et al. (2016) assess the fitness of green fluorescent protein mutants. We adopt one available split, 3-vs-Rest, from the benchmark (Xu et al., 2022). Besides, in order to evaluate the models' effectiveness, we add two splits, 2-vs-Rest and Low-vs-High, in this paper.

## Appendix D Baselines

We present details about our baseline and proposed models in Table 4.

\begin{table}
\begin{tabular}{l|l|l|l|l} \hline \hline
**Method** & **Description** & **Encoder** & **Pooling** & **Output layer** \\ \hline \multirow{2}{*}{Levenshtein} & Levenshtein distance to wild & - & - & - \\ \cline{2-4}  & -type. & - & - & - \\ \hline \multirow{2}{*}{BLOSUM62} & BLOSUM62-score relative & - & - & - \\ \cline{2-4}  & peptide Deviation from & 2-layer MLP & - & - \\ \cline{2-4}  & \multirow{2}{*}{Moran} & \multirow{2}{*}{Moran correlation} & 2-layer MLP & - & - \\ \cline{2-4}  & & & - & - \\ \hline LSTM & Simple LSTM model & 3-layer LSTM & Weighted sum pool & 2-layer MLP \\ \hline \multirow{2}{*}{Transformer} & Simple Transformer model & 4-layer Transformer, & 4-attention heads & - & 2-layer MLP \\ \hline \multirow{2}{*}{CNN} & Simple convolutional model & 2-layer CNN & Max pool & 2-layer MLP \\ \hline \multirow{2}{*}{ResNet} & Classic framework of skip connections and residual blocks & 8-layer ResNet & Attentive weighted sum & 2-layer MLP \\ \hline \multirow{2}{*}{ProtBert} & 750M param transformer & 30-layer BERT & \multirow{2}{*}{Linear pool} & 2-layer MLP \\  & pre-trained on Uniter 50 & 16 attention heads & & 2-layer MLP \\ \hline \multirow{2}{*}{ESM-1b} & 650M param transformer & \multirow{2}{*}{-} & Mean pool & 2-layer MLP \\ \cline{2-4}  & pre-trained on Uniter 50 & & & 2-layer MLP \\ \hline \multirow{2}{*}{ESM-2} & 3B param transformer & \multirow{2}{*}{-} & Mean pool & 2-layer MLP \\  & pre-trained on Uniter 50 & & & 2-layer MLP \\ \hline GCN & Graph convolutional network & 2/3-layer GCN encoder & - & 2-layer MLP \\ \hline GAT & Graph attention network & 2/3-layer GAT encoder & - & 2-layer MLP \\ \hline GraphTransf. & Transformer model designed & 2/3-layer GraphTransf. & - & 1-layer MLP \\ \hline \multirow{2}{*}{Nodeformer} & Efficient Transformer variant & 2/34-layer Nodeformer & - & 1-layer MLP \\  & design for graphs & encoder & - & 1-layer MLP \\ \hline \multirow{2}{*}{Evol.MPNN} & - & 2/3-layer Residue encoder & - & 1-layer MLP \\  & & and Evolution encoder & - & 1-layer MLP \\ \hline \multirow{2}{*}{Evol.GNN} & - & 2/3-layer Residue encoder & \multirow{2}{*}{-} & 1-layer MLP \\  & & and Evolution encoder & - & 1-layer MLP \\ \hline \multirow{2}{*}{Evol.Former} & - & 2/3-layer Residue encoder & \multirow{2}{*}{-} & 1-layer MLP \\  & & and Evolution encoder & - & 1-layer MLP \\ \hline \hline \end{tabular}
\end{table}
Table 4: Description and implementation of baseline methods.
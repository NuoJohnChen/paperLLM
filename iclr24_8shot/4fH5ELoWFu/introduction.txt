Neural networks are increasingly adopted in many domains, including security-critical systems such as self-driving cars (Kurakin et al., 2017b) and face-recognition-based authentication systems (Sharif et al., 2016). Meanwhile, various safety and security issues of neural networks are identified as well. Arguably the most notable one is the presence of adversarial examples. Adversarial examples are inputs that are carefully crafted by adding human imperceptible perturbation to normal inputs to trigger wrong predictions (Kurakin et al., 2017a). Their existence poses a significant threat when the neural networks are deployed in security-critical scenarios. For example, adversarial examples can mislead road sign recognition systems of self-driving cars and cause accidents (Kurakin et al., 2017b). In other use cases, adversarial examples may allow unauthorized access through face-recognition-based authentication (Sharif et al., 2016). To defend against adversarial examples, various methods for improving a model’s robustness have been proposed. Two prominent categories are adversarial training (Bai et al., 2021; Wong et al., 2020) and certified training (Müller et al., 2022; Shi et al., 2021), both of which aim to improve a model’s accuracy in the presence of adversarial examples whilst maintaining their accuracy with normal inputs if possible. Adversarial training works by training the neural network with a mixture of normal and adversarial examples. The latter may be either generated before hand (Miyato et al., 2019) or during the training (e.g., min-max training (Zhang et al., 2019a)). While empirical studies show that adversarial training often improves a model’s robustness whilst maintaining model accuracy, it does not offer any formal guarantee of model robustness (Zhang et al., 2019b), rendering it less than ideal. For instance, a model trained through adversarial training can still be vulnerable to new threats such as adaptive adversarial attacks (Liu et al., 2019a; Tramer et al., 2020). Certified training aims to provide a certain guarantee of robustness. These methods typically incorporate robustness verification techniques (Xu et al., 2020) during training, i.e., they aim to find a valuation of network parameters such that the model is provably robust with respect to the training samples. While they may certify the model robustness on some input samples, they often reduce the model’s accuracy significantly (Chiang et al., 2020). Recent studies have shown that state-of-the-art certified training can result in up to 70% accuracy drop on MNIST and 90% on CIFAR-10 (Chiang et al., 2020). This is unacceptable for many real-world applications. Furthermore, due to the complexity of neural network verification, such techniques are often limited to small or medium models and limited kinds of perturbations (Müller et al., 2022). Therefore, there is a pressing need for an effective and efficient approach that can achieve both high accuracy and certified robustness. An alternative method to certified training is randomized smoothing (Cohen et al., 2019) which certifies certain forms of robustness (e.g., robustness within some L2-norm) by systematically introducing strong noises during training. It however suffers from the same problem of significant accuracy loss. In this work, we introduce a method that certifies a model’s probabilistic robustness whilst maintaining its accuracy. Our method is designed based on the belief that deterministic robustness (i.e., a model always makes the same decision within a certain vicinity) is often infeasible without seriously compromising accuracy, whereas probabilistic robustness (e.g., a model makes the same prediction most of the time within a certain vicinity) is often sufficient in practice. Our approach comprises two parts, i.e., a novel probabilistic robust training method that minimizes divergence variance, and a runtime inference method to certify the model’s probabilistic robustness. In the training phase, our approach focuses on minimizing variance across model predictions on similar inputs to improve the robustness. Unlike existing adversarial training methods that focus on one specific group of adversarial attacks, e.g., PGD-based adversarial training (Zhang et al., 2019a) relies on the PGD attack (Madry et al., 2018), our method improves the model’s robustness without overfitting to specific adversarial attacks. Furthermore, our approach can be easily applied to handle a variety of different perturbations, such as rotation and scaling. Note that unlike randomized smoothing, our method does not introduce noise during training. In the inference phase, our approach certifies the model’s probabilistic robustness by considering a given input in its peripheral region. We show that the probabilistic certified robustness of a model can be derived from the accuracy of the model in the peripheral region. We evaluate our method by training models on multiple standard benchmark datasets and compare them with state-of-the-art robustness-improving methods, including adversarial training, certified training and others. We compare our approach with eight baseline approaches in terms of standard accuracy (i.e., accuracy on normal test data), adversarial accuracy (i.e., accuracy in the presence of adversarial attacks), certified robustness rate (i.e., the probability of a test sample on which the model’s probabilistic robustness is successfully certified), and certified robust accuracy (i.e., probability of a test sample being certified robust and correct). Compared to the state-of-the-art adversarial training, we show that our method achieves a competitive or higher adversarial accuracy while sacrificing significantly less standard accuracy (i.e., up to 50% less). More importantly, we are able to certify the model’s robustness with regards to most of the test inputs (i.e., up to 96.8% on MNIST and 92% on CIFAR-10). Compared to the state-of-the-art certified training, our method achieves a highly robust model whilst maintaining the model’s accuracy, i.e., our standard accuracy is almost twice as high as that of certified training. Overall, the experiments show our method achieves a high level of certified robustness whilst maintaining the model accuracy.
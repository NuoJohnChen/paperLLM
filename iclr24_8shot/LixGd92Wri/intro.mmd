## 1 Introduction

Machine learning (ML) techniques, as a powerful and efficient approach, have been widely used in diverse scientific fields, including high energy physics (HEP) (Duarte and Vlimant, 2022), materials science (Fung et al., 2021), and drug discovery (Vamathevan et al., 2019), propelling ML4S (ML for Science) into a promising direction. In particular, geometric deep learning (GDL) is gaining much focus in scientific applications because many scientific data can be represented as point cloud data embedded in a complex geometric space. Current GDL research mainly focuses on neural network architectures design (Thomas et al., 2018; Fuchs et al., 2020; Jing et al., 2020; Schutt et al., 2021; Tholke and De Fabritiis, 2021; Liao and Smidt, 2022), capturing geometric properties (_e.g.,_ invariance and equivariance properties), to learn useful representations for geometric data, and these backbones have shown to be successful in various GDL scenarios.

However, ML models in scientific applications consistently face challenges related to data distribution shifts (\(\mathbb{P}_{\mathcal{S}}(X,Y)\neq\mathbb{P}_{\mathcal{T}}(X,Y)\)) between the training (source) domain \(\mathcal{S}\) and the testing (target) domain \(\mathcal{T}\). In particular, the regime expected to have new scientific discoveries has often been less explored and thus holds limited data with labels. To apply GDL techniques to such a regime, researchers often resort to training models over labeled data from the well-explored regimes or theory-guided simulations, whose distribution may not align well with the real-world to-be-explored regime of scientific interest. In materials science, for example, the OC20 dataset (Chanussof et al., 2021) covers a broad space of catalyst surfaces and adsorbates. ML models trained over this dataset may be expected to extrapolate to new catalyst compositions such as oxide electrocatalysts (Tran et al., 2023). Additionally, in HEP, models are often trained based on simulated data and are expected to generalize to real experiments, which hold more variable conditions and may differ substantially from simulations (Liu et al., 2023).

Despite the significance, scant research has systematically explored the distribution shift challenges specific to GDL. Findings from earlier studies on CV and NLP tasks (Chang et al., 2020; Creager et al., 2021; Yao et al., 2022b) might not be directly applicable to GDL models, due to the substantially distinct model architectures.

In the context of ML4S, several studies address model generalization issues, but there are two prominent _disparities_ in these works. First, previous studies are often confined to specific scientific scenarios that have different types of distribution shifts. For example, Yang et al. (2022b) concentrated exclusively on drug-related shifts such as scaffold shift, while Hoffmann et al. (2023) investigated model generalization to deal with the label-fidelity shifts in the application of materials property prediction. Due to the disparity in shift types, the findings effective for one application might be ineffectual for another.

Second, studies often assume different levels of the availability of target-domain data information. Specifically, while some studies assume some availability of the data from the target domain (Hoffmann et al., 2023), they differ on whether such data is labeled or not. On the other hand, certain investigations presume total unavailability of the target-domain data (Miao et al., 2022). These varying conditions often dictate the selection of corresponding methodologies.

To address the above disparities, this paper presents **GDL-DS**, a benchmark to evaluate GDL models' capability of dealing with various types of distribution shifts across scientific applications. Specifically, the datasets cover three scientific fields: HEP, biochemistry, and materials science, and are collected from either real experimental scenarios exhibiting distribution shifts, or simulated scenarios designed to mimic real-world distribution shifts. Plus, we leverage the inherent causality of these applications to categorize their distribution shifts into different categories: conditional shift (\(\mathbb{P}_{\mathcal{S}}(X|Y)\neq\mathbb{P}_{\mathcal{T}}(X|Y)\) and \(\mathbb{P}_{\mathcal{S}}(Y)=\mathbb{P}_{\mathcal{T}}(Y)\)), covariate shift (\(\mathbb{P}_{\mathcal{S}}(Y|X)=\mathbb{P}_{\mathcal{T}}(Y|X)\) and \(\mathbb{P}_{\mathcal{S}}(X)\neq\mathbb{P}_{\mathcal{T}}(X)\)), and concept shift (\(\mathbb{P}_{\mathcal{S}}(Y|X)\neq\mathbb{P}_{\mathcal{T}}(Y|X)\)). Furthermore, to address the disparity of assumed available out-of-distribution (OOD) information across previous works, we study three levels: no OOD information (No-Info), only OOD features without labels (Q-Feature), and OOD features with a few labels (Par-Label). We evaluate representative methodologies across these three levels, specifically, OOD generalization methods for the No-Info level, domain adaptation (DA) methods for the O-Feature level, and transfer learning (TL) methods for the Par-Label level.

Our experiments operated on 3 diverse scientific domains and 6 datasets include in total 30 different settings with 10 different distribution shifts times 3 levels of OOD info, covering 3 GDL backbones and 11 learning algorithms in each setting. According to our experiments, we observe that no approach can be the best for all types of shifts, and the levels of OOD information may benefit ML models to various extents across different applications. In the meantime, our comprehensive evaluation also yields three valuable takeaways to guide the selection of practical solutions depending on the availability of OOD data:

* For the setting with some labeled OOD data, TL methods show advantages under concept shifts. This is particularly noticeable when there are significant changes in the marginal label distribution.
* For the setting with only unlabeled OOD data, DA methods show advantages when the distribution shifts happen to the features that are critical for label determination compared with other features.
* For the case without OOD information, OOD generalization methods will have some improvements if the training dataset can be properly partitioned into valid groups that reflect the shifts.

Accordingly, we recommend three steps to GDL practitioners in handling the OOD generalization issues: 1) Assess the type of data distribution shifts in your application by leveraging some domain-specific knowledge; 2) Assess the availability of collecting some labeled or unlabeled OOD data; 3) Utilize the acquired assessments to select the appropriate category of methods. Our benchmark provides practitioners with insights for making the most suitable choice.


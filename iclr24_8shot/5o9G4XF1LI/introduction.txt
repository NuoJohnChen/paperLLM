To solve a problem using Reinforcement Learning (RL), it is necessary first to formalise that problem using a reward function (Sutton & Barto, 2018). However, due to the complexity of many real-world tasks, it is exceedingly difficult to directly specify a reward function that fully captures the task in the intended way. However, misspecified reward functions will often lead to undesirable behaviour (Paulus et al., 2018; Ibarz et al., 2018; Knox et al., 2023; Pan et al., 2021). This makes designing good reward functions a major obstacle to using RL in practice, especially for safety-critical applications. An increasingly popular solution is to learn reward functions from mechanisms such as human or automated feedback (e.g. Christiano et al., 2017; Ng & Russell, 2000). However, this approach comes with its own set of challenges: the right data can be difficult to collect (e.g. Paulus et al., 2018), and it is often challenging to interpret it correctly (e.g. Mindermann & Armstrong, 2018; Skalse & Abate, 2023). Moreover, optimising a policy against a learned reward model effectively constitutes a distributional shift (Gao et al., 2023); i.e., even if a reward function is accurate under the training distribution, it may fail to induce desirable behaviour from the RL agent. Therefore in practice it is often more appropriate to think of the reward function as a proxy for the true objective rather than being the true objective. This means that we need a more principled understanding of what happens when a proxy reward is maximised, in order to know how we should expect RL systems to behave, and in order to design better algorithms. For example, we aim to answer questions such as: When is a proxy safe to maximise without constraint? What is the best way to maximise a misspecified proxy? What types of failure modes should we expect from a misspecified proxy? Currently, the field of RL largely lacks rigorous answers to these types of questions. In this paper, we study the effects of proxy misspecification through the lens of Goodhart’s law, an informal principle often stated as “any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes” (Goodhart, 1984), or more simply: “when a measure becomes a target, it ceases to be a good measure”. For example, a students’ knowledge of a subject ˚Correspondence to jacek.karwowski@cs.ox.ac.uk may be correlated with their ability to pass exams on that subject by default. However, students who have sufficiently strong incentives to do well in exams may also include strategies such as cheating for increasing their test score without increasing their understanding. In the context of RL, we can think of a misspecified proxy reward as a measure correlated, but not robustly aligned, with the true objective across some distribution of policies. Goodhart’s law then says, informally, that we should expect optimisation of the proxy to initially lead to improvements on the true objective, up until a point where the correlation between the proxy reward and the true objective breaks down, after which further optimisation should lead to worse performance according to the true objective (Figure 1). In this paper, we present several novel contributions. First, we show that “Goodharting” occurs with high probability for a wide range of environments and pairs of true and proxy reward functions. Next, we provide a mechanistic explanation of why Goodhart’s law emerges in RL. We use this to derive two new policy optimisation methods and show that they provably avoid Goodharting. Finally, we evaluate these methods empirically. We thus contribute towards building a better understanding of the dynamics of optimising towards imperfect proxy reward functions, and show that these insights may be used to design new algorithms.
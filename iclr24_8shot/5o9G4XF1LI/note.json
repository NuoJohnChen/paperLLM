{
    "id": "5o9G4XF1LI",
    "forum": "5o9G4XF1LI",
    "signatures": [
        "ICLR.cc/2024/Conference/Submission6832/Authors"
    ],
    "readers": [
        "everyone"
    ],
    "writers": [
        "ICLR.cc/2024/Conference",
        "ICLR.cc/2024/Conference/Submission6832/Authors"
    ],
    "content": {
        "title": {
            "value": "Goodhart's Law in Reinforcement Learning"
        },
        "authors": {
            "value": [
                "Jacek Karwowski",
                "Oliver Hayman",
                "Xingjian Bai",
                "Klaus Kiendlhofer",
                "Charlie Griffin",
                "Joar Max Viktor Skalse"
            ]
        },
        "authorids": {
            "value": [
                "~Jacek_Karwowski1",
                "~Oliver_Hayman1",
                "~Xingjian_Bai1",
                "~Klaus_Kiendlhofer1",
                "~Charlie_Griffin1",
                "~Joar_Max_Viktor_Skalse1"
            ]
        },
        "keywords": {
            "value": [
                "reinforcement learning",
                "goodhart's law",
                "misspecification",
                "reward learning"
            ]
        },
        "TLDR": {
            "value": "We study Goodhart's law in RL empirically, provide a theoretical explanation for why it occurs, and use these theoretical insights to derive two methods for avoiding Goodharting."
        },
        "abstract": {
            "value": "Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a *proxy* for the true objective rather than as its definition. We study this phenomenon through the lens of *Goodhart\u2019s law*, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to *quantify* the magnitude of this effect and *show empirically* that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart\u2019s law for a wide range of environments and reward functions. We then provide a *geometric explanation* for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an *optimal early stopping method* that provably avoids the aforementioned pitfall and derive theoretical *regret bounds* for this method. Moreover, we derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we evaluate our early stopping method experimentally. Our results support a foundation for a theoretically-principled study of reinforcement learning under reward misspecification."
        },
        "primary_area": {
            "value": "reinforcement learning"
        },
        "code_of_ethics": {
            "value": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics."
        },
        "submission_guidelines": {
            "value": "I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide."
        },
        "anonymous_url": {
            "value": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity."
        },
        "no_acknowledgement_section": {
            "value": "I certify that there is no acknowledgement section in this submission for double blind review."
        },
        "venue": {
            "value": "ICLR 2024 poster"
        },
        "venueid": {
            "value": "ICLR.cc/2024/Conference"
        },
        "pdf": {
            "value": "/pdf/6153eba150ad68aef8616d6e9956362c49c52bf0.pdf"
        },
        "_bibtex": {
            "value": "@inproceedings{\nkarwowski2024goodharts,\ntitle={Goodhart's Law in Reinforcement Learning},\nauthor={Jacek Karwowski and Oliver Hayman and Xingjian Bai and Klaus Kiendlhofer and Charlie Griffin and Joar Max Viktor Skalse},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=5o9G4XF1LI}\n}"
        },
        "paperhash": {
            "value": "karwowski|goodharts_law_in_reinforcement_learning"
        }
    },
    "number": 6832,
    "odate": 1697213872796,
    "invitations": [
        "ICLR.cc/2024/Conference/-/Submission",
        "ICLR.cc/2024/Conference/-/Post_Submission",
        "ICLR.cc/2024/Conference/Submission6832/-/Revision",
        "ICLR.cc/2024/Conference/-/Edit",
        "ICLR.cc/2024/Conference/Submission6832/-/Camera_Ready_Revision"
    ],
    "domain": "ICLR.cc/2024/Conference",
    "tcdate": 1695441452361,
    "cdate": 1695441452361,
    "tmdate": 1710516940492,
    "mdate": 1710516940492,
    "pdate": 1705410988855,
    "version": 2
}
This paper considers a vertex classification task in the “graph-with-features” setting. A dataset for this setting consists of a graph and a feature associated with each vertex. The task is to classify vertices using the features and the graph. The modern approach to this task is graph neural networks (GNNs) (Gori et al., 2005; Kipf & Welling, 2016a; Veličković et al., 2018). GNNs propagate features over the graph to build expressive latent embeddings; the embeddings are then consumed in downstream classification models. However, due to the nature of these GNN architectures, GNNs are typically known to have a bias towards homophilous information and to not be effective in learning heterophilous information (Hoang & Maehara, 2019; Luan et al., 2022). This bias worsens if we stack GNN layers (known as “over-smoothing” (Li et al., 2018; Oono & Suzuki, 2019)). Some recent GNN models mitigate this bias, such as (Azabou et al., 2023; Pei et al., 2020; Luan et al., 2021). However, such models, including these examples, often involve complicated GNN architectures. In this paper, to overcome this homophilous bias in a simpler way, we propose an alternative approach to GNNs since this bias seems to be inherent in GNN architectures. Instead of mitigating biases by complicating the GNNs, our approach is to obtain a vector representation for the features and graph. Then, we apply standard vector-based learning methods to this vector representation, such as established neural network (NN) based models like variational autoencoder or even support vector machines (SVMs). For this approach, we propose a Resistance Transformation (abbreviated as ResTran), a simple transformation of feature vectors to incorporate graph structural information. We theoretically justify ResTran from a connection between the k-means and spectral clustering. Our justification is inspired by (Dhillon et al., 2004), which justifies using feature maps for spectral clustering applied to vector data. For this purpose, Dhillon et al. (2004) takes the following steps as i) setting up k-means objective for transformed vectors by a feature map and ii) showing the equivalence from this k-means objective to spectral clustering. For ResTran, we follow a similar strategy: i) modifying the k-means to incorporate the vector representation by ResTran and ii) showing the equivalence from this k-means to spectral clustering. We show that this modified k-means for the featureless setting (i.e., looking only at a graph by taking features as an identity matrix) is equivalent to spectral clustering. Moreover, for the graph-with-features setting, we show that this k-means can be seen as a natural extension of spectral clustering from the featureless to the graph-with-features setting. We also discuss why ResTran may preserve the homophilous and heterophilous information better than the established GNNs. Our experiments show that ResTran outperforms graph-only and feature-only representation in unsupervised tasks. We also numerically show that ResTran is more robust to the homophilous bias than established GNNs in the semi-supervised learning (SSL) tasks. Contribution. In summary, our contributions are as follows. i) We propose a simple ResTran for a graph-with-features problem. ii) We theoretically justify ResTran from an effective resistance, k-means, and spectral clustering perspective. iii) We numerically confirm that ResTran is more robust to homophilous bias than established GNNs for common datasets. All proofs are in the Appendix.
# Learning Deep Improvement Representation to Accelerate Evolutionary Optimization

Anonymous authors

Paper under double-blind review

###### Abstract

Evolutionary algorithms excel at versatile optimization for complex (e.g., multiobjective) problems but can be computationally expensive, especially in high-dimensional scenarios, and their stochastic nature of search may hinder swift convergence to global optima in promising directions. In this study, we train a multilayer perceptron (MLP) to learn the improvement representation of transitioning from poor-performing to better-performing solutions during evolutionary search, facilitating the rapid convergence of the evolutionary population towards global optimality along more promising paths. Then, through the iterative stacking of the well-trained lightweight MLP, a larger model can be constructed, enabling it to acquire deep improvement representations (DIR) of solutions. Conducting evolutionary search within the acquired DIR space significantly accelerates the population's convergence speed. Finally, the efficacy of DIR-guided search is validated by applying it to the two prevailing evolutionary operators, i.e., simulated binary crossover and differential evolution. The experimental findings demonstrate its capability to achieve rapid convergence in solving challenging large-scale multi-objective optimization problems.

## 1 Introduction

Optimization serves as a fundamental component in numerous real-world applications and machine learning algorithms. For instance, it plays an essential role in optimizing vehicle routes for cost-efficiency in logistics (Thanh et al., 2023), forms the core of hyperparameter tuning in AutoML (Zhang et al., 2023), defines and minimizes the multiple loss functions in multitask learning (Lin et al., 2019), etc. The optimization problems in these applications may be challenging due to their non-convex, multiobjective, evaluation-expensive, and/or large-scale nature. Addressing such challenges demands the use of well-designed optimizers, with evolutionary algorithms (EAs) standing out as promising problem-solving tools (Liu, 2022). Nevertheless, EAs can be computationally demanding, which limits their adaptability to lightweight optimization requirements (Coello Coello et al., 2020). In recent years, there has been a growing emphasis on conducting computations closer to data sources, such as onboard or alongside a connected camera in a self-driving car, to enable real-time optimization services (Gulotta, 2023). This shift has led to a transition of computing from the centralized cloud to the edge devices, where computing resources are severely limited.

However, many existing EAs were developed without considering these resource limitations. In the quest for lightweight optimization, EAs must enhance efficiency to address the growing complexity of challenges (Del Ser et al., 2019), notably those related to large model and big data optimization that are often computationally demanding, particularly in terms of function evaluations (Chugh et al., 2019). Building on the observations outlined above, this study aims to enhance the efficiency of EAs for solving large-scale multi-objective optimization problems (LMOPs). In the literature, extensive efforts have been dedicated to improve EAs for solving LMOPs, which can be broadly classified into three main categories:

**Decomposition of Search Space:** This approach employs a divide-and-conquer mechanism, where decision variables are grouped or clustered by the developed variable decomposition methods (Zhao et al., 2022), including linear, random, and differential based methods (Ou et al., 2022). Optimization is then carried out collaboratively on each of these groups (subspaces), simplifying the problem-solving process (Zhong et al., 2022). However, it typically relies on rich domain expere use for problem decomposition which may not be available. Incorrect grouping of variables may mislead evolutionary search and slow down population convergence (Duan et al., 2023). Analyzing the importance (or contribution) of variables and their interrelationships before grouping requires a substantial number of function evaluations (Liu et al., 2022).

**Dimension Reduction of Search Space:** This method transforms the original LMOP into smaller-scale problems using existing dimensionality reduction technique, such as random embedding (Qian and Yu, 2017), unsupervised neural networks (Tian et al., 2020), problem transformation (Zille et al., 2016), and principal component analysis (Liu et al., 2020). This conversion allows optimization to take place in a simplified representation space, leading to a substantial reduction in the volume of the high-dimensional search space. Nevertheless, it does not guarantee the preservation of the original global or near-global optimum when operating within the compressed search space, and thus it may potentially miss certain optimal regions, making populations susceptible to local optima entrapment. The dimensionality reduction process often overlooks constraints related to computational resources.

**Design of Novel Search Strategy:** In contrast to the preceding methods that alleviate problem complexity before optimization, this category of algorithms tackles LMOPs directly, taking all decision variables into account. It achieves this by designing new, powerful evolutionary search strategies for offspring reproduction, such as competitive learning-based search (Liu et al., 2021), bidirectional-guided adaptive search (He et al., 2020), adversarial learning-aided search (Wang et al., 2021), and fuzzy-controlled search (Yang et al., 2021). Without proper guidance towards the correct search direction, there's a likelihood of venturing into the misleading areas during optimization, resulting in a wasteful consumption of computing resources (Omidvar et al., 2021). These novel search strategies still fall considerably short of meeting the demands for lightweight optimization.

Despite these efforts, their search capabilities often fall short of effectively handling the exponentially expanded search space within the constraints of acceptable computational resources. In pursuit of accelerated evolutionary optimization, researchers have investigated online innovation progress operators aimed at guiding offspring towards learned promising directions (Deb and Srinivasan, 2006). These operators involve training machine learning models online to get performance improvement representations of solutions (Gaur and Deb, 2017). This process encompasses three primary steps: gathering solutions from previous generations, training the model to identify patterns, and utilizing it to rectify newly generated offspring (Mittal et al., 2020). However, existing innovation operators are only developed for small-scale optimization. In addition, the online training of deep models introduces computational overhead, particularly in the context of large-scale optimization, and the resulting acceleration in convergence still falls short of expectations. In response, to expedite the optimization of LMOPs, this work introduces a deep accelerated evolutionary search strategy driven by an inexpensive large model, which is stacked repeatedly by multiple lightweight models. This study presents three main contributions: 1) Development of a lightweight model capable of learning both compressed and performance improvement representations of solutions. 2) Analysis of the varying impacts of evolutionary search in the learned representation space. 3) Design of a large model for acquiring deep improvement representations (DIR) of solutions, aimed at enabling efficent optimization of LMOPs. The relevant background, technical details, and specific experimental design and verification are respectively elaborated in sections 2, 3, and 4 below.

## 2 Preliminaries and Motivations

### large-scale multiobjective optimization

We exclusively assess the performance of EAs on continuous LMOPs. These LMOPs involve multiple conflicting objectives defined over high-dimensional solution vectors with a considerable number of interrelated variables. For simplicity and generalization, an LMOP is defined as follows:

\[\text{Minimize }F(x)=\left(f_{1}(x),\ldots,f_{m}(x)\right),x\in\Omega \tag{1}\]

where \(x=\left(x_{1},x_{2},\ldots,x_{n}\right)\) is a solution vector with \(n\) variables from the search space, and \(F(x)\) defines \(m\) objective functions \(f_{1}(x),\ldots,f_{m}(x)\), \(m\geq 2\) and \(n\) is a relatively large value (e.g., \(n\geq 1000\)). Due to the inherent conflicts among these objectives, finding a single optimal solution for LMOPs is often unattainable. Instead, LMOPs typically yield a set of trade-off solutions known as the Pareto set (PS). Moreover, the projection of this PS onto the objective space is termed the Pareto front (PF). Consequently, the primary goal when addressing an LMOP with EAs is to discover a set of solutions that effectively and evenly approximate the PS/PF. To facilitate a comprehensive understanding of solving LMOPs, we introduce two key definitions:

**Definition 1** (Pareto Dominance): _given two solutions \(x\) and \(y\). we say \(x\) dominates \(y\), termed as \(x\prec y\), if \(f_{i}(x)\leq f_{i}(y)\) for \(\forall i\in\{1,2,\dots,m\}\) and \(f_{j}(x)<f_{j}(y)\) that for \(\exists j\in\{1,2,\dots,m\}\)._

**Definition 2** (Pareto Optimal Solution): _we say solution \(x^{*}\) is a Pareto optimal if and only if \(x^{*}\) cannot be dominated by any solution \(x\in\Omega\)._

### multiobjective evolutionary algorithms

Multiobjective evolutionary algorithms (MOEAs) have gained widespread popularity in tackling complex multiobjective optimization problems (Guliashki et al., 2009). As shown in Figure 1(a), an MOEA begins with an initial parent population and generates novel offspring using a generative model equipped with evolutionary operators, such as crossover and mutation. These parent and offspring solutions are then evaluated by a selective model, which retains only the elite solutions identified as superior for survival into the next generation. Interestingly, this MOEA approach shares common traits with other problem-solving models like generative adversarial networks (Goodfellow et al., 2014) and reinforcement learning (Wang et al., 2021). Specifically, an MOEA's generator aims to produce offspring with higher quality than their parents, while its selector classifies solutions based on their quality, subsequently filtering out poorly performing ones. Together, the generator and selector constitute a synergistic mechanism driving the search for diverse and increasingly convergent solutions to approximate elusive optima.

Despite significant development over the years, MOEAs still face limitations in effectively addressing LMOPs. The challenges can be attributed to several factors. As the number of variables increases, the search space grows exponentially, demanding that the generator exhibit enhanced search capabilities, such as accelerated convergence, while working within limited computational resources. Moreover, the intricate structural and property characteristics of LMOPs, including factors like separability and nonlinearity, complicate matters further. Consequently, effective search strategies employed by the generator must be scalable to combat the "curse of dimensionality" inherent in extensive search spaces (Liu, 2022). Unfortunately, conventional evolutionary operators like simulated binary crossover (SBX), polynomial mutation (PM), particle swarm optimization, d-ifferential evolution (DE), and evolutionary strategy have been proven ineffective when confronted with the challenges posed by large-scale search spaces (Omidvar et al., 2021).

### learnable evolutionary search

Evolutionary optimization and incremental learning are innate methods humans employ to enhance their problem-solving capabilities (Michalski, 2000a). Relying solely on traditional evolutionary search strategies to solve LMOPs may be inadequate and inefficient (Wu et al., 2023), as the generator lacks the adaptability needed to grasp the precise characteristics of the LMOP they encounter (Bonissone et al., 2006). Consequently, it struggle to flexibly address the challenges posed by such black-box LMOPs. This is underscored by the fact that biological evolution can take thousands of

Figure 1: Illustration of the main process in an MOEA and the proposed Learnable MOEA.

years to optimize a species (Miikkulainen & Forrest, 2021), whereas cumulative learning can dramatically accelerate this optimization process (Li et al., 2023). Moreover, the generator conducts iterative search of the variable space, generating a substantial dataset of feasible solutions. Employing machine learning (ML) techniques for the systematic analysis of these data enhances the understanding of search behavior and improves future search capabilities (Zhang et al., 2011).

Inspired by this, an intriguing research question emerges: Can we merge an evolutionary search with ML, creating learnable evolutionary search, to develop a more potent EA-based optimizer for efficiently addressing the scalability of LMOPs? Relevant existing attempts in this regard are given in the appendix A.1 and A.2. In an ideal scenario, a lightweight model \(M(A)\) is trained using existing feasible solutions (i.e., data \(D\)) to enable one-shot or few-shot optimization. Precisely, after a generation or a few generations of evolutionary search, the trained model can directly output the target LMOP's Pareto optimal representation \(x^{*}\) corresponding to each candidate solution \(x\) in the current population. It can be expressed in the following mathematical form:

\[x^{*}=\Theta(x;A^{*},\theta^{*},D^{*})\leftarrow(A^{*},\theta^{*},D^{*})= \operatorname*{arg\,min}_{D}\{M(A),L(\theta)\} \tag{2}\]

where three key components need to be identified for getting \(x^{*}\): the well-prepared training data \(D^{*}\), the lightweight architecture \(A^{*}\), and the optimal model parameters \(\theta^{*}\) to minimize the loss \(L(\theta)\). Even if \(x^{*}\) is not the Pareto optimal representation of \(x\), its superior performance significantly contributes to accelerating the evolutionary optimization. Thus, rapid population convergence can be guaranteed theoretically. This is obviously a meaningful but very challenging multi-layer optimization problem. Nevertheless, this work seeks breakthroughs along this research direction to improve the performance and efficiency of EAs for solving complex LMOPs.

Similar initiatives include autoencoder-based learning (Tian et al., 2020), as depicted in Figure 2(a), which aims to obtain compressed representations in the code layer, and innovization progress learning (Mittal et al., 2021), illustrated in Figure 2(b), which focuses on acquiring improvement representations. The autoencoder is primarily employed to reconstruct explored non-dominated solutions, lacking the ability to enhance solution quality, thus falling short in accelerating the convergence of the evolutionary search. The innovization progress model is mainly designed for repairing newly generated solutions (Mittal et al., 2021), as indicated in formula (2), and may not fully exploit the potential of evolutionary search. Moreover, their reliance on relatively large models necessitates a substantial amount of training data, which can be inefficient and less adaptable as the optimization progresses. Typically, they draw data from extensive past populations. However, as the optimization progresses, the promising directions of improvement change, and past populations may mislead model training. Therefore, contemporary populations often provide a more accurate reflection of the path towards optimal future solutions. Building upon these insights, this study aims to train a lightweight MLP model that effectively leverages the current population. This trained model is then iteratively stacked to create a larger model, with the goal of capturing deep improvement representations of solutions. Subsequently, an evolutionary search is conducted within this learned representation space to maximize the potential for discovering high-quality solutions.

## 3 Accelerated Evolutionary Optimization

The learnable MOEA (LMOEA) framework presented in this work closely resembles a standard MOEA, with the primary distinction residing in the generator component, as shown in Figure 1(b).

Figure 2: Illustration of the autoencoder-based learning and the innovization progress learning.

The pseudocode for the LMOEA process is given in the appendix, which consists of three fundamental steps: initialize a start parent population \(P\) with \(N\) random solutions, reproduce an offspring population \(Q\) composed of \(N\) child solutions by the generator, and filter half of the underperforming solutions from the combined population of \(P+Q\) with the selector. This generator-selector iteration continues until a predefined stopping condition is met, typically when the total number of function evaluations reaches the maximum budget \(FE_{max}\). What plays a major role in the generator is how to do effective evolutionary search. In this study, we design new learnable evolutionary search strategies in the learned representation space to accelerate the optimization for LMOPs.

### build a lightweight model

**Architecture \(A^{\star}\):** In our MLP design, both the input and output layers have the same number of neurons, aligning with the LMOP's variable size (\(n\)). We've carefully considered the computation-al cost of integrating a ML model into an EA, opting for a single hidden layer with \(K\) neurons to manage computational overhead (where \(K<<n\)). The computational complexity of running this model is akin to traditional evolutionary search operators. The activation is the sigmoid function. Training the MLP involves iteratively updating its parameters (weights and biases) using backpropagation with gradient descent. Specifically, we calculate the steepest descent direction by evaluating the loss relative to the current parameters and iteratively adjust the parameters along this gradient descent direction to minimize the loss. For evaluation, the mean-square error (MSE) is used as the loss function to be minimized.

**Training Data \(D^{\star}\):** Given the training dataset \(D=\left\{\left(x_{i},x_{i}^{l}\right)\right\}_{i=1}^{M}\), consisting of \(M\) input-label examples, the goal is to adjust the MLP's parameters so that the actual output \(y_{i}\) closely matches its corresponding label for all \(i=1,2,\ldots,M\), following statistical principles. The MLP undergoes supervised learning, guided by the labels \(x^{l}\), with the ultimate expectation of acquiring knowledge about the performance improvement representation of a given input solution \(x\). To ensure this representation is effective, it's essential that the label \(x^{l}\) corresponds to a solution vector that surpasses \(x\) according to predefined criteria. Furthermore, to ensure diversity within the dataset and encompass a broad range of scenarios for solving the target LMOP (i.e., generalization), we decompose it into \(N\) subproblems, leveraging a set of uniformly distributed reference vectors \((r_{1},r_{2},\ldots,r_{N})\) in the objective space. The classical Penalty-based Boundary Intersection (PBI) approach is used to define each subproblem, which can be expressed mathematically as follows:

\[\text{Minimize }g\left(x\mid r_{i}\right)=d_{1}^{i}+d_{2}^{i}\text{, where }d_{1}^{i}=F^{\prime}(x)^{T}r_{i}/\left|r_{i}\right|,d_{2}^{i}=\left|F^{\prime}(x )-\left(d_{1}^{i}/\left|r_{i}\right|\right)r_{i}\right| \tag{3}\]

PBI is a balanceable scalarizing function, which consists of two components, i.e., a convergence distance \(d_{1}^{i}\) and a diversity distance \(d_{2}^{i}\), where \(d_{1}^{i}\) is the projection distance of \(F^{\prime}(x)\) on the \(r_{i}\) and \(d_{2}^{i}\) is the perpendicular distance between \(F^{\prime}(x)\) and \(r_{i}\). The procedure for selecting an input-label pair of the \(ith\) subproblem is as follows: Locate the two solutions from the current population \(P\) with the smallest \(d_{2}^{i}\), and designate the solution with the higher \(g\left(x\mid r_{i}\right)\) value as the input \(x\), with the other serving as its label \(x^{l}\). Both objectives and variable values in the training data are normalized, with \(x_{i}\) and \(f_{j}(x)\) of solution \(x\) normalized as follows:

\[\text{Normalization: }x_{i}^{\prime}=\frac{x_{i}-L_{i}}{U_{i}-L_{i}},i=1, \ldots,n;f_{j}^{\prime}(x)=\frac{f_{j}(x)-z_{j}^{\min}}{z_{j}^{\max}-z_{j}^{ \min}},j=1,\ldots,m \tag{4}\]

where \(z_{j}^{\min}\) and \(z_{i}^{\max}\) are, respectively, the minimum and maximum values of the \(ith\) objective for all solutions in \(P_{i}^{l}\), \(L_{i}\) and \(U_{i}\) are the lowest and uptest bound of the \(ith\) variable. These \(N\) PBI subproblem-guided solution pairs form \(D^{\star}\). Thus, we start by initializing the MLP with random parameters and train it on \(D^{\star}\) using a learning rate of 0.1, momentum of 0.9, and 2 epochs.

### deep accelerated evolutionary search

After training the MLP, new offspring of the target LMOP can be generated in four ways: 1) Traditional evolutionary search in the original space. 2) Inputting newly generated offspring into the MLP to obtain improvement representations directly. 3) Creating compressed representations, conducting an evolutionary search in the compressed space to generate new codes, and decoding them for improvement representations. 4) Obtaining improvement representations first and then evolutionary search in the improvement representation space. Expanding on the foundations laid by NSGA-II(Deb et al., 2002) and MOEA/D (Zhang and Li, 2007), we will delve into these four scenarios. In the first scenario, SBX and DE serve as the evolutionary search operators respectively in NSGA-II and MOEA/D. In the subsequent three scenarios, three distinct learnable MOEA variants are proposed for both NSGA-II (termed LNSGAV1-3) and MOEA/D (referred to as LMOEADV1-3). These variants improve upon the SBX and DE strategies by incorporating the MLP (see appendix A.3).

To further boost efficiency, we stack the trained MLP \(t\) times to create a larger model. This expanded model provides a deeper improvement representation of solutions, as shown in Figure 3. Then, we can repair new generated solutions to get their DIRs or carry out evolutionary search within the DIR space, with the goal of substantially accelerating the optimization process and achieving few-shot optimization of LMOPs. Combining these two search strategies, another two new learnable MOEA variants for both NSGA-II (termed LNSGAV4-5) and MOEA/D (referred to as LMOEADV4-5) are developed. In addition, completely avoiding search in the original space carries the risk of losing crucial information, potentially leading to slow growth of the MLP model and a decline in overall optimization performance. To mitigate this concern, LNSGAV1-5 and LMOEADV1-5 balance between original and learnable evolutionary search with an adaptive probability for each to generate offspring solutions at each generation. Their pseudo-code is provided in the appendix A.3.

## 4 Experimental Studies

The source codes for all the EA solvers and test LMOPs in our experimental studies are implemented on PlatEMO (Tian et al., 2023). We conduct all experiments on a personal computer with an Intel(R) Core(TM) i5-10505 CPU (3.2 GHz) and 24GB RAM. To ensure a statistically sound comparison, the proposed optimizers and their competitors run 20 times independently on each test problem. In each run, we set the termination condition as \(FE_{max}=10^{5}\). The population size (\(N\)) is fixed at 100 for 2-objective LMOPs and 150 for 3-objective LMOPs. To assess the performance of an EA on LMOPs, we use two well-established metrics: inverted generational distance (IGD) (Ishibuchi et al., 2015) and hypervolume (HV) (Boelrijk et al., 2022). They gauge convergence and diversity in the final population. IGD is computed using \(10^{4}\) points from the true Pareto front, while normalized HV employs a reference point \((1,1,\ldots,1)\). Smaller IGD and larger HV values signal better performance, indicating effective coverage of the true PF by the obtained final population.

### Effectiveness validation of proposed accelerated evolutionary search

We commence the validation of the proposed accelerated evolutionary search strategies (NSGA-II vs. LNSGAV1-V5 and MOEA/D vs. LMOEADV1-V5) by optimizing synthetic LMOPs widely studied in the related literature. We focus on 2-objective DTLZ1 to DTLZ4 problems (Deb et al.),

Figure 3: Illustration of the main process of the proposed deep improvement representation learning.

with the number of variables (\(n\)) varying from 1000 to 10000. The used MLP model's hidden layer consists of 10 neurons, and the MLP is stacked three times during the DIR learning process.

Figure 4 depicts the evolutionary process based on IGD results for comparisons involving 2-objective DTLZ2 and DTLZ4 problems with 1000 variables. These convergence graphs highlight the notable superiority of the improved versions (LNSGAV1-V5 and LMOEADV1-V5) over their respective original versions (NSGA-II and MOEAD/D), particularly in terms of convergence speed. Specifically, when compared to NSGA-II (and likewise MOEA/D), most of its accelerated variants require only one-tenth of the computational resources to achieve near-Pareto optimal results for solving these two benchmarks. Furthermore, optimizers that explore the DIR space (LNSGAV4-5 and LMOEADV4-5) exhibit superior acceleration effects and final population performance.

Detailed IGD and HV results for solving 2-objective DTLZ1 to DTLZ4 problems with 1000 variables are given in Table 1, while the results for solving other DTLZ cases are presented in Tables 4 to 8 of the appendix. These results demonstrate the effectiveness of our proposed accelerated search strategies in improving evolutionary optimization efficiency. Nevertheless, several noteworthy observations can be drawn from these results: 1) The overall performance of all optimizers falls short when tackling DTLZ1 and DTLZ3, both of which are multimodal optimization problems, in which the number of local optima increases exponentially with the search space dimension. 2) The DIR-based search methods (LNSGAV4-5 and LMOEADV4-5) exhibit superior performance compared to their non-MLP stacking counterparts (LNSGAV1, LNSGAV3, LMOEADV1, and LMOEADV3) in solving DTLZ2 and DTLZ4, but the results show the opposite trend for DTLZ1 and DTLZ3. 3) Solvers that rely on searching in the compressed representation space (LNSGAV2 and LMOEADV2) exhibit slightly less stability and are not as effective in accelerating convergence. 4) The learned model typically provides a short-term acceleration effect on evolutionary optimization, and its fundamental utility becomes less evident in the later stages of evolution.

There are several reasons for these observations. Firstly, the effectiveness of learning the improvement representation of solutions depends heavily on the quality of training data. Our training data

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metric & Problem & MOEA/D & LMOEADV1 & LMOEADV2 & LMOEADV3 & LMOEADV4 & LMOEADV5 \\ \hline \multirow{2}{*}{IGD} & DTLZ1 & 3.805e+3 & **1.114e+0** & 5.949e+0 & 4.947e+0 & 1.966e+1 & 5.903e+2 \\  & & (1.5e+3) & (**2.8e+0**) & (2.9e+2) & (1.9e-2) & (2.9e-2) & (1.8e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 1.945e+0 & 1.225e-2 & 8.04e-2 & 5.419e-2 & 1.127e-2 & **4.96e-3** \\  & & (5.5e-1) & (1.1e-2) & (7.4e-2) & (6.2e-2) & (1.6e-1) & **(5.1e-3)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 1.172e+4 & **1.240e+1** & 3.047e+2 & 7.887e+2 & 1.273e+2 & 1.059e+3 \\  & & (3.6e+3) & (**2.6e+2**) & (8.3e+2) & (7.7e-2) & (6.8e+2) & (6.1e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 1.510e+0 & 1.286e-1 & 1.599e-2 & 5.569e-2 & 1.480e-2 & **8.609e-3** \\  & & (7.2e-2) & (1.3e-1) & (3.4e-1) & (8.9e-1) & (2.3e-2) & **(2.7e-2)** \\ \hline \multirow{2}{*}{HV} & DTLZ1 & 0.00e+0 & **4.289e-2** & 1.605e-2 & 3.325e-2 & 0.00e+0 & 0.00e+0 \\  & & (0.0e+0) & **(1.0e-1)** & (1.1e-1) & (5.1e-1) & (0.0e+0) & (0.0e+0) \\ \hline \multirow{2}{*}{n=1000} & DTLZ2 & 0.00e+0 & 3.340e-1 & 2.169e-1 & 2.583e-1 & 3.355e-1 & **3.506e-1** \\  & & (0.0e+0) & (1.7e-2) & (1.4e-1) & (1.4e-1) & (1.2e-1) & **(1.7e-1)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 0.00e+0 & 0.00e+0 & 0.00e+0 & 0.00e+0 & 0.00e+0 & 0.000e+0 \\  & & (0.0e+0) & (0.0e+0) & (0.0e+0) & (0.0e+0) & (0.0e+0) & (0.0e+0) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 0.00e+0 & 1.695e-1 & 3.026e-1 & 2.611e-1 & 3.174e-1 & **3.287e-1** \\  & & (0.0e+0) & (1.3e-1) & (1.5e-1) & (1.5e-1) & (1.5e-1) & **(2.0e-1)** \\ \hline \end{tabular}
\end{table}
Table 1: Average IGD and HV results of MOEA/D and its five accelerated versions on DTLZ1-4 with \(m=2,n=1000,FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.

Figure 4: Illustration of the evolutionary process in solving DTLZ2 and DTLZ4 problems.

is constructed based on how well solutions perform in the objective space. If there isn't a straightforward one-to-one correspondence between the search space and the objective space, such as in multi-modal problems, the learned MLP may not accurately capture the promising directions for improvement, and stacking pre-trained MLPs could potentially hinder the optimization process. Secondly, as the evolutionary process continues, the distinctions between different solutions tend to diminish, making the learned models progressively less helpful in aiding the optimization process.

### Comparison with state-of-the-art LMOEAs

To further evaluate the effectiveness of our DIR-based algorithms, namely LNSGAV4-V5 and LMOEADV4-5, we do a comparative analysis against five state-of-the-art LMOEAs (CCGDE3 (Antonio and Coello, 2013), LMOCSO (Tian et al., 2019), DGEA (He et al., 2020a), FDV (Yang et al., 2021), and MOEA/PSL (Tian et al., 2020)) representing different categories in solving 3-objective DTLZ1 to DTLZ7 problems. These competitors span a range of existing LMOEA approaches. The Table 9 in appendix contains the average IGD results for all considered solvers tackling these seven problems. These results clearly highlight the struggles most existing LMOEA competitors face when dealing with these large-scale DTLZ benchmarks. In contrast, our proposed optimizers, LNSGAV4-V5 and LMOEADV4-5, which employ deep accelerated evolutionary search with stacked MLP models, consistently outperform the five competitors when solving six out of seven DTLZ problems, although they do not achieve the best IGD results for DTLZ7. Additionally, Figure 5 illustrates the final solutions obtained by our algorithms for the \(10^{4}\)-dimensional DTLZ2, DTLZ4, DTLZ5, and DTLZ7 problems. These solutions (represented by blue points) closely approximate the true PF (red lines) of the target LMDP.

### Comparison of actual running times

The practical runtimes of accelerated NSGA-II variants and their six competitors are evaluated for computational complexity. Figure 6 displays the average runtime (in seconds: s) for all ten optimizers over 20 runs on the 3-objective DTLZ1 to DTLZ7 problems with \(n=10^{4},FE_{max}=10^{5}\). No

Figure 5: Illustration of the final solutions obtained by our proposed accelerated solvers on DTLZ2, DTLZ4, DTLZ5, and DTLZ7 with \(m=3,n=10^{4},FE_{max}=10^{5}\).

Figure 6: Illustration of the average running time (s) that each solver cost.

tably, LNSGAV1 to LNSGAV5 exhibit similar runtimes to NSGA-II and most compared LMOEAs, suggesting that the lightweight MLP model's computational overhead in these learnable EAs is manageable. In contrast, MOEAPSL, utilizing a larger model and more training epochs, not only performs suboptimally but also incurs a higher computational cost. The underperformance of MOEA/PSL may also stem from its reliance on autoencoder-based learning, which limits its ability to acquire improvement representations of solutions.

### Parameter sensitivity analysis

We do sensitivity analysis on the number of stacked MLP models (\(t\)) for LNSGAV4 and LMOEAD-V4. Average IGD results in Figure 7 show that \(t=3\) yields best overall performance, with diminishing returns beyond this value. Additionally, we analyze the number of hidden layer nodes (\(K\)) in the MLP model for LNSGAV1 and LMOEADV1, revealing that \(K=5\) and \(K=10\) perform well, except for DTLZ7, where larger \(K\) values more are advantageous. This is likely because lighter models are easier to train and perform better.

### Optimization of real-world LMOPs

We also tested our proposed algorithms on practical LMOPs, particularly the time-varying ratio error estimation (TREE) problems related to voltage transformers (He et al., 2020). The results, summarized in Table 2, indicate that our algorithms with deep accelerated evolutionary search outperform the competitors across all five TREE problems in terms of HV scores.

## 5 Conclusions

This study proposes novel strategies to enhance evolutionary algorithms for LMOPs. Key contributions involve creating a lightweight model for learning improvement representations, assessing the impact of learnable evolutionary search, and designing a large model for deep improvement representation, all with the goal of efficient LMOP optimization. However, the method has limitations, including reliance on training data, limited effectiveness in multimodal problems, optimization instability, and short-term speed improvements.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline Solvers & TREE1-3000 & TREE2-3000 & TREE3-6000 & TREE4-6000 & TREE5-6000 \\ \hline NSGAII & 6.095e-1(5.4e-3) & 6.691e-1(4.6e-3) & NaN(NaN) & NaN(NaN) & NaN(NaN) \\ \hline MOEAD/ & 7.523e-1(3.0e-3) & 7.788e-1(3.6e-3) & 7.268e-1(8.5e-3) & 1.045e-1(6.8e-2) & 6.807e-1(3.9e-3) \\ \hline CCGD23 & Na(NaN) & NaN(NaN) & NaN(NaN) & NaN(NaN) & NaN(NaN) \\ \hline LMOCSO & 8.063e-1(8.3e-3) & 7.876e-1(3.6e-3) & NaN(NaN) & 0.00e+0(0.0e+0) & NaN(NaN) \\ \hline DGEA & 7.928e-1(3.6e-2) & 7.999e-1(1.2e-2) & 6.543e-1(2.6e-1) & 4.719e-1(4.0e-1) & 7.457e-1(2.4e-1) \\ \hline FDV & 7.117e-1(5.0e-2) & 7.720e-1(4.8e-3) & NaN(NaN) & NaN(NaN) & NaN(NaN) \\ \hline MOEA/PSL & 8.141e-1(1.7e-2) & 8.096e-1(5.3e-2) & 8.744e-1(2.3e-2) & 7.942e-1(1.86e-1) & 8.853e-1(5.19e-2) \\ \hline LNSGAV5 & 8.115e-1(3.2e-2) & **8.34e-1(9.5e-2)** & 8.745e-1(1.5e-2) & 9.525e-1(1.9e-2) & 8.967e-1(2.3e-2) \\ \hline LNSGAV6 & **8.36e-1(1.8e-2)** & 8.164e-1(3.9e-2) & **8.86e-1(1.5e-4)** & 9.212e-1(5.7e-2) & **9.21e-1(2.5e-3)** \\ \hline LMEADV5 & 8.153e-1(5.9e-2) & 7.954e-1(4.3e-2) & 8.736e-1(1.6e-2) & **9.57e-1(2.8e-3)** & 8.834e-1(7.8e-2) \\ \hline LMEADV6 & 7.824e-1(6.6e-2) & 8.058e-1(3.8e-2) & 8.828e-1(4.5e-3) & 9.021e-1(3.8e-1) & 9.116e-1(1.3e-2) \\ \hline \end{tabular}
\end{table}
Table 2: Average HV results of selected algorithms in solving real-world TREE problems

Figure 7: Illustration of the sensitivity analysis for two parameters \(t\) and \(K\).

## References

* Antonio and Coello Coello (2013) Luis Miguel Antonio and Carlos A Coello Coello. Use of cooperative coevolution for solving large scale multiobjective optimization problems. In _2013 IEEE Congress on Evolutionary Computation_, pp. 2758-2765. IEEE, 2013.
* Bandaru and Deb (2010) Sunith Bandaru and Kalyanmoy Deb. Automated discovery of vital knowledge from pareto-optimal solutions: First results from engineering design. In _Ieee congress on evolutionary computation_, pp. 1-8. IEEE, 2010.
* Boelrijk et al. (2022) Jim Boelrijk, Bernd Ensing, and Patrick Forre. Multi-objective optimization via equivariant deep hypervolume approximation. In _The Eleventh International Conference on Learning Representations_, 2022.
* Bonissone et al. (2006) Piero P Bonissone, Raj Subbu, Neil Eklund, and Thomas R Kiehl. Evolutionary algorithms+ domain knowledge= real-world evolutionary computation. _IEEE Transactions on Evolutionary Computation_, 10(3):256-280, 2006.
* Chugh et al. (2019) Tinkle Chugh, Karthik Sindhya, Jussi Hakanen, and Kaisa Miettinen. A survey on handling computationally expensive multiobjective optimization problems with evolutionary algorithms. _Soft Computing_, 23:3137-3166, 2019.
* Coello et al. (2020) Carlos A Coello Coello et al. Evolutionary multiobjective optimization: open research areas and some challenges lying ahead. _Complex & Intelligent Systems_, 6:221-236, 2020.
* Deb & Srinivasan (2006) Kalyanmoy Deb and Aravind Srinivasan. Innovization: Innovating design principles through optimization. In _Proceedings of the 8th annual conference on Genetic and evolutionary computation_, pp. 1629-1636, 2006.
* Deb et al. (2002) Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems for evolutionary multiobjective optimization. In _Evolutionary multiobjective optimization: theoretical advances and applications_, pp. 105-145. Springer.
* Deb et al. (2002) Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. _IEEE transactions on evolutionary computation_, 6(2):182-197, 2002.
* Del Ser et al. (2019) Javier Del Ser, Eneko Osaba, Daniel Molina, Xin-She Yang, Sancho Salcedo-Sanz, David Camacho, Swagatam Das, Ponnuthurai N Suganthan, Carlos A Coello Coello, and Francisco Herrera. Bio-inspired computation: Where we stand and what's next. _Swarm and Evolutionary Computation_, 48:220-250, 2019.
* Duan et al. (2023) Qiqi Duan, Chang Shao, Guochen Zhou, Haobin Yang, Qi Zhao, and Yuhui Shi. Cooperative coevolution for non-separable large-scale black-box optimization: Convergence analyses and distributed accelerations. _arXiv preprint arXiv:2304.05020_, 2023.
* Gaur & Deb (2017) Abhinav Gaur and Kalyanmoy Deb. Effect of size and order of variables in rules for multi-objective repair-based innovization procedure. In _2017 IEEE Congress on Evolutionary Computation (CEC)_, pp. 2177-2184. IEEE, 2017.
* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.
* Guliashki et al. (2009) Vassil Guliashki, Hristo Toshev, and Chavdar Korsemov. Survey of evolutionary algorithms used in multiobjective optimization. _Problems of engineering cybernetics and robotics_, 60(1):42-54, 2009.
* Gulotta (2023) Dario Paolo Gulotta. _Real time, dynamic cloud offloading for self-driving vehicles with secure and reliable automatic switching between local and edge computing_. PhD thesis, Politecnico di Torino, 2023.
* He et al. (2020a) Cheng He, Ran Cheng, and Danial Yazdani. Adaptive offspring generation for evolutionary large-scale multiobjective optimization. _IEEE Transactions on Systems, Man, and Cybernetics: Systems_, 52(2):786-798, 2020a.

* He et al. (2020) Cheng He, Ran Cheng, Chuanji Zhang, Ye Tian, Qin Chen, and Xin Yao. Evolutionary large-scale multiobjective optimization for ratio error estimation of voltage transformers. _IEEE Transactions on Evolutionary Computation_, 24(5):868-881, 2020b.
* Ishibuchi et al. (2015) Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation in generational distance and inverted generational distance. In _Evolutionary Multi-Criterion Optimization: 8th International Conference, EMO 2015, Guimaraes, Portugal, March 29-April 1, 2015. Proceedings, Part II 8_, pp. 110-125. Springer, 2015.
* Khan et al. (2002) Nazan Khan, David E Goldberg, and Martin Pelikan. Multi-objective bayesian optimization algorithm. In _Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation_, pp. 684-684, 2002.
* Li et al. (2023) Bin Li, Ziping Wei, Jingjing Wu, Shuai Yu, Tian Zhang, Chunli Zhu, Dezhi Zheng, Weisi Guo, Chenglin Zhao, and Jun Zhang. Machine learning-enabled globally guaranteed evolutionary computation. _Nature Machine Intelligence_, pp. 1-11, 2023.
* Lin et al. (2019) Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. _Advances in neural information processing systems_, 32, 2019.
* Liu et al. (2020) Ruochen Liu, Rui Ren, Jin Liu, and Jing Liu. A clustering and dimensionality reduction based evolutionary algorithm for large-scale multi-objective problems. _Applied Soft Computing_, 89:106120, 2020.
* Liu (2022) Songbai Liu. A survey on learnable evolutionary algorithms for scalable multiobjective optimization. _arXiv preprint arXiv:2206.11526_, 2022.
* Liu et al. (2021) Songbai Liu, Qizhen Lin, Qing Li, and Kay Chen Tan. A comprehensive competitive swarm optimizer for large-scale multiobjective optimization. _IEEE Transactions on Systems, Man, and Cybernetics: Systems_, 52(9):5829-5842, 2021.
* Liu et al. (2022) Songbai Liu, Min Jiang, Qizhen Lin, and Kay Chen Tan. Evolutionary large-scale multiobjective optimization via self-guided problem transformation. In _2022 IEEE Congress on Evolutionary Computation (CEC)_, pp. 1-8. IEEE, 2022.
* Ma et al. (2021) Lianbo Ma, Nan Li, Yinan Guo, Xingwei Wang, Shengxiang Yang, Min Huang, and Hao Zhang. Learning to optimize: reference vector reinforcement learning adaption to constrained many-objective optimization of industrial copper burdening system. _IEEE Transactions on Cybernetics_, 2021.
* Michalski (2000a) Ryszard S Michalski. Learnable evolution model: Evolutionary processes guided by machine learning. _Machine learning_, 38:9-40, 2000a.
* Michalski (2000b) Ryszard S Michalski. Learning and evolution: An introduction to non-darwinian evolutionary computation. In _International Symposium on Methodologies for Intelligent Systems_, pp. 21-30. Springer, 2000b.
* Michalski et al. (2007) Ryszard S Michalski, Janusz Wojtusiak, and Kenneth Kaufman. Progress report on the learnable evolution model. 2007.
* Miikkulainen & Forrest (2021) Risto Miikkulainen and Stephanie Forrest. A biological perspective on evolutionary computation. _Nature Machine Intelligence_, 3(1):9-15, 2021.
* Mittal et al. (2020) Sukrit Mittal, Dhish Kumar Saxena, and Kalyanmoy Deb. Learning-based multi-objective optimization through ann-assisted online innovization. In _Proceedings of the 2020 genetic and evolutionary computation conference companion_, pp. 171-172, 2020.
* Mittal et al. (2021a) Sukrit Mittal, Dhish Kumar Saxena, Kalyanmoy Deb, and Erik D Goodman. Enhanced innovized progress operator for evolutionary multi-and many-objective optimization. _IEEE Transactions on Evolutionary Computation_, 26(5):961-975, 2021a.
* Mittal et al. (2021b) Sukrit Mittal, Dhish Kumar Saxena, Kalyanmoy Deb, and Erik D Goodman. A learning-based innovized progress operator for faster convergence in evolutionary multi-objective optimization. _ACM Transactions on Evolutionary Learning and Optimization (TELO)_, 2(1):1-29, 2021b.

* Omidvar et al. (2021) Mohammad Nabi Omidvar, Xiaodong Li, and Xin Yao. A review of population-based metaheuristics for large-scale black-box global optimization-part i. _IEEE Transactions on Evolutionary Computation_, 26(5):802-822, 2021.
* Ou et al. (2022) Wen-Jie Ou, Xuan-Li Shi, and Wei-Neng Chen. A distributed cooperative co-evolutionary algorithm-m based on ring network for distributed large-scale optimization. In _2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)_, pp. 3018-3025. IEEE, 2022.
* Qian & Yu (2017) Hong Qian and Yang Yu. Solving high-dimensional multi-objective optimization problems with low effective dimensions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* Thanh et al. (2023) Bui Tien Thanh, Dinh Van Tuan, Tuan Anh Chi, Nguyen Van Dai, Nguyen Tai Quang Dinh, and Nguyen Thuy. Multiobjective logistics optimization for automated atm cash replenishment process. _arXiv preprint arXiv:2304.13671_, 2023.
* Tian et al. (2019) Ye Tian, Xiutao Zheng, Xingyi Zhang, and Yaochu Jin. Efficient large-scale multiobjective optimization based on a competitive swarm optimizer. _IEEE Transactions on Cybernetics_, 50(8):3696-3708, 2019.
* Tian et al. (2020) Ye Tian, Chang Lu, Xingyi Zhang, Kay Chen Tan, and Yaochu Jin. Solving large-scale multiobjective optimization problems with sparse optimal solutions via unsupervised neural networks. _IEEE transactions on cybernetics_, 51(6):3115-3128, 2020.
* Tian et al. (2023) Ye Tian, Weijian Zhu, Xingyi Zhang, and Yaochu Jin. A practical tutorial on solving optimization problems via platemo. _Neurocomputing_, 518:190-205, 2023.
* Wang et al. (2021a) Yutong Wang, Ke Xue, and Chao Qian. Evolutionary diversity optimization with clustering-based selection for reinforcement learning. In _International Conference on Learning Representations_, 2021a.
* Wang et al. (2021b) Zhenzhong Wang, Haokai Hong, Kai Ye, Guang-En Zhang, Min Jiang, and Kay Chen Tan. Manifold interpolation for large-scale multiobjective optimization via generative adversarial networks. _IEEE Transactions on Neural Networks and Learning Systems_, 2021b.
* Wu et al. (2023) Kai Wu, Penghui Liu, and Jing Liu. Lea: Beyond evolutionary algorithms via learned optimization strategy. _arXiv preprint arXiv:2304.09599_, 2023.
* Yang et al. (2021) Xu Yang, Juan Zou, Shengxiang Yang, Jinhua Zheng, and Yuan Liu. A fuzzy decision variables framework for large-scale multiobjective optimization. _IEEE Transactions on Evolutionary Computation_, 2021.
* Zhang et al. (2011) Jun Zhang, Zhi-hui Zhan, Ying Lin, Ni Chen, Yue-jiao Gong, Jing-hui Zhong, Henry SH Chung, Yun Li, and Yu-hui Shi. Evolutionary computation meets machine learning: A survey. _IEEE Computational Intelligence Magazine_, 6(4):68-75, 2011.
* Zhang & Li (2007) Qingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition. _IEEE Transactions on evolutionary computation_, 11(6):712-731, 2007.
* Zhang et al. (2023) Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. Automl-gpt: Automatic machine learning with gpt. _arXiv preprint arXiv:2305.02499_, 2023.
* Zhao et al. (2022) Yiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo, and Yuandong Tian. Multi-objective optimization by learning space partitions. In _International Conference on Learning Representations (ICLR'22)_, 2022.
* Zhong et al. (2022) Rui Zhong, Enzhi Zhang, and Masaharu Munetomo. Accelerating the genetic algorithm for large-scale traveling salesman problems by cooperative coevolutionary pointer network with reinforcement learning. _arXiv preprint arXiv:2209.13077_, 2022.
* Zille et al. (2016) Heiner Zille, Hisao Ishibuchi, Sanaz Mostaghim, and Yusuke Nojima. Weighted optimization framework for large-scale multi-objective optimization. In _Proceedings of the 2016 on genetic and evolutionary computation conference companion_, pp. 83-84, 2016.

Appendix

We provide more discussion, details on the proposed methods, experimental results and analysis in this appendix.

### learnable multiobjective evolutionary algorithms

The conventional evolutionary generator and discriminator (or selector) in a typical MOEA are constructed using fixed genetic operators, such as crossover, mutation, and selection. Consequently, they lack the capability to learn and adapt to the specific characteristics of the optimization problem they are tasked with. As a result, they cannot effectively respond to the potential challenges posed by solving such a black-box optimization problem. In the context of evolutionary computation, research studies focused on learnable multi-Objective Evolutionary Algorithms (MOEAs) have garnered substantial attention. Machine learning (ML) techniques are leveraged to support and enhance various modules within learnable MOEAs, including the generator, and discriminator (or selector).

Specifically, the MOEA's generator iteratively explores the variable space, generating a significant volume of data comprising feasible solutions. ML techniques systematically analyze this data to gain insights into the search behavior and enhance its future search capabilities. By traversing promising directions learned within the search space, the MOEA's generator efficiently identifies solutions with high potential (Michalski, 2000; Michalski et al., 2007).

The MOEA's discriminator benefits from online predictions of Pareto Front (PF) shapes, enabling it to adeptly filter out underperforming solutions when dealing with MOPs featuring irregular PFs. Prior to tackling these problems, dimension reduction and spatial transformation techniques simplify both the objective and search spaces.

Furthermore, reinforcement learning (RL) techniques come into play in determining suitable evolutionary operators (i.e., actions) based on the current parent state. These actions guide the generator in producing high-quality offspring. Domain adaptation techniques are employed to learn domain-invariant feature representations across different optimization problems, enabling the analysis of distribution divergence. This knowledge transfer facilitates the sequential or simultaneous solution of these problems (Ma et al., 2021).

Bayesian optimization is an optimization technique that uses probabilistic models, like Gaussian Processes, to efficiently optimize complex, expensive functions. It sequentially samples the function, updates the surrogate model, and selects the next sampling point using an acquisition function (Khan et al., 2002). This process balances exploration and exploitation to find the global optimum. However, Bayesian optimization has limitations: it can be computationally expensive, struggles with high-dimensional spaces, assumes smooth functions, depends on the quality of the initial model, and may converge to local optima for multimodal functions.

### online innovization operators

To enhance the search capability when addressing LMOPs, certain MOEAs have developed competitive learning-based search strategies. In these strategies, the population is divided into two groups: winners and losers, and the search process is guided so that the losers move closer to the winners. Typically, these competitions occur within the objective space. The effectiveness of these strategies heavily relies on the quality of the winners, as losers are guided towards them through competitive learning. If the winners encounter difficulties, such as falling into local optima, the entire evolutionary population may experience slow convergence. Furthermore, winners, often subjected to genetic operators, may only exhibit slight improvements. Consequently, the challenge lies in determining how these higher-quality winners can further evolve with a faster convergence rate.

To address this challenge and expedite the search process, efforts have been made through the design of online innovization operators (Deb & Srinivasan, 2006). The term "innovization" is derived from "innovation via optimization" and was originally defined as a post-optimality analysis of optimal solutions to provide valuable design principles to engineering designers. In the context of MOEAs, offspring generated by genetic operators are further refined by innovization operators to progress along the learned directions of performance improvement (Gaur & Deb, 2017). In the innovization process, various data-mining and machine learning techniques are employed to automatically uncover innovative and crucial design principles within optimal solutions. These principles may include inter-variable relationships, commonalities among optimal solutions, and distinctions that set them apart from one another (Mittal et al., 2020). These operators enable the population to converge faster without consuming additional function evaluations compared to traditional local search methods.

Expanding on the concept of innovization, the notion of knowledge-driven optimization is introduced (Bandaru and Deb, 2010). In this approach, MOEAs assimilate knowledge, such as latent patterns shared by high-quality solutions, learned from intermediate solutions to guide their search away from mediocre solutions and toward potentially promising regions. This constitutes an online learning process that involves deciphering what makes one solution optimal (or near-optimal) within the final solution set and understanding what causes one solution to outperform another during the optimization process. Online innovation aims to accelerate evolutionary search and enhance the efficiency of generators.

In an online innovization operator, a supervised learning model is typically constructed and trained online with the aim of implicitly learning historical directional improvements, such as transitioning from dominated to non-dominated solutions, within the previously explored search space. A solution \(x=(x_{1},x_{2},\dots,x_{n})\) is identified as performing poorly, while \(x^{*}\) represents a high-quality solution. Throughout the evolutionary process, various solution pairs \((x,x^{*})\) can be collected. Subsequently, the selected model, which can be a multilayer perceptron, a random forest, or a deep neural network, is trained using this labeled data. In this context, \(x\) serves as the input, and \(x^{*}\) serves as its label or expected target output. The trained model is believed to have the capacity to capture underlying patterns that reflect the directional improvement of solutions within the search space.

Ideally, a newly generated offspring solution, \(x^{new}\), produced by genetic operators can be enhanced (or progressed) by inputting it into the well-trained model to obtain an improved version, \(y^{new}\), as the output. This process of repairing offspring is also referred to as innovized progress. It holds the potential to enhance the search capability of generators when tackling scalable MOPs, primarily due to the following four merits: 1) Incorporation of all conflicting objective information during the learning process. 2) Elimination of the need for additional function evaluations during the innovized progress of solutions. 3) Adaptability of the learned directional improvement of solutions as generations progress. 4) The potential for a substantial leap in the objective space when transitioning from \(x^{new}\) to \(y^{new}\) in the search space, which can expedite the search process. However, it's crucial to consider four key considerations when customizing such an innovization progress:

**Selection of the learning model:** The choice of the learning model is flexible and can align with available supervised learning models that meet the requirements. However, it's essential to take into account the computational cost associated with training the selected model.

**Collection of training data:** The process of collecting training data involves gathering paired data based on the performance of available solutions, either from previous generations or the current one. Therefore, when selecting a pair \((x,x^{*})\), it is crucial to consider that \(x^{*}\) outperforms \(x\). For example, in a Pareto-based MOEA, \(x^{*}\) should dominate \(x\), in a decomposition-based MOEA, the aggregation fitness of \(x^{*}\) should be superior to that of \(x\), or \(x^{*}\) should have the potential to guide \(x\) towards rapid convergence.

**Training of the adopted model:** The process of training a model is itself an optimization problem, involving numerous hyperparameters such as model architecture, learning rate, and training epochs, which often require manual tuning. This can lead to various challenges, including the risk of overfitting the model. Additionally, it's essential to investigate whether the model should be updated regularly, such as every generation, and whether training should be conducted online or offline.

**Advancement of the search capability with the learned model:** The expectation is that the generator's search capability can be enhanced with the assistance of this well-trained model. Specifically, subpar and average solutions within the population can be repaired, facilitating rapid convergence in the learned promising direction. Simultaneously, high-quality solutions can be further improved to explore a broader range of elite solutions. However, two important considerations arise: Is it necessary to repair all newly generated solutions? Is it necessary to perform the innovization progress in every generation?

**Autoencoder-based representation learning:** The concept of representation learning has been previously introduced in MOEA/PSL, where it focuses on acquiring a compressed representation of the input solution, referred to as Pareto Subspace Learning (PSL). This approach involves training a Denoise Autoencoder (DAE), a specific type of artificial neural network with equal nodes in the input and output layers. The DAE comprises two main components: an encoder and a decoder. The encoder learns to create a representation (or code) of the partially corrupted input, while the decoder maps this representation back to a reconstructed input. Through this process, the DAE can extract higher-level features from the input distribution.

During training, the DAE iteratively minimizes the reconstruction error, which measures the disparity between the output and the input. This training process resembles that of a feedforward neural network. To elaborate, each input undergoes perturbation via the mutation operator, and the non-dominated solutions within the current population serve as the training data.

Following training, each solution can be mapped between the original search space and the code representation space. Subsequently, a new offspring, denoted as \(y\), can be generated through the following process:

**Step 1:** select two random parents \(x^{1}\) and \(x^{2}\) from the current population.

**Step 2:** map \(x^{1}\) and \(x^{2}\) to the code space to get their corresponding representations \(c^{1}\) and \(c^{2}\). The value of each \(c^{1}_{i}\in c^{1}\) (the same as \(c^{1}\)) in the code layer can be computed as follows,

\[c_{i}=\sigma\left(b_{i}+\sum_{i}x_{i}w_{ij}\right) \tag{5}\]

**Step 3:** run evolutionary search (e.g., SBX, DE, PM) on \(c^{1}\) and \(c^{2}\) to generate a new code \(c\);

**Step 4:** map \(c\) back to the original space for getting the new solution \(y\) and the value of each \(y_{j}\in y\) can be computed by

\[y_{j}=\sigma\left(b^{\prime}_{j}+\sum_{j}c_{i}w^{\prime}_{ji}\right) \tag{6}\]

where \(b\) and \(w\) are respectively the bias and the weight of this DAE with only one hidden layer, while \(\sigma\) represents the sigmoid function.

### Pseudocode for this to work

Here is the pseudo-code of the algorithms designed in this paper, including the general MOEA algorithm framework and our two proposed improved learnable MOEA algorithm frameworks.

**Algorithm 1** is the basic MOEA algorithm framework. It differs from Algorithms 2 and 3 in the way solutions are generated.

**Algorithm 2** corresponds to our proposed algorithms LNSGAV1, LNSGAV4, LMOEADV1, LMOEADV4. **Algorithm 3** corresponds to our proposed algorithms LNSGAV2, LNSGAV3, LNSGAV5, LMOEADV2, LMOEADV3, LMOEADV5. Among these variants, LNSGAV2 and LMOEADV2 utilize the learned compressed representation space for searching, employing SBX and DE, respectively. Variants (LNSGAV1, LNSGAV3) and (LMOEADV1, LMOEADV3) leverage the learned improvement representation space for their search operations, employing SBX and DE, respectively. Finally, variants (LNSGAV4, LNSGAV5) and (LMOEADV4, LMOEADV5) utilize the learned deep improvement representation space for their search, employing SBX and DE, respectively. The detailed configuration is listed in Table 3.

In this study, the process of using SBX to generate a child solution is as follows:

**Step 1:** Randomly select two different parent solutions: \(x^{1}=\left(x^{1}_{1},\cdots,x^{1}_{n}\right)\) and \(x^{2}=\left(x^{2}_{1},\cdots,x^{2}_{n}\right)\) from the current population \(P\);

**Step 2:** generate a child solution \(c=\left(c_{1},\cdots,c_{n}\right)\), where \(c_{i}\) is computed as follows:

\[c_{i}=0.5\times\left[(1+\beta)\cdot x^{1}_{i}+(1-\beta)\cdot x^{2}_{i}\right] \tag{7}\]

where \(\beta\) is dynamically computed as follows:

\[\beta=\begin{cases}(\text{rand }\times 2)^{1/(1+\eta)}&\text{rand }\leq 0.5\\ (1/(2-\text{rand }\times 2))^{1/(1+\eta)}&\text{otherwise.}\end{cases} \tag{8}\]where \(\eta\) is a hyperparameter (the spread factor distribution index), which is set as 20. The greater the value of \(\eta\), the greater the probability that the resulting child solution will be close to the parent.

The DE/rand/l operator is used in this study. For each solution \(x\in P\), the process of using DE to generate a child solution of \(x\) is as follows:

**Step 1:** Pick two solutions \(x^{1}\) and \(x^{2}\) from the population \(P\) at random, they must be distinct from each other as well as from the base vector \(x\).

**Step 2:** The mutated individual \(v\) is obtained according to the following formula:

\[v_{i}=x_{i}+F(x_{i}^{1}-x_{i}^{2}) \tag{9}\]

**Step 3:** The final individual \(c\) is obtained by crossover according to the following formula

\[c_{i}=\begin{cases}v_{i}&\text{if rand}_{i}[0,1]\leq CR\text{ or }i=k\\ x_{i}&\text{Otherwise}\end{cases} \tag{10}\]

In this study, we set \(F=0.5\) and \(CR=0.75\).

```
Input: the LMOP with \(m\) objectives and \(n\) variables, the function evaluation budget \(FE_{max}\) Output: the final population \(P\) to approximate the PF/PS  initialize \(P\) with \(N\) random solutions;  initialize the function evaluation counter \(FE=0\); while\(FE\leq FE_{max}\)do \(Q\) = Generator(\(P\));//evolutionary search in variable space to find new offspring solutions \(P\) = Selector(P, Q);//environmental selection in objective space to filter poor solutions. \(FE=FE+N\);  end while return\(P\)
```

**Algorithm 1**The general framework of an MOEA

### Time-Varying Ratio Error Estimation

The precise estimation of voltage transformers' (VTs) ratio error (RE) holds significant importance in modern power delivery systems. Existing RE estimation methods predominantly revolve around periodic calibration, disregarding the time-varying aspect. This oversight presents challenges in achieving real-time VT state estimation. To address this concern, the formulation of a time-varying RE estimation (TREE) problem as a large-scale multiobjective optimization problem is proposed in (He et al., 2020). Multiple objectives and inequality constraints are defined based on statistical and physical rules extracted from power delivery systems. Additionally, a benchmark test suite is

\begin{table}
\begin{tabular}{c|c|c} \hline Algorithms & Evolutionary search of the Generator & Environmental selection \\ \hline NSGA-II & SBX in the original space & Pareto-based selection \\ \hline \multirow{2}{*}{LNSGA-V1} & SBX-based search in the original space followed by & Pareto-based selection \\  & reparing part of offspring with MLP & Pareto-based selection \\ \hline \multirow{2}{*}{LNSGA-V2} & SBX-based search in the original space + SBX in & Pareto-based selection \\  & the compressed representation space & Pareto-based selection \\ \hline \multirow{2}{*}{LNSGA-V3} & SBX-based search in the original space + SBX in & Pareto-based selection \\  & the improvement representation space & Pareto-based selection \\ \hline \multirow{2}{*}{LNSGA-V4} & SBX-based search in the original space followed by & Pareto-based selection \\  & repairing part of offspring with stacked MLP & Pareto-based selection \\ \hline \multirow{2}{*}{LNSGA-V5} & SBX-based search in the original space + SBX in & Pareto-based selection \\  & the deep improvement representation space & Pareto-based selection \\ \hline MOEA/D & DE in the original space & Decomposition-based selection \\ \hline \multirow{2}{*}{LMOEAD-V1} & DE-based search in the original space followed by & Decomposition-based selection \\  & reparing part of offspring with MLP & Decomposition-based selection \\ \hline \multirow{2}{*}{LMOEAD-V2} & DE-based search in the original space + DE in & Decomposition-based selection \\  & the compressed representation space & Decomposition-based selection \\ \hline \multirow{2}{*}{LMOEAD-V3} & DE-based search in the original space + DE in & Decomposition-based selection \\  & the improvement representation space & Decomposition-based selection \\ \hline \multirow{2}{*}{LMOEAD-V4} & DE-based search in the original space followed by repairing part of offspring with stacked MLP & Decomposition-based selection \\ \hline \multirow{2}{*}{LMOEAD-V5} & DE-based search in the original space + DE in & Decomposition-based selection \\  & the deep improvement representation space & Decomposition-based selection \\ \hline \end{tabular}
\end{table}
Table 3: The search and selection strategy configuration of our proposed algorithmssystematically created, encompassing various TREE problems from different substations to depict their distinct characteristics. This formulation not only transforms a costly RE estimation task into a more economical optimization problem but also contributes to the advancement of research in large-scale multiobjective optimization by providing a real-world benchmark test suite featuring intricate variable interactions and objective correlations. The source code for these optimization problems can be found on the PlatEMO.

### Supplementary Experimental Studies

Due to space limitations, a supplement of some experimental data from this work is provided here. Mainly are the average IGD and HV results of each algorithm in solving the DTLZ problem with different settings.

#### a.5.1 Future research directions

**Enhancing Evolutionary Selectors or Discriminators Through Machine Learning:** In the context of Many-Objective Optimization Problems (MaOPs), the application of machine learning tech

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metric & Problem & NSGA-II & LNSGA-V1 & LNSGA-V2 & LNSGA-V3 & LNSGA-V4 & LNSGA-V5 \\ \hline \multirow{2}{*}{IGD} & \multirow{2}{*}{DTLZ1} & 4.477e+3 & **7.366e+0** & 6.010e+1 & 4.143e+2 & 1.957e+1 & 1.913e+3 \\  & & (5.3e+1) & **(9.6e+0)** & (5.8e+1) & (1.2e+2) & (2.0e+1) & (5.3e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 2.004e+0 & 1.291e-2 & 9.554e-2 & 4.820e-2 & 4.985e-3 & **4.849e-3** \\  & & (1.8e-1) & (4.7e-2) & (3.2e-2) & (2.6e-2) & (7.8e-3) & **(3.0e-3)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 1.144e+4 & **7.236e-1** & **7.576e-1** & 1.767e+2 & 2.453e+2 & 1.258e+3 \\  & & (3.9e-2) & (**7.6e-1)** & **(4.5e-1)** & (1.2e+2) & (7.4e+2) & (5.0e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 2.935e+0 & 1.652e-1 & 7.123e-1 & 8.984e-2 & 1.286e-2 & **6.248e-3** \\  & & (1.2e-1) & (3.4e-1) & (2.9e-1) & (1.9e-1) & (2.6e-2) & **(8.4e-3)** \\ \hline \multirow{2}{*}{HV} & \multirow{2}{*}{DTLZ1} & 0.00e+0 & **2.938e-1** & 0.00e+0 & 0.00e+0 & 0.00e+0 & 0.00e+0 \\  & & (0.0e+0) & **(3.0e-1)** & (0.0e+0) & (0.0e+0) & (0.0e+0) & (0.0e+0) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 0.00e+0 & 3.230e-1 & 1.830e-1 & 2.760e-1 & 3.429e-1 & **3.457e-1** \\  & & (0.0e+0) & (1.6e-1) & (4.6e-2) & (1.4e-1) & (1.2e-1) & **(1.0e-1)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 0.00e+00 & 9.807e-2 & **1.197e-1** & 0.00e+0 & 0.00e+0 & 0.00e+0 \\  & & (0.0e+0) & (4.4e-2) & (**6.3e-1**) & (0.0e+0) & (0.0e+0) & (0.0e+0) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 0.00e+00 & 1.716e-1 & 9.751e-2 & 2.134e-1 & 1.659e-1 & **3.046e-1** \\  & & (0.0e+0) & (1.2e-1) & (1.2e-1) & (1.4e-1) & (1.4e-1) & **(1.4e-1)** \\ \hline \end{tabular}
\end{table}
Table 4: Average IGD and HV results of NSGA-II and its five accelerated versions on DTLZ1-4 with \(m=2,n=1000,FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metric & Problem & NSGA-II & LNSGA-V1 & LNSGA-V2 & LNSGA-V3 & LNSGA-V4 & LNSGA-V5 \\ \hline \multirow{2}{*}{IGD} & \multirow{2}{*}{DTLZ1} & 7.274e+4 & **1.823e+0** & 5.372e+0 & 1.149e+2 & 1.647e+2 & 2.706e+2 \\  & & (1.7e+3) & **(4.4e+0)** & (7.2e+0) & (9.3e+3) & (4.0e+3) & (3.3e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 1.514e+2 & 1.919e-2 & 7.376e+0 & 1.403e-2 & 1.017e-2 & **8.503e-3** \\  & & (6.3e-0) & (3.2e-2) & (1.7e+1) & (2.9e-2) & (4.0e-2) & (**6.2e-3)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 1.931e+5 & **3.447e+3** & **9.285e-3** & 1.261e+4 & 4.499e+3 & 3.657e+3 \\  & & (6.6e+3) & **(8.0e+3)** & **(2.0e+4)** & (2.5e+4) & (9.5e+3) & (8.4e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 1.712e+2 & 2.107e-1 & 2.799e-1 & 1.304e-1 & 1.980e-1 & **7.511e-2** \\  & & (1.1e+1) & (2.9e-1) & (3.7e-1) & (2.0e-1) & (2.5e-1) & **(1.8e-1)** \\ \hline \multirow{2}{*}{HV} & \multirow{2}{*}{DTLZ1} & 0.00e+0 & **4.607e-1** & 1.916e-1 & 0.00e+0 & 0.00e+0 & 0.00e+0 \\  & & (0.0e+0) & **(2.3e-1)** & (2.9e-1) & (0.0e+0) & (0.0e+0) & (0.0e+0) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 0.00e+00 & 3.235e-1 & 2.302e-1 & 3.150e-1 & 3.264e-1 & **3.465e-1** \\  & & (0.0e+0) & (4.6e-2) & (1.7e-1) & (1.7e-1) & (1.7e-1) & **(1.6e-1)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 0.00e+00 & 1.153e-1 & **5.784e-2** & 1.153e-1 & 9.413e-2 & 1.141e-1 \\  & & (0.0e+0) & (1.7e-1) & **(1.4e-1)** & (1.7e-1) & (1.8e-1) & (1.3e-1) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 0.00e+00 & 2.181e-1 & 2.198e-1 & 2.692e-1 & 2.472e-1 & **2.851e-1** \\  & & (0.0e+0) & (1.4e-1) & (1.4e-1) & (1.8e-1) & (1.3e-1) & **(1.4e-1)** \\ \hline \end{tabular}
\end{table}
Table 5: Average IGD and HV results of NSGA-II and its five accelerated versions on DTLZ1-4 with \(m=2,n=5000,FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.

niques serves as a subtle yet powerful augmentation to the environmental selection process. This augmentation proves invaluable when confronted with the escalating complexity of objective spaces within MaOPs. As the number of objectives in MaOPs increases, the efficacy of traditional environmental selection strategies in distinguishing subpar solutions from elite ones diminishes significantly. More precisely, the convergent pressure of the discriminator falls short, and its ability to maintain solution diversity becomes inadequate. Consequently, it becomes imperative to explore methods for augmenting the discriminative capabilities of environmental selection strategies when tackling MaOPs.

**Empowering Evolutionary Generators with Machine Learning:** Within the realm of Large-Scale Multi-Objective Problems (LMOPs), the integration of machine learning techniques plays a strategic role in augmenting the evolutionary search process. This augmentation enables a dynamic response to the formidable challenges posed by the expansive search spaces characteristic of LMOPs. In such vast search spaces, the effectiveness of conventional genetic operators markedly declines, resulting in the unfortunate consequence of generating suboptimal offspring by the generator. Hence, it becomes imperative to delve into methods aimed at elevating the search prowess of these generators when tackling LMOPs.

**Advancing Evolutionary Modules Through Learnable Transfer Techniques:** In the realm of multi-objective optimization problems (MOPs), we introduce evolutionary modules employing transfer learning principles to facilitate the exchange of valuable optimization insights between source and target MOPs. In essence, this approach serves as a shortcut to solving target MOPs by leveraging the knowledge acquired from the optimization processes of related source problems. The optimization of source MOPs can be accomplished either concurrently with or prior to addressing the target MOPs, leading to two distinct forms of transfer optimization: sequential and multitasking. In the sequential form, the target MOPs are tackled one after another, benefiting from the cumulative wisdom gleaned from prior optimization exercises. This approach ensures that the experiences garnered from solving earlier problems are effectively applied to optimize subsequent ones. In contrast, the multitasking form involves the simultaneous optimization of all MOPs from the outset, with each problem drawing upon the knowledge cultivated during the optimization of other MOPs. This collaborative optimization approach maximizes the utility of learned knowledge, significantly enhancing the efficiency of solving multiple MOPs simultaneously.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline Metric & Problem & NSGA-II & LNSGA-V1 & LNSGA-V2 & LNSGA-V3 & LNSGA-V4 & LNSGA-V5 \\ \hline \multirow{2}{*}{IGD} & \multirow{2}{*}{DTLZ1} & 1.993e+5 & **3.055e+1** & **2.068e+1** & 7.163e+1 & 4.098e+1 & 4.805e+1 \\  & & (3.8e+3) & **(6.8e+1)** & **(2.3e+1)** & (1.5e+2) & (9.7e+1) & (8.5e+1) \\ \cline{2-7}  & \multirow{2}{*}{DTLZ2} & 4.251e+2 & 8.651e-3 & 9.888e-3 & 1.481e-2 & 8.039e-3 & **7.789e-3** \\
10000 & & (8.1e+0) & (8.7e-3) & (6.2e-3) & (4.2e-2) & (8.2e-3) & **(6.2e-3)** \\ \cline{2-7}  & \multirow{2}{*}{DTLZ3} & 5.616e+5 & **5.040e+0** & **6.319e+2** & 1.415e+2 & 1.115e+3 & 2.994e+2 \\  & & (8.7e+3) & **(1.1e+1)** & **(1.5e+3)** & (1.6e+2) & (2.7e+3) & (4.6e+3) \\ \cline{2-7}  & \multirow{2}{*}{DTLZ4} & 4.414e+2 & 1.686e-1 & 2.826e-1 & 1.060e-1 & 6.481e-2 & **1.045e-2** \\  & & (5.7e+0) & (2.9e-1) & (4.1e-1) & (2.5e-1) & (1.0e-2) & **(1.4e-2)** \\ \hline \multirow{2}{*}{HV} & \multirow{2}{*}{DTLZ1} & 0.00e+0 & **1.934e-1** & 8.803e-2 & 7.176e-2 & **3.269e-1** & 1.514e-1 \\  & & (0.0e+0) & **(3.0e-1)** & (2.1e-1) & (1.7e-1) & **(2.8e-1)** & (2.4e-1) \\ \cline{2-7}  & \multirow{2}{*}{DTLZ2} & 0.00e+0 & 3.407e-1 & 3.425e-1 & 3.152e-1 & 3.385e-1 & **3.438e-1** \\
10000 & & (0.0e+0) & (1.3e-2) & (4.4e-3) & (1.9e-1) & (1.2e-2) & **(1.6e-1)** \\ \cline{2-7}  & \multirow{2}{*}{DTLZ3} & 0.00e+0 & **1.762e-1** & **6.261e-2** & 0.00e+0 & 0.00e+0 & 5.786e-2 \\  & & (0.0e+0) & **(1.7e-1)** & **(1.4e-1)** & (0.0e+0) & (0.0e+0) & (1.4e-1) \\ \cline{2-7}  & \multirow{2}{*}{DTLZ4} & 0.00e+0 & 2.598e-1 & 2.306e-1 & 2.645e-1 & 2.820e-1 & **3.076e-1** \\  & & (0.0e+0) & (1.3e-1) & (1.7e-1) & (1.1e-1) & (1.8e-1) & **(1.6e-1)** \\ \hline \end{tabular}
\end{table}
Table 6: Average IGD and HV results of NSGA-II and its five accelerated versions on DTLZ1-4 with \(m=2,n=10^{4},FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metric & Problem & MOEAD & LMOEADV1 & LMOEADV2 & LMOEADV3 & LMOEADV4 & LMOEADVS \\ \hline \multirow{2}{*}{IGD} & \multirow{2}{*}{DTLZ1} & 2.645e+4 & **3.449e+0** & 5.419e+2 & 1.126e+2 & 1.072e+2 & 5.725e+2 \\  & & (8.4e+3) & **(7.5e+0)** & (1.3e+2) & (1.2e+2) & (2.6e+2) & (1.3e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 2.106e+1 & 9.524e-3 & 8.157e-3 & 1.151e-2 & 6.612e-3 & **6.442e-3** \\  & & (4.1e+0) & (7.7e-4) & (1.8e-4) & (1.9e-2) & (6.4e-3) & **(4.9e-3)** \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 7.016e+4 & 1.705e+2 & **4.303e+1** & 3.896e+3 & 2.942e+43 & 3.05e+43 \\  & & (1.1e+4) & (4.1e+2) & (5.8e+1) & (4.5e+3) & (4.4e+3) & (4.7e+3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 1.433e+1 & 5.325e-1 & 5.145e+0 & 4.159e-1 & 3.105e-1 & **2.053e-1** \\  & & (3.5e+0) & (6.6e-1) & (8.6e+0) & (1.7e+0) & (3.5e-1) & **(4.4e-1)** \\ \hline \multirow{2}{*}{HV} & \multirow{2}{*}{DTLZ1} & 0.00e+0 & **2.036e-1** & 0.00e+0 & 0.00e+0 & 9.560e-2 & 1.508e-2 \\  & & (0.0e+0) & (**1.3e-1**) & (0.0e+0) & (0.0e+0) & (1.4e-1) & (3.7e-2) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 0.00e+00 & 3.01e-1 & 3.149e-1 & 2.269e-1 & **3.471e-1** & 3.449e-1 \\  & & (0.0e+0) & (1.6e-2) & (4.9e-2) & (1.7e-1) & (**8.4e-2**) & (1.7e-1) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 0.00e+0 & **8.548e-2** & 2.126e-2 & 0.00e+0 & 4.454e-2 & 0.00e+0 \\ \cline{2-8}  & & (0.0e+0) & **(6.2e-2)** & (5.1e-2) & (0.0e+0) & (6.4e-2) & (0.0e+0) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 0.00e+00 & 1.996e-1 & 2.153e-1 & 2.670e-1 & 2.749e-1 & **2.815e-1** \\  & & (0.0e+0) & (1.5e-1) & (1.7e-1) & (1.3e-1) & (1.4e-1) & **(1.2e-1)** \\ \hline \end{tabular}
\end{table}
Table 7: Average IGD and HV results of MOEA/D and its five accelerated versions on DTLZ1-4 with \(m=2,n=5000,FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline Metric & Problem & MOEAD & LMOEADV1 & LMOEADV2 & LMOEADV3 & LMOEADV4 & LMOEADVS \\ \hline \multirow{2}{*}{IGD} & \multirow{2}{*}{DTLZ1} & 5.122e+4 & **1.692e-1** & 1.670e+0 & 6.430e+1 & 1.294e+1 & 2.858e+1 \\  & & (1.1e+4) & **(1.8e-2)** & (3.4e+0) & (1.2e+2) & (2.3e+1) & (3.1e+1) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 4.815e+1 & **5.108e-3** & 6.582e-2 & 4.342e-2 & 2.127e-2 & 7.635e-3 \\
10000 & & (6.8e+0) & **(2.2e-4)** & (1.4e-1) & (1.0e-2) & (3.2e-1) & (9.5e-3) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 1.517e+5 & 2.812e+1 & 3.389e+2 & **2.201e+1** & 6.353e+2 & 5.255e+2 \\  & & (6.2e+4) & (6.7e+1) & (8.3e+2) & **(6.5e+1)** & (8.6e-2) & (8.2e+2) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 3.575e+1 & 3.554e-1 & 1.276e+0 & 2.646e-1 & 3.396e-1 & **1.796e-1** \\  & & (3.3e+0) & (3.7e-1) & (1.2e+0) & (7.3e-1) & (4.9e-1) & **(4.0e-1)** \\ \hline \multirow{2}{*}{HV} & \multirow{2}{*}{DTLZ1} & 0.00e+0 & **3.050e-1** & 2.452e-1 & 1.434e-1 & 1.384e-1 & 1.909e-1 \\  & & (0.0e+0) & **(1.7e-2)** & (2.9e-1) & (2.4e-1) & (1.5e-1) & (2.2e-1) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ2} & 0.00e+00 & **3.452e-1** & 2.919e-1 & 2.840e-1 & 3.008e-1 & 3.229e-1 \\  & & (0.0e+0) & (**2.3e-4)** & (1.2e-1) & (1.3e-1) & (1.7e-1) & (1.7e-1) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ3} & 0.00e+00 & 1.062e-1 & 4.207e-2 & **1.209e-1** & 2.087e-2 & 6.192e-2 \\  & & (0.0e+0) & (5.2e-2) & (6.5e-2) & **(1.2e-1)** & (5.1e-2) & (6.8e-2) \\ \cline{2-8}  & \multirow{2}{*}{DTLZ4} & 0.00e+00 & 2.347e-1 & 1.278e-1 & 2.703e-1 & 2.360e-1 & **2.713e-1** \\  & & (0.0e+0) & (1.3e-1) & (1.6e-1) & (1.3e-1) & (1.5e-1) & **(1.7e-1)** \\ \hline \end{tabular}
\end{table}
Table 8: Average IGD and HV results of MOEA/D and its five accelerated versions on DTLZ1-4 with \(m=2,n=10^{4},FE_{max}=10^{5}\). The standard deviation indicated in parentheses following.
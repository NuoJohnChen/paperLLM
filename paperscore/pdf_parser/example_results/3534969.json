{
    "metadata": {
        "figures": [
            {
                "caption": "Table 1. System Parameters",
                "captionBoundary": {
                    "x1": 191.11500549316406,
                    "x2": 294.71185302734375,
                    "y1": 82.86237335205078,
                    "y2": 87.0989990234375
                },
                "figType": "Table",
                "imageText": [
                    "xt",
                    "The",
                    "input",
                    "vector",
                    "x",
                    "at",
                    "timestep",
                    "t",
                    "EP",
                    "Element-based",
                    "Parallelism",
                    "ht",
                    "The",
                    "hidden",
                    "vector",
                    "h",
                    "at",
                    "timestep",
                    "t",
                    "VP",
                    "Vector-based",
                    "Parallelism",
                    "Lx",
                    "The",
                    "number",
                    "of",
                    "elements",
                    "in",
                    "an",
                    "input",
                    "vector",
                    "x",
                    "N",
                    "The",
                    "number",
                    "of",
                    "sub-accelerator",
                    "cores",
                    "W",
                    "Weights",
                    "matrix",
                    "Lh",
                    "The",
                    "number",
                    "of",
                    "elements",
                    "in",
                    "a",
                    "hidden",
                    "vector",
                    "h",
                    "Hw",
                    "The",
                    "number",
                    "of",
                    "columns",
                    "of",
                    "weight",
                    "matrix",
                    "TS",
                    "Timestep",
                    "Lw",
                    "The",
                    "number",
                    "of",
                    "rows",
                    "of",
                    "weight",
                    "matrix",
                    "NPE",
                    "The",
                    "number",
                    "of",
                    "processing",
                    "elements"
                ],
                "name": "1",
                "page": 5,
                "regionBoundary": {
                    "x1": 51.0,
                    "x2": 439.0,
                    "y1": 97.0,
                    "y2": 163.0
                }
            },
            {
                "caption": "Fig. 8. Overview of the design.",
                "captionBoundary": {
                    "x1": 186.54312133789062,
                    "x2": 299.8684997558594,
                    "y1": 380.81640625,
                    "y2": 385.05303955078125
                },
                "figType": "Figure",
                "imageText": [],
                "name": "8",
                "page": 10,
                "regionBoundary": {
                    "x1": 49.0,
                    "x2": 434.0,
                    "y1": 82.0,
                    "y2": 368.0
                }
            },
            {
                "caption": "Table 4. Performance Comparison of the Remarn versus CPUs and GPUs",
                "captionBoundary": {
                    "x1": 107.62200164794922,
                    "x2": 378.5568542480469,
                    "y1": 82.86237335205078,
                    "y2": 87.0989990234375
                },
                "figType": "Table",
                "imageText": [
                    "TDP",
                    "Power",
                    "(W)",
                    "15",
                    "300",
                    "125b",
                    "Throughput",
                    "(GOPS)",
                    "8",
                    "1180",
                    "7670",
                    "Power",
                    "Effi.",
                    "(GOPS/W)",
                    "0.53",
                    "3.93",
                    "61.36",
                    "aDual-threaded",
                    "Quad-core",
                    "mode.",
                    "bTDP",
                    "Power.",
                    "LSTM",
                    "Size",
                    "(xt",
                    ",ht",
                    ")",
                    "(1024,",
                    "1024)",
                    "Precision",
                    "F32",
                    "F16",
                    "INT8",
                    "Technology",
                    "14",
                    "nm",
                    "12",
                    "nm",
                    "14",
                    "nm",
                    "Frequency",
                    "2.0",
                    "GHz",
                    "1.38",
                    "GHz",
                    "260",
                    "MHz",
                    "Platform",
                    "Intel",
                    "Xeon",
                    "Skylake",
                    "Tesla",
                    "V100",
                    "Stratix",
                    "10",
                    "2800",
                    "CPU",
                    "GPU",
                    "Remarna"
                ],
                "name": "4",
                "page": 14,
                "regionBoundary": {
                    "x1": 89.0,
                    "x2": 400.0,
                    "y1": 97.0,
                    "y2": 227.3280029296875
                }
            },
            {
                "caption": "Fig. 4. Timeline of three LSTM tasks based on the proposed coarse-grained multithreading.",
                "captionBoundary": {
                    "x1": 74.9520263671875,
                    "x2": 411.4654846191406,
                    "y1": 243.81837463378906,
                    "y2": 248.05499267578125
                },
                "figType": "Figure",
                "imageText": [],
                "name": "4",
                "page": 6,
                "regionBoundary": {
                    "x1": 87.0,
                    "x2": 397.0,
                    "y1": 82.0,
                    "y2": 231.0
                }
            },
            {
                "caption": "Fig. 7. Two LSTMs (A and B) are mapped to Remarn. (a) A multi-layer LSTM model showing intra-model data dependencies after unfolding. The number of layers is three and the number of timesteps is four in this example. Layer 0/1/2 are labeled in green/yellow/blue respectively. ci j represents the cell state of layer i and timestep j. (b) Computation order for minimum data dependencies. (c) Two LSTMs are mapped to a singlethread single-core design. (d) Two LSTMs are mapped to a three-thread single-core design. (e) Two LSTMs are mapped to a single-thread three-core design. (f) Two LSTMs are mapped to a dual-thread three-core design.",
                "captionBoundary": {
                    "x1": 45.773040771484375,
                    "x2": 441.8035583496094,
                    "y1": 444.81536865234375,
                    "y2": 514.8073120117188
                },
                "figType": "Figure",
                "imageText": [],
                "name": "7",
                "page": 9,
                "regionBoundary": {
                    "x1": 49.0,
                    "x2": 434.0,
                    "y1": 81.0,
                    "y2": 432.0
                }
            },
            {
                "caption": "Table 3. Resource Utilization",
                "captionBoundary": {
                    "x1": 189.46800231933594,
                    "x2": 296.3375549316406,
                    "y1": 188.32435607910156,
                    "y2": 192.56097412109375
                },
                "figType": "Table",
                "imageText": [
                    "Stratix",
                    "10",
                    "(2800)",
                    "Single",
                    "Single",
                    "487,232",
                    "(52.2%)",
                    "10,061",
                    "(85.8%)",
                    "4,368",
                    "(76%)",
                    "260",
                    "MHz",
                    "Stratix",
                    "10",
                    "(2800)",
                    "Dual",
                    "Quad",
                    "533,722",
                    "(57.2%)",
                    "10,157",
                    "(86.6%)",
                    "4,368",
                    "(76%)",
                    "260",
                    "MHz",
                    "Thread",
                    "Core",
                    "ALMs",
                    "M20K",
                    "DSP",
                    "Freq."
                ],
                "name": "3",
                "page": 13,
                "regionBoundary": {
                    "x1": 50.0,
                    "x2": 439.0,
                    "y1": 202.0,
                    "y2": 240.0
                }
            },
            {
                "caption": "Table 2. Benchmarks of Various LSTMs for This Study",
                "captionBoundary": {
                    "x1": 142.78500366210938,
                    "x2": 343.03961181640625,
                    "y1": 82.86237335205078,
                    "y2": 87.0989990234375
                },
                "figType": "Table",
                "imageText": [
                    "Show",
                    "&",
                    "Tell",
                    "[79]",
                    "512",
                    "512",
                    "Image",
                    "Caption",
                    "Generation",
                    "DeepBench",
                    "[29]",
                    "256",
                    "/",
                    "512",
                    "/",
                    "1024",
                    "256",
                    "/",
                    "512",
                    "/",
                    "1024",
                    "Speech",
                    "Recognition",
                    "IMDB",
                    "[45]",
                    "128",
                    "128",
                    "Sentiment",
                    "Classification",
                    "LRCN",
                    "[17]",
                    "256",
                    "2048",
                    "Activity",
                    "Recognition",
                    "Telemanom",
                    "[31]",
                    "64",
                    "/",
                    "128",
                    "25",
                    "Anomaly",
                    "Detection",
                    "Name",
                    "Length",
                    "(ht",
                    ")",
                    "Length",
                    "(xt",
                    ")",
                    "Domain"
                ],
                "name": "2",
                "page": 13,
                "regionBoundary": {
                    "x1": 73.0,
                    "x2": 416.0,
                    "y1": 97.0,
                    "y2": 172.0
                }
            },
            {
                "caption": "Fig. 1. Timeline of three LSTM inference tasks based on a first-come first-serve scheduling policy. The example of the LSTM-1 has two timesteps (TS), the LSTM-2 has three timesteps and the LSTM-3 has two timesteps in this figure.",
                "captionBoundary": {
                    "x1": 45.945030212402344,
                    "x2": 441.9496154785156,
                    "y1": 212.8134307861328,
                    "y2": 238.96572875976562
                },
                "figType": "Figure",
                "imageText": [],
                "name": "1",
                "page": 2,
                "regionBoundary": {
                    "x1": 65.0,
                    "x2": 418.0,
                    "y1": 82.0,
                    "y2": 200.0
                }
            },
            {
                "caption": "Table 5. Comparison with Existing Implementations of LSTMs on Stratix 10 GX 2800",
                "captionBoundary": {
                    "x1": 86.822998046875,
                    "x2": 398.99896240234375,
                    "y1": 82.86237335205078,
                    "y2": 87.0989990234375
                },
                "figType": "Table",
                "imageText": [
                    "LSTM",
                    "HW",
                    "Utilization",
                    "0.8%",
                    "14.3%",
                    "56.1%",
                    "81.6%",
                    "aDual-threaded",
                    "Quad-core",
                    "mode.",
                    "bTDP",
                    "Power",
                    "is",
                    "used.",
                    "Performance",
                    "(GOPS)",
                    "370",
                    "1431",
                    "4790",
                    "6965",
                    "Powerb",
                    "Effi.",
                    "(GOPS/W)",
                    "2.96",
                    "11.45",
                    "38.32",
                    "55.72",
                    "Powerb",
                    "(W)",
                    "125",
                    "LSTM",
                    "Size",
                    "(ht",
                    ")",
                    "256",
                    "Frequency",
                    "(MHz)",
                    "250",
                    "260",
                    "260",
                    "260",
                    "DSP",
                    "Used",
                    "5245",
                    "(91%)",
                    "4880",
                    "(85%)",
                    "4368",
                    "(76%)",
                    "4368",
                    "(76%)",
                    "Model",
                    "Storage",
                    "On-chip",
                    "Precision",
                    "(bits)",
                    "BFP-1s5e2m",
                    "8",
                    "fixed",
                    "8",
                    "fixed",
                    "8",
                    "fixed",
                    "FPGA",
                    "Stratix",
                    "10",
                    "GX2800",
                    "Remarna",
                    "FCCM19-",
                    "NPU",
                    "[51]",
                    "FCCM20-",
                    "NPU",
                    "[57]",
                    "ISCA18-",
                    "BW",
                    "[19]"
                ],
                "name": "5",
                "page": 17,
                "regionBoundary": {
                    "x1": 82.0,
                    "x2": 406.0,
                    "y1": 97.0,
                    "y2": 264.2640075683594
                }
            },
            {
                "caption": "Fig. 5. The four weight matrices from the four LSTM gates and the fused weight matrix. The weights from the four gates are interleaved. The lengths of the input vector and hidden vector are 2 in this figure.",
                "captionBoundary": {
                    "x1": 45.77399826049805,
                    "x2": 440.30902099609375,
                    "y1": 195.81214904785156,
                    "y2": 211.01107788085938
                },
                "figType": "Figure",
                "imageText": [],
                "name": "5",
                "page": 7,
                "regionBoundary": {
                    "x1": 85.0,
                    "x2": 398.0,
                    "y1": 81.0,
                    "y2": 183.0
                }
            },
            {
                "caption": "Fig. 6. The partition of an LSTM weight matrix. (a) The fused weight matrix, showing two sub-layers. (b) The MVM of a sub-layer based on a tile of EP \u00d7VP shaded in blue.",
                "captionBoundary": {
                    "x1": 45.77396774291992,
                    "x2": 440.28753662109375,
                    "y1": 342.9621887207031,
                    "y2": 358.1611328125
                },
                "figType": "Figure",
                "imageText": [],
                "name": "6",
                "page": 7,
                "regionBoundary": {
                    "x1": 98.0,
                    "x2": 386.0,
                    "y1": 231.0,
                    "y2": 330.0
                }
            },
            {
                "caption": "Fig. 2. Timeline of three LSTM inference tasks using a batching technique.",
                "captionBoundary": {
                    "x1": 105.58796691894531,
                    "x2": 380.4781188964844,
                    "y1": 226.81748962402344,
                    "y2": 231.05410766601562
                },
                "figType": "Figure",
                "imageText": [],
                "name": "2",
                "page": 3,
                "regionBoundary": {
                    "x1": 90.0,
                    "x2": 394.0,
                    "y1": 82.0,
                    "y2": 214.0
                }
            },
            {
                "caption": "Fig. 11. Performance speedup of the multi-threaded quad-core Remarn over the multi-threaded single-core design.",
                "captionBoundary": {
                    "x1": 45.94499969482422,
                    "x2": 440.4853820800781,
                    "y1": 226.81736755371094,
                    "y2": 242.00735473632812
                },
                "figType": "Figure",
                "imageText": [],
                "name": "11",
                "page": 16,
                "regionBoundary": {
                    "x1": 106.0,
                    "x2": 378.0,
                    "y1": 81.0,
                    "y2": 214.0
                }
            },
            {
                "caption": "Fig. 3. A diagram of an LSTM Cell.",
                "captionBoundary": {
                    "x1": 178.8929901123047,
                    "x2": 307.51239013671875,
                    "y1": 260.81951904296875,
                    "y2": 265.05615234375
                },
                "figType": "Figure",
                "imageText": [],
                "name": "3",
                "page": 4,
                "regionBoundary": {
                    "x1": 86.0,
                    "x2": 391.0,
                    "y1": 85.0,
                    "y2": 241.0
                }
            },
            {
                "caption": "Fig. 9. Performance speedup for quad-core Remarn with various threads.",
                "captionBoundary": {
                    "x1": 108.25199890136719,
                    "x2": 377.82586669921875,
                    "y1": 269.8193664550781,
                    "y2": 274.0559997558594
                },
                "figType": "Figure",
                "imageText": [],
                "name": "9",
                "page": 15,
                "regionBoundary": {
                    "x1": 88.0,
                    "x2": 395.0,
                    "y1": 81.0,
                    "y2": 257.0
                }
            },
            {
                "caption": "Fig. 10. Hardware utilization targeting various LSTM tasks. (a) Hardware utilization of different LSTM tasks using various threads with a single big accelerator. (b) Hardware utilization of different LSTM tasks using various threads with four sub-accelerator cores.",
                "captionBoundary": {
                    "x1": 45.56687927246094,
                    "x2": 440.3297119140625,
                    "y1": 477.5572509765625,
                    "y2": 503.7095642089844
                },
                "figType": "Figure",
                "imageText": [],
                "name": "10",
                "page": 15,
                "regionBoundary": {
                    "x1": 52.0,
                    "x2": 432.0,
                    "y1": 292.0,
                    "y2": 465.0
                }
            }
        ],
        "sections": [
            {
                "paragraphs": [
                    {
                        "page": 0,
                        "region": {
                            "x1": 45.42300033569336,
                            "x2": 467.1479187011719,
                            "y1": 84.15505981445312,
                            "y2": 386.1369934082031
                        },
                        "text": "4 Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks ZHIQIANG QUE, Imperial College London, UK HIROKI NAKAHARA, Tokyo Institute of Technology, Japan HONGXIANG FAN, Imperial College London, UK HE LI, University of Cambridge, UK JIUXI MENG, Imperial College London, UK KUEN HUNG TSOI and XINYU NIU, Corerain Technologies Ltd., China ERIKO NURVITADHI, Intel Corporation, USA WAYNE LUK, Imperial College London, UK This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters."
                    },
                    {
                        "page": 0,
                        "region": {
                            "x1": 45.944889068603516,
                            "x2": 440.6512145996094,
                            "y1": 396.824951171875,
                            "y2": 413.9830322265625
                        },
                        "text": "CCS Concepts: \u2022 Hardware \u2192 Hardware accelerators; Application specific processors; \u2022 Computer systems organization\u2192Multicore architectures; Neural networks; Special purpose systems;"
                    },
                    {
                        "page": 0,
                        "region": {
                            "x1": 45.629268646240234,
                            "x2": 440.4693908691406,
                            "y1": 425.2582702636719,
                            "y2": 440.5872497558594
                        },
                        "text": "Additional Key Words and Phrases: Accelerator architecture, recurrent neural networks, multi-tenant execution"
                    },
                    {
                        "page": 0,
                        "region": {
                            "x1": 45.39585876464844,
                            "x2": 441.6772766113281,
                            "y1": 492.7535705566406,
                            "y2": 645.798583984375
                        },
                        "text": "The support of the United Kingdom EPSRC (grant numbers EP/V028251/1, EP/L016796/1, EP/N031768/1, EP/P010040/1, and EP/S030069/1), Corerain and Intel is gratefully acknowledged. Authors\u2019 addresses: Z. Que, H. Fan, J. Meng, and W. Luk, Imperial College London, Exhibition Rd, South Kensington, London SW7 2BX, UK; emails: {z.que, h.fan17, jiuxi.meng16, w.luk}@imperial.ac.uk; H. Nakahara, Tokyo Institute of Technology, Ohokayama 1-21-2, Tokyo, 1528550, Japan; email: nakahara.h.ad@m.titech.ac.jp; H. Li, University of Cambridge, Cambridge CB2 1TN, UK; email: he.li@ieee.org; K. H. Tsoi and X. Niu, Corerain Technologies Ltd.,14F Changfu Jinmao Building (CFC), Shenzhou, China; email: {kuenhung.tsoi, xinyu.niu}@corerain.com; E. Nurvitadhi, Intel Corporation, Jones Farm Campus, Hillsboro, OR, 97124-6463, USA; email: eriko.nurvitadhi@intel.com. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2022 Association for Computing Machinery. 1936-7406/2022/12-ART4 $15.00 https://doi.org/10.1145/3534969"
                    },
                    {
                        "page": 0,
                        "region": {
                            "x1": 45.90037155151367,
                            "x2": 440.4544982910156,
                            "y1": 665.8298950195312,
                            "y2": 669.7113037109375
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 1,
                        "region": {
                            "x1": 45.441001892089844,
                            "x2": 441.7889404296875,
                            "y1": 85.09010314941406,
                            "y2": 133.225341796875
                        },
                        "text": "ACM Reference format: Zhiqiang Que, Hiroki Nakahara, Hongxiang Fan, He Li, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and Wayne Luk. 2022. Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks. ACM Trans. Reconfig. Technol. Syst. 16, 1, Article 4 (December 2022), 26 pages. https://doi.org/10.1145/3534969"
                    }
                ]
            },
            {
                "paragraphs": [
                    {
                        "page": 1,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.39892578125,
                            "y1": 179.7440948486328,
                            "y2": 256.32659912109375
                        },
                        "text": "Recurrent Neural Network (RNN) has been the key component of artificial intelligence (AI) applications where the inputs are sequential, such as natural language processing [25], speech recognition [2, 28], and video analysis [17, 87]. Long Short-Term Memory (LSTM) is the most popular type of RNNs. Since low latency is key for a seamless user experience in such applications, efficient and real-time acceleration of RNNs/LSTMs is required. FPGAs have been used to speed up the inference of LSTMs [19, 27, 51, 52, 75], showing the benefits of low latency and low power consumption compared to CPUs or GPUs."
                    },
                    {
                        "page": 1,
                        "region": {
                            "x1": 45.41334915161133,
                            "x2": 442.08721923828125,
                            "y1": 263.42694091796875,
                            "y2": 519.3421020507812
                        },
                        "text": "However, existing RNN/LSTM accelerators cannot support cost-effective multi-RNN execution. Cloud providers must minimize their huge operation costs by running as many applications on a given server as possible, while satisfying the quality of each service. In Google data centers, Convolutional Neural Networks (CNNs) and Multi-Layer Perceptions (MLP) comprise 5% and 61% of the workload, respectively, while LSTMs makes up 29% [35]. However, most of the existing LSTM accelerators are only able to perform one inference at a time [19, 27, 28, 47, 51, 57]. These accelerators can process multi-LSTM tasks by executing one by one in sequence, resulting in inefficiency when multiple requests come at the same time as shown in Figure 1. It may make later tasks wait for a long time before a hardware core is available, since earlier LSTM tasks may have a large number of timesteps involving many iterations, e.g., an LSTM layer in DeepSpeech [29] has 1,500 timesteps. Besides, some applications employ not one but multiple LSTMs to collaboratively achieve satisfactory performance. A spacecraft anomalies detection system [31] even contains over hundreds of LSTM models, each modeling a single telemetry channel and predicting values for that channel, which demonstrates the necessity of supporting multi-LSTM execution. Furthermore, conventional LSTM accelerators are often implemented by deploying all computing resources to support a single computational engine on a large scale, leveraging data-level parallelism. For instance, Brainwave [19] devised by Microsoft is a single-threaded neural processing unit (NPU) that involves 96,000 processing elements (PEs). However, when the workload of a targeted LSTM task is small, these hardware resources will not be fully utilized, e.g., the hardware utilization is lower than 1% [19] for Brainwave and lower than 15% for the Brainwave-like NPU [51] when running an LSTM model (ht =256). It is challenging to design an accelerator to support cost-effective multi-LSTM execution."
                    },
                    {
                        "page": 1,
                        "region": {
                            "x1": 45.7747917175293,
                            "x2": 441.9924621582031,
                            "y1": 526.4424438476562,
                            "y2": 650.8514404296875
                        },
                        "text": "This article introduces a reconfigurable multi-threaded multi-core NPU for accelerating RNN/ LSTM inferences by increasing the hardware utilization for better performance. It improves the processing abilities of cloud-based NPUs as well as the Quality of Service. Our primary goal is to efficiently enhance the scalability potential of NPU cores. The most area- /cost-efficient way to add logical cores is multithreading. Essentially, multithreading retrieves unused performance (where computational cores are idle because of events) by switching to another thread. Multithreading also does not affect peak performance when working in a single-threaded mode. Usually, the execution of multiple neural networks has the potential to mitigate the idle issues, because layers from different neural networks can be scheduled freely without any issue of dependencies. Running multiple tasks can also be realized by batch techniques that provide multiple requests to a neural network to produce multiple results together. However, the batch techniques can harm latency,"
                    },
                    {
                        "page": 1,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 2,
                        "region": {
                            "x1": 45.945030212402344,
                            "x2": 440.48394775390625,
                            "y1": 262.3462829589844,
                            "y2": 291.1112976074219
                        },
                        "text": "because different requests may not arrive at the same time [22], which means that a newly arrived request must wait until the batch is formed, which brings a significant latency penalty as shown in Figure 2."
                    },
                    {
                        "page": 2,
                        "region": {
                            "x1": 45.576412200927734,
                            "x2": 442.1606750488281,
                            "y1": 298.2116394042969,
                            "y2": 518.2645263671875
                        },
                        "text": "Remarn is inspired by coarse-grained multithreading utilized in modern CPUs such as IBM RS64-IV [6] and Intel Montecito [46], which makes a core switch to a different hardware context when a thread is stalled due to some events. Remarn consists of a custom coarse-grained multi-threaded (CGMT) LSTM hardware architecture that switches tasks among threads when LSTM computational engines meet data hazard. When one logical NPU core is stalled, the other can make progress. Coarse-grained multithreading is a mature technique in modern CPU designs. However, few studies concern combining the CGMT and NPUs, especially for RNNs/LSTMs. There is also fine-grained multi-threading [73, 77] that switches the context every cycle, but it brings more hardware complexity than CGMT. Unlike CNNs that do not have memory cells and can run different layers from different neural networks iteratively, RNNs/LSTMs contains many memory cells, which makes it difficult to process different timesteps from different RNN layers or models, since they have different cell memory statuses when using a single-threaded NPU. It has to finish the running of the preceding RNN inference or layer until the next inference or layer can start. The existence of inter-timestep dependencies within an RNN model prevents the following timesteps from even starting their execution until the current timestep\u2019s completion, leading to hardware underutilization and inefficiency. To address this challenge, this work proposes the CGMT-LSTM, which can intelligently switch to an alternate thread of computation when the data hazard happens, e.g., the inter-timestep dependencies of RNNs, to increase hardware utilization and boost design performance."
                    },
                    {
                        "page": 2,
                        "region": {
                            "x1": 45.945030212402344,
                            "x2": 442.16168212890625,
                            "y1": 525.3648681640625,
                            "y2": 649.7738647460938
                        },
                        "text": "Besides, instead of deploying all computing resources to form a single physical core on a large scale like Brainwave [19] and our previous design [59], we design an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, and each core can accept new RNN requests. The multiple LSTM models used in the application of spacecraft anomaly detection system [31] have only a small hidden vector size that is less than 128, but they have 250 timesteps that are large. LSTM models that have a large number of timesteps but utilize a small size are the most tangible examples, since they require dealing with lots of dependencies, as well as the parallel task of matrix-vector multiplications (MVMs) [86]. The Brainwave [19] involves big MVM \u201ctile engines\u201d that can effectively process a 400\u00d7240 matrix in parallel. Besides, our previous NPU [59] is based on a tile size of 1, 024 \u00d7 16, resulting in 16,384 effective PEs. Any MVM that does not map to this dimension will leave some resources idle and small MVMs even"
                    },
                    {
                        "page": 2,
                        "region": {
                            "x1": 45.90003204345703,
                            "x2": 440.45416259765625,
                            "y1": 671.3135375976562,
                            "y2": 675.1949462890625
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 45.406959533691406,
                            "x2": 440.5675964355469,
                            "y1": 252.0053253173828,
                            "y2": 412.27880859375
                        },
                        "text": "require zero paddings, resulting in low utilization. Splitting a large accelerator into several small sub-accelerator cores can not only help to find a better mapping to improve hardware utilization but also provide more physical cores to spatially co-locate multiple RNN inferences on the same hardware. As Reference [19] mentions, RNN programs have a critical loop-carry dependence on the ht vector. If the full pipeline cannot return ht to the vector register file in time for the next timestep computation, then the MVM unit will stall until ht is available. Thus, even if a large MVM engine finishes the LSTM gates operations in a short period, e.g., one cycle, then the design still needs to wait for the ht returned from the pipeline, which shows the limitation of the architecture using a large engine. This work splits the accelerator with a single large engine into several smaller sub-accelerator cores, each of them processing small LSTM models more efficiently by adopting the tiling-based columnwise MVM approach [57]. It addresses the challenge of accelerating a large number of small LSTM models with large timesteps. Please note that these sub-accelerator cores can work together as a single large accelerator when necessary to deal with the high priority workloads that require the lowest end to end latency."
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 45.776580810546875,
                            "x2": 440.30865478515625,
                            "y1": 419.3791809082031,
                            "y2": 448.1441650390625
                        },
                        "text": "To the best of our knowledge, Remarn is the first coarse-grained multi-threaded and multi-core LSTM accelerator architecture capable of achieving high performance and efficient multi-LSTM execution."
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 55.73917770385742,
                            "x2": 203.0501251220703,
                            "y1": 455.2445373535156,
                            "y2": 460.0963134765625
                        },
                        "text": "Our contributions are the following:"
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 61.247501373291016,
                            "x2": 441.9693908691406,
                            "y1": 470.140625,
                            "y2": 500.94793701171875
                        },
                        "text": "\u2022 A novel reconfigurable multi-threaded multi-core neural processing unit to enable effective and efficient multi-neural network execution for LSTM inferences. \u2022 A custom CGMT LSTM accelerator architecture that also can be partitioned into several full-"
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 61.247501373291016,
                            "x2": 440.3105773925781,
                            "y1": 508.0483093261719,
                            "y2": 536.8132934570312
                        },
                        "text": "fledged sub-accelerator cores, which significantly improves hardware utilization and design performance. \u2022 A custom tiling method for LSTMs, which minimizes the intermediate results buffers when"
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 61.247501373291016,
                            "x2": 404.62750244140625,
                            "y1": 543.9136352539062,
                            "y2": 560.717529296875
                        },
                        "text": "combining CGMT and partition, thereby increasing the accelerator area efficiency. \u2022 A comprehensive evaluation on the proposed methods and hardware architecture."
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 45.77756881713867,
                            "x2": 442.0032043457031,
                            "y1": 574.397216796875,
                            "y2": 650.9887084960938
                        },
                        "text": "Relationship to Prior Publications: This article expands on a conference paper [59] with the baseline design proposed in Reference [57]. The baseline design [57] involves a novel latency-hiding hardware architecture based on columnwise matrix-vector multiplication. It has much higher performance and hardware utilization than other designs for RNN models with different sizes, but it still suffers from underutilization when the model size is small. Reference [59] addresses the underutilization issue by introducing CGMT that enables temporal multi-neural network execution to improve the performance when the RNN models are small while still maintaining the"
                    },
                    {
                        "page": 3,
                        "region": {
                            "x1": 45.49496841430664,
                            "x2": 440.04986572265625,
                            "y1": 671.3135986328125,
                            "y2": 675.1950073242188
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 45.648094177246094,
                            "x2": 442.19061279296875,
                            "y1": 287.1053771972656,
                            "y2": 435.4185791015625
                        },
                        "text": "performance of RNN models with medium to large sizes. It mainly time-multiplexes a large RNN accelerator across various RNNs workloads. This article addresses a limitation of the work described in our previous papers, that they do not allow the large computation engine to be partitioned into several smaller ones, so they do not support spatial co-execution of multiple RNN inferences. This limitation brings a severe hardware underutilization issue when targeting acceleration for a large number of small RNN models that are commonly used in many applications [27, 31, 62, 88]. This work introduces an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, combining multithreading with multi-core techniques to enable temporal and spatial multi-neural network execution on cloud-based NPUs. The proposed novel dimension of optimization allows us to obtain significant improvement in throughput while reducing latency over the previous design [59]. This article adds the design of multiple sub-accelerator cores for spatial co-execution of RNNs in Section 3.3. Sections 4.1\u20134.3 describe a revised hardware architecture, and Sections 5.3\u20135.6 contain new evaluation results."
                    }
                ],
                "title": {
                    "page": 1,
                    "region": {
                        "x1": 45.77399826049805,
                        "x2": 138.85256958007812,
                        "y1": 164.98448181152344,
                        "y2": 169.64697265625
                    },
                    "text": "1 INTRODUCTION"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 4,
                        "region": {
                            "x1": 45.59330368041992,
                            "x2": 442.15362548828125,
                            "y1": 468.1238098144531,
                            "y2": 520.8020629882812
                        },
                        "text": "RNNs/LSTMs have been shown to have useful properties with many significant applications. Among the many RNN variants, the most popular one is the LSTM that was initially proposed in 1997 [30]. This study follows the standard LSTM cell [17, 19, 27, 51]. Figure 3 contains a diagram of an LSTM cell. It utilizes the following equations to compute the gates and produce the results for the next time step."
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 104.91261291503906,
                            "x2": 382.22882080078125,
                            "y1": 535.4949340820312,
                            "y2": 542.1121826171875
                        },
                        "text": "it = \u03c3 (Wi [xt ,ht\u22121] + bi ), ft = \u03c3 (Wf [xt ,ht\u22121] + bf )"
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 103.24017333984375,
                            "x2": 440.46783447265625,
                            "y1": 550.7496337890625,
                            "y2": 557.2501220703125
                        },
                        "text": "\u0434t = tanh(W\u0434[xt ,ht\u22121] + bu ), ot = \u03c3 (Wo[xt ,ht\u22121] + bo ) (1)"
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 103.78873443603516,
                            "x2": 355.85821533203125,
                            "y1": 565.8969116210938,
                            "y2": 572.3970336914062
                        },
                        "text": "ct = ft ct\u22121 + it \u0434t , ht = ot tanh(ct )."
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 45.576454162597656,
                            "x2": 442.15771484375,
                            "y1": 585.6517333984375,
                            "y2": 650.4365234375
                        },
                        "text": "Here, \u03c3 and tanh represent the sigmoid function and hyperbolic tangent function. Both are activation functions; it , ft ,\u0434t , and ot denote the output of the input gate (i-gate), forget gate (f - gate), input modulation gate (\u0434-gate), and output gate (o-gate) at timestep t , respectively. The \u0434gate is often considered as a sub-part of the i-gate. Each LSTM gate consists of a MVM unit and a corresponding activation function unit, as shown in Figure 3. The operator denotes an elementwise multiplication.W is the weight matrix for both input and hidden units, since the input vector"
                    },
                    {
                        "page": 4,
                        "region": {
                            "x1": 45.899986267089844,
                            "x2": 440.4541015625,
                            "y1": 671.3136596679688,
                            "y2": 675.195068359375
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.30059814453125,
                            "y1": 179.54481506347656,
                            "y2": 208.45501708984375
                        },
                        "text": "and hidden vector are combined in the equations. The b terms denote the bias vectors. ct is the internal memory cell status at timestep t while ht is the hidden vector that is the output of the cell and passed to the next timestep calculation or next LSTM layer."
                    },
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.278568267822266,
                            "x2": 441.8219299316406,
                            "y1": 215.56434631347656,
                            "y2": 351.9243469238281
                        },
                        "text": "The LSTM information flow is controlled by these four gates with details shown in Figure 3. The i-gate decides what new information is to be written into the internal memory cell; the \u0434-gate modulates the information processed by i-gate via adding non-linearity. Note that only \u0434-gate utilizes hyperbolic tangent as its activation function while all the other three gates utilize sigmoid. The f -gate decides what old information is no longer needed and can be discarded so there are element-wise multiplications between the output of f -gate and memory cell status in the previous timestep ct\u22121. Its output will be added to the products of the outputs from i-gate and \u0434-gate to form the current status of the internal memory cell. The o-gate decides what the value of the current hidden vector (ht ) should be by multiplying the current status of the memory cell after the hyperbolic tangent function, as shown in the LSTM-Tail in Figure 3. Our work focuses on the optimization of RNN inferences involving standard LSTMs, but the proposed techniques can be applied to other deep neural networks (DNN) inferences."
                    }
                ],
                "title": {
                    "page": 4,
                    "region": {
                        "x1": 45.94398498535156,
                        "x2": 235.99754333496094,
                        "y1": 453.3731689453125,
                        "y2": 458.0356750488281
                    },
                    "text": "2 BACKGROUND AND PRELIMINARIES"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.42171859741211,
                            "x2": 441.994140625,
                            "y1": 383.126220703125,
                            "y2": 447.756591796875
                        },
                        "text": "Accelerating RNN/LSTM inferences efficiently is challenging because of their recurrent nature and data dependencies. This section first presents the coarse-grained multithreading for accelerating RNN/LSTM and then introduces a partitioning strategy of LSTM weight matrix to apply sub-layer granularity scheduling with fine-grained tasks for Remarn architecture. Finally, we present the multi-core accelerator to enable spatial co-execution of RNN models or layers to improve the design performance. Some design parameters are defined in Table 1."
                    }
                ],
                "title": {
                    "page": 5,
                    "region": {
                        "x1": 45.77239990234375,
                        "x2": 280.648681640625,
                        "y1": 368.3755798339844,
                        "y2": 373.0380859375
                    },
                    "text": "3 DESIGN AND OPTIMIZATION METHODOLOGY"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.40378952026367,
                            "x2": 441.9582214355469,
                            "y1": 478.9584655761719,
                            "y2": 615.319580078125
                        },
                        "text": "There is a large demand for architectural support of multi-DNN execution to maximize the hardware utilization and reduce the costs of running a large-scale production system. However, most of the existing LSTM accelerators can only run one task at a time [19, 27, 28, 47, 51, 57]. This work proposes a CGMT LSTM NPU that switches on the event when the data hazard of a computational unit happens in the LSTM computation. The general idea is when a thread is stalled because of some events, e.g., cache misses, the multi-threaded core can switch to a different hardware context. In the proposed CGMT LSTM accelerator, the event is the hazard caused by the data dependency between the timesteps of sequential calculation in LSTMs. We propose to maintain multiple thread contexts in a recurrent NPU core so that when the first thread stalls, the second one can carry on, as shown in Figure 4. Thus, it utilizes computational resources more efficiently than a single thread core. It can increase the design performance by utilizing thread-level parallelism and enhance the NPU hardware utilization."
                    },
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.775390625,
                            "x2": 440.30731201171875,
                            "y1": 622.419921875,
                            "y2": 651.1849365234375
                        },
                        "text": "Besides, a preceding LSTM model or layer may have thousands of sequences (timesteps), which occupies the NPU core for a long time, leading to a long waiting time for the latter requests before they have an available core to run. However, some services have strict latency constraints, since"
                    },
                    {
                        "page": 5,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 6,
                        "region": {
                            "x1": 45.467811584472656,
                            "x2": 442.0372314453125,
                            "y1": 271.37322998046875,
                            "y2": 395.7732238769531
                        },
                        "text": "the response time directly relates to user experience, e.g., intelligent personal assistants are one of the examples where real-time DL is utilized to handle user speech and output smart responses. The conventional single-threaded NPUs can only store the old thread\u2019s context to memory and retrieve the new thread\u2019s context using preemption mechanisms [14] to run another task. However, it will have a large context switch penalty. In our multi-threaded NPU for LSTMs, the new task can execute from another thread as soon as it comes, as shown in Figure 4. Please note that a particular thread may still stall due to data hazard, but the physical core is not stalled, since multiple threads share the same computational physical core, which leads to \u201cVirtual idle\u201d as shown in this figure. We believe that further optimizations, e.g., simultaneous multithreading (SMT) can be adopted to our NPU design to gain even higher performance. We leave it for our future work, because it does not have a big impact on the conclusions we draw from this work."
                    }
                ],
                "title": {
                    "page": 5,
                    "region": {
                        "x1": 45.77239990234375,
                        "x2": 308.0606994628906,
                        "y1": 464.2078552246094,
                        "y2": 468.870361328125
                    },
                    "text": "3.1 Multithreading for Recurrent Neural Processing Unit"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 6,
                        "region": {
                            "x1": 45.64813232421875,
                            "x2": 440.7171325683594,
                            "y1": 429.75665283203125,
                            "y2": 530.2523803710938
                        },
                        "text": "The LSTM calculation of one timestep in an LSTM layer has four MVM operations according to the Equations (1). Besides, these four MVM are independent and share the same size. Since the four matrices of i, f ,o,u gates in LSTMs have the same size, these matrices can be combined into a single large matrix [1, 57]. Figure 5 illustrates the basic idea. Thus, in the calculation of one timestep of an LSTM layer, we can only focus on the optimizations of one large matrix multiplying one vector for the whole LSTM cell rather than four small matrices multiplied by one vector. This is a general optimization that can be utilized for any MVM operations that share the same input vector. Because each matrix from LSTM gates has a size of Lh \u00d7 (Lx + Lh), the large fused matrix has a size of (4 \u00d7 Lh) \u00d7 (Lx + Lh)."
                    },
                    {
                        "page": 6,
                        "region": {
                            "x1": 45.946022033691406,
                            "x2": 442.17376708984375,
                            "y1": 537.3526611328125,
                            "y2": 649.8005981445312
                        },
                        "text": "Usually, deep neural networks have many compute intensive operations involving large weight matrices, which can be accelerated by running in parallel. However, when deploying on FPGAs, the parallelism is constrained by the limited hardware resources on the targeted FPGAs. It means that the whole MVM computation may not be fully unrolled and performed at once, especially for the large LSTMs. To use the computational resources efficiently, the combined weight matrix of an LSTM layer is partitioned into multiple sub-layers in advance depending on the configuration of the accelerator cores and LSTM layer sizes. Specifically, an LSTM layer of one timestep is partitioned into a number of sub-layers with equal size, as shown in Figure 6(a). For simplicity, the number of the sub-layers in the example is set as 2 to illustrate the idea, but a real design could have more sub-layers, and it depends on the design constraints. Then, Element-based Parallelism"
                    },
                    {
                        "page": 6,
                        "region": {
                            "x1": 45.9000244140625,
                            "x2": 440.45416259765625,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 7,
                        "region": {
                            "x1": 45.46796798706055,
                            "x2": 441.8220520019531,
                            "y1": 380.84002685546875,
                            "y2": 402.49481201171875
                        },
                        "text": "(EP) and Vector-based Parallelism (VP) are applied to exploit the available parallelism [57]. The number of the sub-layers is determined by (4\u00d7Lh) V P"
                    },
                    {
                        "page": 7,
                        "region": {
                            "x1": 45.774078369140625,
                            "x2": 442.0039367675781,
                            "y1": 394.2050476074219,
                            "y2": 650.123046875
                        },
                        "text": ". The sub-layer is then divided into many small tiles, each having a size of (EP ,VP ), as shown in Figure 6(b). In each cycle, our Remarn core is able to process the MVM of a tile and a sub-vector of [xt ,ht\u22121] with a size of EP . To increase the design parallelism, VP should be chosen to be as large as possible. However, the largest value of VP equals Hw , which is 4 \u00d7 Lh, since there are only four gates in an LSTM. Thus, the smallest number of sub-layers is one when VP is 4 \u00d7 Lh. In this study, sub-layer granularity scheduling is adopted with fine-grain tasks of sub-layers for multi-LSTM execution. Besides, we interleave the rows of the four weight matrices, so that the related elements from the four gates output are adjacent in the result vector. Thus, the LSTM gate outputs can multiply with each other easily using the element-wise operations in the LSTM-tail unit. It also means that there is no data dependency between these sub-layers. The optimization of interleaving also removes the requirement to buffer large intermediate outputs from four LSTM gates, since the result sub-vectors from the sub-layers are not related and will be reduced in the tail unit soon. Each sub-vector of the MVM output can be handled individually in the LSTM-tail units. There is no need to wait for other sub-layer results to get the LSTM memory cell status and hidden vector of the current sub-layer. The proposed CGMT core will always finish processing all the sub-layers in one timestep before it switches to another thread, because the data hazard in the LSTMs happens when timestep changes. Compared with a fine-grained multithreading scheme that switches the context between threads in every cycle, we can avoid buffering these large intermediate MVM operation values as well as element-wise operation values, since these values will finally form the sub-vectors of LSTM memory cells and hidden units. Only the thread dedicated buffers are needed to be added for storing the LSTM memory cells and hidden units in each timestep, since different threads process different LSTM tasks."
                    },
                    {
                        "page": 7,
                        "region": {
                            "x1": 45.49496841430664,
                            "x2": 440.04986572265625,
                            "y1": 671.3132934570312,
                            "y2": 675.1947021484375
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 6,
                    "region": {
                        "x1": 45.94502258300781,
                        "x2": 222.06881713867188,
                        "y1": 414.9970397949219,
                        "y2": 419.6595458984375
                    },
                    "text": "3.2 Weights Matrix Blocking Strategy"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 8,
                        "region": {
                            "x1": 45.57524108886719,
                            "x2": 442.16754150390625,
                            "y1": 99.4817123413086,
                            "y2": 307.5730895996094
                        },
                        "text": "Many RNN/LSTM accelerators deploy all the computing resources to form a single accelerator core with a large scale, leveraging data-level parallelism. However, this type of accelerator is only able to process one task at a time, resulting in potential underutilization and inefficiency when multiple tasks come at the same time, especially in large data centers. Multithreading described in Section 3.1 can address the issue partially but it still cannot easily deal with a large number of small workloads. Recently, spatial architectures have become a popular solution to build high throughput accelerators for DNNs [24, 78, 82]. Generally, multiple PEs are grouped as an engine, and these engines are connected with an on-chip interconnection to enable data communication during processing. f-CNNx [78] proposes multiple custom dedicated engines for the workloads with various CNN models. Planaria [24] proposes to dynamically fission a systolic array-based DNN accelerator at runtime to spatially co-locate multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations that are absent from accelerators targeting CNNs and fully connected (FC) layers. This work adopts a similar idea to Reference [24]. But instead of using a systolic two-dimensional matrix-matrix multiplication array, our architecture uses matrix-vector multiplication engines, since RNNs and MLPs are dominated by matrix-vector multiplication operations. With multiple sub-accelerator cores, it enables spatial co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations."
                    },
                    {
                        "page": 8,
                        "region": {
                            "x1": 45.941429138183594,
                            "x2": 442.1846008300781,
                            "y1": 314.6734313964844,
                            "y2": 606.4552612304688
                        },
                        "text": "In addition to inter-model parallelism, which is naturally supported by using multiple subaccelerator cores, such as co-execution of multiple models, our design also enables intra-model parallelism among different layers and timesteps of the RNN layers. Generally, in an RNN model, a latter layer or timestep should wait until its preceding layer or timestep finishes, since some of its inputs are the output of the preceding layer or timestep, as shown in Figure 7(a). For example, the computation that requires c00 and h00 cannot start until they are available from the preceding layer or timestep. Using a single-threaded single-core accelerator, two LSTM workloads will run as Figure 7(c) shows. The accelerator will process the LSTM layers and timesteps iteratively. The temporal inefficiency comes from the stall of data dependencies and the spatial inefficiency comes from mapping a small network to a large core leaving some resources idle. With a multi-threaded accelerator, each layer of the LSTM can run on different threads to remove the stall or the temporal inefficiency that comes from the RNN nature data dependencies. Please note, a special computation order, as shown in Figure 7(b) is required to achieve stall-free processing using a multi-threaded single-core accelerators, as shown in Figure 7(d). However, when LSTMs are small, many resources are left unused when mapping them to a large core. In such cases, a small core will get a similar latency to the one using a large core. Besides, with multiple sub-accelerator cores, the processing of the cascaded LSTM layers can be overlapped in a layerwise coarse-grained pipeline as shown in Figure 7(e). The second layer does not need to wait for the whole sequence of hidden vectors of the first layer to be ready. Just one hidden vector from the preceding LSTM layer is sufficient to start the calculation of the next LSTM layer. It helps to reduce the overall design latency. Besides, since the output of the preceding layer sinks directly in the following sub-accelerator core, there is no need for buffering large intermediate outputs, which could help to improve the area efficiency. We can further optimize the processing after combining the multithreading with multi-core. Figure 7(f) shows two LSTMs are mapping to a dual-thread three-core accelerator, which achieves the best total latency."
                    },
                    {
                        "page": 8,
                        "region": {
                            "x1": 45.941429138183594,
                            "x2": 440.4853820800781,
                            "y1": 613.5556030273438,
                            "y2": 642.3206176757812
                        },
                        "text": "While the multi-core scheme leads to high performance, it also presents a new challenge: In some situations it cannot provide sufficient bus bandwidth per core. In Remarn, the multiple cores share the same front side bus. But this will not affect on the bus bandwidth, because we do not add"
                    },
                    {
                        "page": 8,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 9,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.3131408691406,
                            "y1": 536.6301879882812,
                            "y2": 602.8709716796875
                        },
                        "text": "new Remarn big cores but only split the original big core into several small ones, e.g., four small cores. When the four small cores co-operate to handle a large network, the quad-core requires the same bandwidth as the one with a big monolithic core. The total bus bandwidth they require is the same as the large core from which they are derived, which is smaller than that for four large cores. For example, the bandwidth requirement of four small LSTM models with Lh = 64 is much lower than the one of one big LSTM model with Lh = 1,024."
                    }
                ],
                "title": {
                    "page": 8,
                    "region": {
                        "x1": 45.94485092163086,
                        "x2": 286.74981689453125,
                        "y1": 84.7221450805664,
                        "y2": 89.3846435546875
                    },
                    "text": "3.3 Multi-core for Recurrent Neural Processing Unit"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 9,
                        "region": {
                            "x1": 45.77421951293945,
                            "x2": 441.8465270996094,
                            "y1": 633.7399291992188,
                            "y2": 650.5438842773438
                        },
                        "text": "Based on the optimization techniques introduced above, we implement the proposed Remarn on top of a state-of-the-art (SOTA) RNN accelerator [57] for low-latency cloud-based applications."
                    },
                    {
                        "page": 9,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 10,
                        "region": {
                            "x1": 45.57650375366211,
                            "x2": 442.1408386230469,
                            "y1": 400.1992492675781,
                            "y2": 428.9552917480469
                        },
                        "text": "In Section 4.2, we outline the main components of the architecture and detail the necessary hardware modifications required to support our Remarn unit. Several FPGA-specific optimizations are also introduced."
                    }
                ],
                "title": {
                    "page": 9,
                    "region": {
                        "x1": 45.77421951293945,
                        "x2": 198.7598876953125,
                        "y1": 618.9804077148438,
                        "y2": 623.6428833007812
                    },
                    "text": "4 HARDWARE ARCHITECTURE"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 10,
                        "region": {
                            "x1": 45.46791076660156,
                            "x2": 442.1737365722656,
                            "y1": 460.7419738769531,
                            "y2": 656.8726806640625
                        },
                        "text": "A high-level block diagram of Remarn accelerator is shown in Figures 8(a) and 8(b). The original monolithic accelerator with all the MVM kernels in our previous work [59] has been split into N (e.g., N = 4 in the figure) sub-accelerator cores, each working in a big-little engines architecture. All these N cores are connected via the switch for hidden vectors and partial sum data movement. When N is small as in this case, the switch can be implemented by a crossbar. When N is large, then a network-on-chip could be more effective. Hence, in one extreme, all four cores can be attached together to construct a large logical accelerator, running only one RNN inference using the entire accelerator. Alternatively, it can also provide four stand-alone sub-accelerator cores, spatially co-executing 4 different RNNs simultaneously. Combining with the CGMT of four threads, it can support up to 16 different RNN inferences simultaneously. In this work, all the sub-accelerator cores are identical. But the design can be extended easily to employ a different accelerator hardware architecture, e.g., a systolic array-based one, for some of the cores to support the heterogeneous acceleration of multi-DNN workloads [40]. This work utilizes a general switch as the interconnection between the cores, which brings a slight performance drop when fusing as a large accelerator compared to a monolithic one, since the computational engines now need one more hop to share the partial results. We leave that for future work, since it has a limited impact on the conclusions we draw from our study in this article."
                    },
                    {
                        "page": 10,
                        "region": {
                            "x1": 45.900123596191406,
                            "x2": 440.4542541503906,
                            "y1": 671.3135375976562,
                            "y2": 675.1949462890625
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 10,
                    "region": {
                        "x1": 45.94512176513672,
                        "x2": 148.01693725585938,
                        "y1": 445.98236083984375,
                        "y2": 450.6448669433594
                    },
                    "text": "4.1 System Overview"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 11,
                        "region": {
                            "x1": 45.405540466308594,
                            "x2": 441.9830017089844,
                            "y1": 99.4817123413086,
                            "y2": 307.57354736328125
                        },
                        "text": "This work proposes a hardware architecture with big-little engines for the DNN accelerators, which is illustrated in Figure 8(a). The proposed accelerator core consists of a big engine, a little engine, an adapter unit, the crossbar as well as weight and vector buffers. The big engine is for the matrix operations that are computation intensive while the little one is for the small but essential components in the neural networks, such as ReLU, pooling, batch normalization, activation, and so on. The big-little engines architecture follows a wait-free single-producer/single-consumer mechanism using the adapter unit, which adjusts the data parallelism between the two engines. In this work, the big engine unit has VP MVM kernels to perform matrix-vector multiplications for LSTM gates operations. It occupies the major computational resources, such as DSPs. Practically, the design will deploy a large number of hardware resources in the big engine to increase the data parallelism to improve the processing ability. However, since the big engine produces one output vector after multiple accumulation cycles, the little engine does not require a large parallelization factor by deploying many LSTM tail units like the one in the big engine unit. The adapter unit can convert the parallelism between the two engines. With a proper adapting factor between the big engine output and little engine input, the little engine can be fully occupied without a stall or wait. The little engine unit has a vector activation function unit and EP tail units that execute the element-wise operations in the LSTM computation. It does not contain many computation resources like the big engine, but it is essential for running the whole neural networks on-chip."
                    },
                    {
                        "page": 11,
                        "region": {
                            "x1": 45.404541015625,
                            "x2": 442.0188903808594,
                            "y1": 314.67388916015625,
                            "y2": 427.12176513671875
                        },
                        "text": "Besides, with different hardware components in the little engine, such as ReLU, pooling, batch normalization, and so on, our design can easily be extended to support other types of neural networks, such as CNNs. The Brainwave [19] accelerate and serve CNN models using its MVM engines and custom multi-function units. Sometimes, to accelerate DNN designs, the big engine can be a large systolic array supporting large matrix operations and the little engine utilizes a SIMD vector unit to support general vector operations [24] for the other operations in NNs. The LSTMs are much more challenging to accelerate, exhibiting lower available parallelism and more data dependencies than two-dimensional (2D) CNNs [19]. This design focuses on accelerating RNN/LSTM inferences using a Brainwave-like architecture, but the proposed optimizations and hardware architecture can be easily extended to support other neural networks, which will be our future work."
                    }
                ],
                "title": {
                    "page": 11,
                    "region": {
                        "x1": 45.77415084838867,
                        "x2": 228.45033264160156,
                        "y1": 84.7221450805664,
                        "y2": 89.3846435546875
                    },
                    "text": "4.2 The Big-Little Engines Architecture"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 11,
                        "region": {
                            "x1": 45.4067497253418,
                            "x2": 441.8385925292969,
                            "y1": 459.33184814453125,
                            "y2": 549.365966796875
                        },
                        "text": "4.3.1 Overview of the Sub-accelerator Core. Each sub-accelerator core is a full-fledged RNN neural processing unit with coarse-grained multithreading. It has many logical NPUs that share nearly all resources of the physical NPU, e.g., weights buffer, kernel units, activation function units, element-wise units, as shown in Figure 8(a). The core has VP MVM kernels, each having EP PEs, resulting in VP \u00d7 EP available PEs. The VP and EP values are determined via the design space exploration described in Reference [57]. In this article, each PE is one fully pipelined multiplier. The kernels are used to perform the matrix-vector multiplications between the weights and xt as well as ht\u22121 that is required in LSTM gates operations shown in Equations (1)."
                    },
                    {
                        "page": 11,
                        "region": {
                            "x1": 45.77299880981445,
                            "x2": 441.9876403808594,
                            "y1": 562.1453857421875,
                            "y2": 650.6800537109375
                        },
                        "text": "4.3.2 The MVM Kernel Units. Generally, the row-wise MVM is based on an architecture with parallel multipliers followed by a balanced adder tree. Accumulating the products of these multiplications is usually achieved using a balanced adder tree structure so that a number of related additions can be scheduled in parallel, which minimizes the overall latency of the system. This architecture of the kernel unit is commonplace in FPGA-based designs of RNNs [26, 69]. Since the elements in the partial result vector are not related, we adopt the columnwise MVM [57] that is based on the architecture of parallel multipliers followed by parallel accumulators. Besides, to support element-based parallelism, a small balanced adder tree is placed between the multipliers"
                    },
                    {
                        "page": 11,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.9438591003418,
                            "x2": 440.4678039550781,
                            "y1": 84.37843322753906,
                            "y2": 101.34576416015625
                        },
                        "text": "and the accumulators as shown in Figure 8(c). This adder tree can help to balance EP and VP to improve parallelism."
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.46664810180664,
                            "x2": 442.1802978515625,
                            "y1": 108.4460678100586,
                            "y2": 185.02825927734375
                        },
                        "text": "In our proposed multi-threaded NPU core, all the logical cores share all the MVM kernel units. When one logical core (thread) is stalled, the other can still conduct calculations using the same kernel units. Thus, the proposed CGMT NPU core can utilize the resources of MVM kernels efficiently to improve the NPU performance and hardware utilization. Since each thread requires its own input vector xt and ht\u22121, the design has to maintain multiple contexts by using buffers in the thread-aware vector manager and buffers unit. The thread selection logic is necessary to choose the required buffer to retrieve the data and conduct the remaining computations."
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.57575988769531,
                            "x2": 442.0474853515625,
                            "y1": 201.36988830566406,
                            "y2": 302.0200500488281
                        },
                        "text": "4.3.3 The Activation Function Unit. The\u03c3 / tanh unit performs the vector-based sigmoid (\u03c3 ) and hyperbolic tangent (tanh) operations. They are implemented using programmable lookup tables with size of 2,048 similar to References [28, 51]. The implementation using lookup tables brings several benefits. First, our NPU can run a trained model with custom activation functions (e.g., a hard sigmoid from Keras [15]) from our users without re-training of the model. Because the equations of the custom sigmoid/tanh are not changed in the model, re-training is unnecessary. More importantly, we do not touch the sensitive data of users, which is vital for many users. Second, the lookup table has a fixed latency (e.g., one cycle) but other implementations, e.g., a piecewise linear approximation, may involve multiplications that have a much larger pipe stage and latency."
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.57575988769531,
                            "x2": 442.15313720703125,
                            "y1": 318.51611328125,
                            "y2": 407.0597229003906
                        },
                        "text": "4.3.4 The LSTM Tail Units. Figure 8(d) illustrates the LSTM-tail units that perform the elementwise operations in the Equations (1). The LSTM memory cell FIFOs are employed to temporarily store the status of the running LSTMs, since one thread may be switched due to data hazard before the design finish the calculation of the current LSTM layer with multiple timesteps. Because these threads are performing different LSTM layers or models, the design must keep multiple contexts of cell status of LSTMs, as shown in Figure 8(d). Other hardware contexts, such as the input data and hidden units, are maintained in the thread-aware vector manager and buffer unit with the same mechanism."
                    }
                ],
                "title": {
                    "page": 11,
                    "region": {
                        "x1": 45.77315902709961,
                        "x2": 345.69720458984375,
                        "y1": 444.5812072753906,
                        "y2": 449.24371337890625
                    },
                    "text": "4.3 Detailed Hardware Components in One Sub-accelerator Core"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.847408294677734,
                            "x2": 441.564208984375,
                            "y1": 441.3290710449219,
                            "y2": 530.0255737304688
                        },
                        "text": "Since the proposed accelerator core can process a small tile with a size of (EP ,VP ) in each cycle, all the MVM kernel units in one core share the same input of a sub-vector of (xt ,ht\u22121), which has EP elements. These EP elements are broadcasted to all the MVM kernels for the computation. In addition, the design also needs to broadcast a single address to all the weight buffers units to fetch the weights. To alleviate the issue of large fan-out, the tree-shaped [51] inter-connection is adopted to decrease the fan-out of each node with pipeline stages between the source and destination. The RTL code is carefully written to enable the use of HyperFlex registers on Stratix 10 to get high operating frequency [32, 76]."
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.61528015136719,
                            "x2": 440.743896484375,
                            "y1": 537.1259155273438,
                            "y2": 649.5737915039062
                        },
                        "text": "The DSP blocks in modern FPGAs, which are highly configurable, are often underutilized when implementing 8-bit DNN systems. [84] illustrates methods to extract two INT8 multipliers from Xilinx DSP48E2 Blocks that contain a 27\u00d718 multiplier. Reference [41] introduces a method to pack 2 INT8 multiplications into one INT18 multiplier with extra ALMs. Both the methods require two multiplications to share one input operand. In the columnwise architecture [57], the computation order of MVM is different from the one in row-wise MVM. With the columnwise MVM used in RNN designs, one column of the weights matrix naturally shares the same element of the input vector and conducts the multiplications at the same time. Thus, these multiplications share one input operand, which helps us to pack four INT8 multiplications into one DSP block on Intel FPGAs [41] to decrease the hardware resources. In addition, this would not be a restriction (and will come"
                    },
                    {
                        "page": 12,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.404388427734375,
                            "x2": 440.2879638671875,
                            "y1": 258.4132080078125,
                            "y2": 275.21710205078125
                        },
                        "text": "at a lower cost) if we apply a novel DSP similar to the one that was presented in Reference [7] and will be adopted in the next generation Agilex devices [33]."
                    }
                ],
                "title": {
                    "page": 12,
                    "region": {
                        "x1": 45.94337844848633,
                        "x2": 200.6017608642578,
                        "y1": 426.73284912109375,
                        "y2": 431.3953552246094
                    },
                    "text": "4.4 FPGA-Specific Optimizations"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.47711181640625,
                            "x2": 441.96868896484375,
                            "y1": 307.7868347167969,
                            "y2": 324.5997314453125
                        },
                        "text": "This section introduces the experimental results on an Intel Stratix 10 FPGA that show the advances of the Remarn for RNN/LSTM accelerations."
                    }
                ],
                "title": {
                    "page": 13,
                    "region": {
                        "x1": 45.77399826049805,
                        "x2": 255.8154754638672,
                        "y1": 293.03619384765625,
                        "y2": 297.6986999511719
                    },
                    "text": "5 EXPERIMENTAL RESULTS AND ANALYSIS"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.47711181640625,
                            "x2": 440.3507385253906,
                            "y1": 357.1694641113281,
                            "y2": 421.7998046875
                        },
                        "text": "Table 2 lists some benchmark workloads that are chosen from several typical LSTM applications to make a fair and direct comparison of our design with others. Multi-tasked LSTM workloads are constructed randomly from these LSTM workloads. We evaluate the Remarn on an Intel Stratix 10 2800 (S10) and compare the results with other work. The Remarn runs the inferences of persistent LSTM models that store the weights in on-chip memory [19, 51, 57, 59, 66\u201368]. Remarn is designed using Verilog RTL. And Quartus Prime Pro 19.4 is used for the compilation of FPGA designs."
                    },
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.127288818359375,
                            "x2": 441.3995361328125,
                            "y1": 428.9001770019531,
                            "y2": 517.434326171875
                        },
                        "text": "The choice of the number of sub-core is based on the size of the benchmark LSTM workloads and the accelerator hardware architecture. The value of EP , explored in Reference [57], is set to 16, the same size as the previous designs [57, 59]. The value of VP should be chosen to be as large as possible. However, the largest effective value of VP is 4Lh as discussed in Section 3.2. Hence, the VP = 256 in this work, since the Lh = 64 for the smallest LSTM benchmark as shown in Table 2, resulting in 256\u00d716 = 4, 096 effective PEs for each sub-accelerator core. To make a fair comparison, the number of sub-accelerator cores, N , is set to 4 so that Remarn will have 16,386 PEs, which is the same as the number of total PEs in our previous designs [57, 59]."
                    }
                ],
                "title": {
                    "page": 13,
                    "region": {
                        "x1": 45.77399826049805,
                        "x2": 157.73370361328125,
                        "y1": 342.40985107421875,
                        "y2": 347.0723571777344
                    },
                    "text": "5.1 Experimental Setup"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.47869110107422,
                            "x2": 442.00421142578125,
                            "y1": 550.0130004882812,
                            "y2": 650.4998168945312
                        },
                        "text": "Table 3 shows the resource utilization of our designs with two configurations on FPGAs. The first design is a baseline design that is a single-threaded design without partition. It utilizes the parameter of (EP ,VP ) as (16, 1024), which includes 16,384 8-bit multipliers in the MVM kernels implemented using DSP blocks with extra ALMs. The second design is a dual-threaded quadcore Remarn that has four sub-accelerator cores. Each core utilizes the parameter of (EP ,VP ) as (16, 256). Thus, it has the same number of multipliers as the baseline design. The dual-threaded quad-core design consumes 5.0% more of total logic (ALMs) resources and 0.8% more block ram (M20K) than the baseline design. Since dual threads share the same physical core it consumes the same DSP resources as the single-threaded one."
                    },
                    {
                        "page": 13,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 13,
                    "region": {
                        "x1": 45.77558135986328,
                        "x2": 160.5537109375,
                        "y1": 535.25341796875,
                        "y2": 539.9158935546875
                    },
                    "text": "5.2 Resource Utilization"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 14,
                        "region": {
                            "x1": 45.648109436035156,
                            "x2": 442.1636657714844,
                            "y1": 261.203125,
                            "y2": 409.516357421875
                        },
                        "text": "To compare the performance of the proposed design on FPGA with other platforms, the DeepBench published results [89] on an Intel Xeon Syklake CPU and NVIDIA Tesla V100 GPU are used for comparison. TensorFlow is used for both CPU and GPU. Besides, the CPU with AVX2 Vector instructions enabled is utilized while the CuDNN libraries are enabled for the GPU. cuDNN is a GPU-accelerator Library from NVIDIA, which is specialized for deep learning. Both CPU and GPU implementations run with a batch size of 1, which provides the lowest service latency of cloud, since requests are required to be processed as soon as they arrive. For a fair comparison with the throughput of our dual-threaded Remarn, the throughput of the CPU and GPU have been doubled in Table 4. The GPU is significantly underutilized even when cuDNN library API calls, since it is designed for throughput-oriented workloads. It prefers BLAS level-3 (matrix-matrix) operations that are not common in RNN computations [89]. Our FPGA design of dual-threaded Remarn achieves 6.5 times higher performance and 15.6 times higher power efficiency, respectively, than the one running on the Tesla V100 GPU as shown in Table 4."
                    }
                ],
                "title": {
                    "page": 14,
                    "region": {
                        "x1": 45.94499969482422,
                        "x2": 349.8042907714844,
                        "y1": 246.45249938964844,
                        "y2": 251.114990234375
                    },
                    "text": "5.3 Performance and Efficiency Comparison with CPUs and GPUs"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 14,
                        "region": {
                            "x1": 45.57538986206055,
                            "x2": 442.1863098144531,
                            "y1": 442.3022766113281,
                            "y2": 580.2719116210938
                        },
                        "text": "To show the benefits of the proposed approaches, we compare the multi-threaded multi-core Remarn with the baseline design [57] in Figures 9 and 10. The baseline is a single-threaded singlecore accelerator. Hardware utilization is the percentage of the peak Tera Operations Per Second (TOPS) achieved for each LSTM, as compared with the peak TOPS of the design with all the PEs. Figure 9 shows the speedup of the quad-core Remarn over the baseline design [57]. With the proposed approaches of CGMT and multi-core, the quad-core Remarn designs of a single thread (1-T-Q), dual threads (2-T-Q) and quad threads (4-T-Q) achieve 1.82, 2.91, and 3.16 times higher performance respectively than the baseline when targeting mixed requests from LSTM workloads with h = 64, 128, and 256. Please note that Remarn has the same number of PEs as the baseline, and they consume the same DSP blocks on FPGAs. Particularly, with four threads (4-T-Q), Remarn achieves 14.44 times higher performance than the baseline [57] when targeting the small LSTMs (Lh = 64)."
                    },
                    {
                        "page": 14,
                        "region": {
                            "x1": 45.94491958618164,
                            "x2": 440.7078857421875,
                            "y1": 585.770263671875,
                            "y2": 650.3917236328125
                        },
                        "text": "When only targeting large LSTMs, the performance gain of the quad-core Remarn is small, since the baseline design [57] already achieves high hardware utilization for these LSTMs. However, the baseline still suffers from low utilization when targeting small-sized LSTMs that are commonly used in many applications [17, 55]. LSTM models that have a large number of timesteps but use small sizes are the most tangible examples that require dealing with lots of dependencies, as well as the parallel task of MVMs [86]. Our proposed approach and hardware architecture can alleviate"
                    },
                    {
                        "page": 14,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 15,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.5133972167969,
                            "y1": 525.9381713867188,
                            "y2": 566.6553955078125
                        },
                        "text": "this problem by leveraging the coarse-grained multithreading combining the spatial co-execution using multiple sub-accelerator cores. With the demand for high-performance designs, it is vital to maximize the hardware utilization to attain the best possible effective performance and power efficiency."
                    },
                    {
                        "page": 15,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.55389404296875,
                            "y1": 573.7646484375,
                            "y2": 650.3462524414062
                        },
                        "text": "When compared to our previous work [59] with only multithreading, Remarn can attain much higher hardware utilization and performance. Remarn achieves higher than 50% utilization for all the LSTM cases with Lh = 128 as shown in Figure 10(b) while the previous work can only achieve up to 36.4%. Particularly, with dual threads, the previous work achieves the utilization lower than 25% for LSTMs with Lh = 128; however, the proposed Remarn can achieve higher than 75%. Compared to our previous work [59], the Remarn proposed in this work achieves up to 3.61 times higher performance when targeting the small LSTMs, as shown in Figure 11, which"
                    },
                    {
                        "page": 15,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 440.49993896484375,
                            "y1": 263.06622314453125,
                            "y2": 315.74444580078125
                        },
                        "text": "shows the advances of the proposed architecture using multiple sub-accelerator cores. There is a small reduction from a linear speedup for the model size of 64, which is due to control logic added to each small cores when splitting the big engine. The impact is negligible when the workload is large with a long runtime. We believe this can be optimized with more efforts but doing so has little impact on the conclusions we draw from this study."
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.7198486328125,
                            "x2": 440.65521240234375,
                            "y1": 322.8448181152344,
                            "y2": 411.3794250488281
                        },
                        "text": "The design with multithreading alone shows a steady increase from 1-T to 2-T and to 4-T [59] for the model size of 128. However, the additional speedups for the quad-core design dropped when it is from 2-T to 4-T threads as shown in Figure 11. It is because with 2-T on small cores, the design can already remove most of the stalls caused by the RNN data dependencies for the model size of 128. Hence, the quad-core design with 2-T and 4-T achieves a similar performance and runtime hardware utilization as shown in Figure 10(b). But with 2-T on the big core, the design still suffers from some stalls so the performance of 4-T on the big core is much better than 2-T. Hence, the additional speedups drop when going from 2-T to 4-T threads on the quad-core design."
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.57538986206055,
                            "x2": 441.57891845703125,
                            "y1": 418.48876953125,
                            "y2": 530.9366455078125
                        },
                        "text": "Moreover, when compared to References [57, 59], which has a single large core, the quad-core Remarn has a slight performance drop (less than 2%) when all 4 cores are working together as a big accelerator to run a large workload, as shown in Tables 4 and 5. It is because the multi-core design can lead to synchronization costs than the one using a monolithic accelerator. When four cores are running the same large workload, a synchronization mechanism is required to maintain the correct data dependencies in RNNs. It is a design tradeoff. With slight performance loss on the large workloads, the design can achieve much higher performance on the small workloads, which can result in a much better performance gain for all the workloads. Besides, the users can re-configure the FPGA into a big and monolithic accelerator when most of large RNN workloads are large, which can achieve the best performance."
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.64712142944336,
                            "x2": 440.4771728515625,
                            "y1": 538.0369262695312,
                            "y2": 594.5910034179688
                        },
                        "text": "The evaluation results also show that Remarn gets a low utilization when running small LSTMs (h = 64), as shown in Figure 10(b). Even with four threads, the design still stalls, because the cycles of processing MVMs in the other threads still cannot completely hide the whole pipeline latency to get the ht available before it is required for the next timestep. The number of cycles to process the MVM of the LSTM gates is given by Lx+Lh EP"
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.94485855102539,
                            "x2": 440.7328796386719,
                            "y1": 586.1467895507812,
                            "y2": 650.9315795898438
                        },
                        "text": ". Different choices of (EP , VP ) impact the performance and utilization of the designs that run LSTMs with different sizes. There is a tradeoff between the extra performance that can be achieved and the hardware design complexity as well as extra hardware resources of supporting various sets of (EP ,VP ) at runtime. It is our future work to enhance this accelerator architecture to support various sets of (EP ,VP ), since different sizes of LSTMs may prefer different optimal EP and VP parameters."
                    },
                    {
                        "page": 16,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 14,
                    "region": {
                        "x1": 45.944000244140625,
                        "x2": 333.0631408691406,
                        "y1": 427.5516357421875,
                        "y2": 432.2141418457031
                    },
                    "text": "5.4 Performance and Efficiency Comparison with the Baseline"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 17,
                        "region": {
                            "x1": 45.404388427734375,
                            "x2": 441.99468994140625,
                            "y1": 297.7071228027344,
                            "y2": 505.7982177734375
                        },
                        "text": "Table 5 shows the comparison between our Remarn and the existing LSTM accelerator implementations using a Stratix 10 GX 2800 FPGA. For a fair comparison, we only show the previous work with a detailed implementation of the LSTM system using the same FPGA. We list the model storage, precision, DSP used, runtime frequency, power, throughput and power efficiency as well as the hardware (HW) utilization. The thermal design power (TDP) that is 125 W is utilized for a fair comparison, since it is reported in all the listed work. Overall, Remarn provides over 4.87 times higher performance and 5.71 times higher hardware utilization than the SOTA design [51] when targeting LSTM models with ht =256, as shown in Table 5. Compared with the baseline that is a single-threaded single-core design [57], Remarn achieves 1.45 to 14.4 higher throughput and HW utilization. Since multithreading also ensures there is no downside to peak performance in a single-threaded mode, the Remarn can achieve a similar peak performance with the one in Reference [57], which is the highest with respect to the SOTA FPGA-based RNN/LSTM designs with a commonly used INT8 precision. The only prior work that has a higher peak performance is Reference [19] that employs an 8-bit block floating-point representation. However, when targeting the small LSTM models, the throughput of the proposed Remarn is 18.82 times higher than [19] as shown in Table 5. Moreover, Remarn achieves the highest hardware utilization among all the listed designs across various LSTM models. This work focuses on maximizing throughput and minimizing latency by increasing HW utilization."
                    }
                ],
                "title": {
                    "page": 17,
                    "region": {
                        "x1": 45.77399826049805,
                        "x2": 400.1047668457031,
                        "y1": 282.95648193359375,
                        "y2": 287.6189880371094
                    },
                    "text": "5.5 Performance and Efficiency Comparison with Other FPGA-based Designs"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 17,
                        "region": {
                            "x1": 45.40460968017578,
                            "x2": 441.99072265625,
                            "y1": 538.1616821289062,
                            "y2": 650.6095581054688
                        },
                        "text": "The scheduling strategy for multi-core accelerators for DNN workloads is a new and rarely explored topic. PREMA [14] introduces a preemptive scheduling algorithm on a single-core NPU to support multi-DNN execution. AI-MultiTasking [4] proposes to process different neural networks by partitioning their various layers into identical sub-layers in compile time. It also develops a heuristic of mapping DNN jobs to the accelerator. In Reference [40], Heterogeneous Dataflow accelerators (HDAs) are proposed to support the heterogeneous acceleration of multiDNN workloads using manual-designed mapping of jobs to sub-accelerators and compile time layerwise scheduling. LayerWeaver [53] introduces a layerwise time-multiplexing scheduler on single NPU by interweaving layer execution to increase the hardware utilization. Besides, MAGMA [36] proposes a novel framework for mapping multiple DNNs on multiple accelerator cores."
                    },
                    {
                        "page": 17,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 18,
                        "region": {
                            "x1": 45.57624053955078,
                            "x2": 442.17156982421875,
                            "y1": 84.5328598022461,
                            "y2": 280.67254638671875
                        },
                        "text": "The focus of this article is on the accelerator architecture, and on examples of optimal utilization of that architecture based on fine-grained scheduling of tasks made up of LSTM sub-layers running on sub-cores. We exploit the intra-model data dependencies of both timesteps and layers in LSTMs, as shown in Figure 7(a). Both time step and layer dependencies have a linear dependence chain in the model. For LSTM inference, it is processed from the first layer to the last layer sequentially and from the first timestep to the last timestep in each layer. Besides, layers from different LSTMs are independent. Inspired by References [4, 40], which rely on manually designed heuristics, this work develops a set of heuristics that exploit the characteristics of LSTM inference workloads to reduce the scheduling overhead for the proposed homogeneous multi-core accelerator. We do not include algorithmic details (e.g., flowchart or pseudocode) about scheduling of these sub-layers tasks, other than mentioning related research on scheduling strategies [4, 14, 40] and LayerWeaver [53] as well as the custom mapping algorithm MAGMA [36]. Architectural tradeoffs of these scheduling strategies, based on queuing theory and other techniques, are important and would be best addressed by future work. Besides, further research will cover fine-grained dynamic scheduling for our accelerator, such as using Gang scheduling that schedules a gang of jobs onto free cores. Furthermore, other sophisticated scheduling strategies relevant to our approach will be studied and tested."
                    }
                ],
                "title": {
                    "page": 17,
                    "region": {
                        "x1": 45.77421951293945,
                        "x2": 119.16970825195312,
                        "y1": 523.4111328125,
                        "y2": 528.0736083984375
                    },
                    "text": "5.6 Scheduling"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 18,
                        "region": {
                            "x1": 45.64796829223633,
                            "x2": 442.1685791015625,
                            "y1": 314.34014892578125,
                            "y2": 390.92266845703125
                        },
                        "text": "The proposed extension of the multi-core feature is based on splitting a large monolithic accelerator into several sub-accelerator cores, which does not change the total number of existing PEs in hardware. There is only a small bump in the hardware resources for the extended accelerator because of supporting new features, which may cause a little more power/energy consumption. However, the extra power consumption caused by extra FPGA hardware resources is small, since the new design only has 5% more area than the baseline [57] but can get 1.45 to 14.4 times higher performance, resulting in high power efficiency."
                    },
                    {
                        "page": 18,
                        "region": {
                            "x1": 45.57722854614258,
                            "x2": 442.16558837890625,
                            "y1": 398.031982421875,
                            "y2": 534.39306640625
                        },
                        "text": "Besides, the architectural extensions lead to higher runtime dynamic power, since these extensions provide more parallelism and higher hardware utilization. In general, the performance and power are positively correlated; the more accelerators used, the higher the power consumption. However, higher utilization also means that our architecture wastes less power on the underutilized execution resources than the baseline [57], which increases the overall power/energy efficiency. The overall increase in utilization also means the proposed design completes the same work in a shorter period of time, resulting in a potential lower energy consumption. Besides, with multiple sub-accelerator cores, the design has the potential to totally turn off some of the cores when there is only a small amount of workload to save power, which is not possible in the baseline where only a big and monolithic core is used. In future work, we will build an architecture-level power model targeting multi-threaded multi-core DNN accelerators so that we can quantify the impact of these architectural extensions on power/energy consumption."
                    },
                    {
                        "page": 18,
                        "region": {
                            "x1": 45.57722854614258,
                            "x2": 442.0230712890625,
                            "y1": 541.493408203125,
                            "y2": 641.9891357421875
                        },
                        "text": "There are some potential optimizations for the power consumption targeting our architecture. Since the proposed design achieves a high hardware utilization, it may not require so many PEs to reach a design performance goal. Reducing the number of total PEs or reducing the hardware resources in design time can help to reduce the total power consumption. However, the design causes large dynamic power consumption because of the high utilization. Hence, one possible optimization is to control dynamic power consumption by gracefully degrading the core activities when a power consumption threshold is exceeded. It can be done by a feedback of the current power consumption of the cores, which may be estimated from some performance counters or read from some on-die sensors."
                    },
                    {
                        "page": 18,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 18,
                    "region": {
                        "x1": 45.94485092163086,
                        "x2": 248.35299682617188,
                        "y1": 299.5895080566406,
                        "y2": 304.25201416015625
                    },
                    "text": "5.7 Power Consumption and Optimizations"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 19,
                        "region": {
                            "x1": 45.405540466308594,
                            "x2": 442.00482177734375,
                            "y1": 99.4817123413086,
                            "y2": 307.57354736328125
                        },
                        "text": "There are many previous studies about FPGA-based accelerations of persistent RNN/LSTM with weights stored in on-chip memory [19, 51, 58, 66\u201369]. Rybalkin et al. [69] introduces the hardware architecture of BiLSTM targeting OCR applications. All the weights and activations are quantized to 5-bit and stored in on-chip memory. [66] quantizes the design using 1-8 bits and achieves an accuracy that surpasses the one using floating-point on a given dataset. Besides, their later work [67, 68] proposes a novel hardware architecture of 2D-LSTM and investigates the tradeoff between precision and performance. There are also many previous studies about LSTM implementations with weights stored in off-chip memory on FPGAs [10, 26, 27, 54, 61], which has been recognized as the performance bottleneck. In LSTM computations, a weights matrix tends to be repeatedly loaded from off-chip memory if the size of on-chip memory is not large enough to store the entire matrix, which brings large latency. Guan et al. [27] proposes a smart memory organization with on-chip double buffers to overlap computations with data transfers. And References [22, 72] apply the batching technique to increase the throughput of LSTM inferences. For example, E-BATCH is proposed [72] for RNNs, which increases throughput while also improving energy efficiency on an ASIC-based accelerator. Apart from batching, References [54, 61, 63] introduce novel LSTM weights reuse schemes that utilizes the weights sharing characteristics in different timestep computations in one inference. These schemes reduce the access of the off-chip memory and decrease the energy cost as well as improve the design throughput."
                    },
                    {
                        "page": 19,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 442.0078430175781,
                            "y1": 314.67388916015625,
                            "y2": 486.8971252441406
                        },
                        "text": "Some of the previous work [42, 80, 81, 85] adopts circulant matrix to optimize LSTMs by reducing the weight matrices of LSTM inferences. Besides, an approximate computing scheme is deployed for LSTMs using small tiles [65]. And stochastic computing is used to improve the energy efficiency of the RNN inference [43]. Reference [37] proposes an LSTM architecture based on reversible logic gates for low power circuit designs. POLAR [5] and BRDS [23] present FPGA-based pipelined and overlapped architecture for dense and sparse LSTM inferences, respectively. A multiFPGA approach [74] is proposed to accelerate multi-layer RNNs. It achieves a single-layer latency targeting deep RNNs with arbitrarily multiple layers using an FPGA-based cluster. Reference [49] presents a multi-FPGA-based architecture to accelerate neural machine translation. Reference [39] explores various partitioning strategies of large RNN inferences to achieve scalable multi-FPGA acceleration. It also analyses the performance impact of software pipelining and collective communications. PDL-FGPU [44], a specialized FPGA-based GPU overlay architecture is proposed to run persistent Deep Learning, including various RNN variants. Initiation interval (II) balancing [62] is proposed with a layerwise implementation of LSTMs to achieve ultra low latency. The layerwise implementation of RNNs is also used in accelerating Bayesian RNNs [18]."
                    },
                    {
                        "page": 19,
                        "region": {
                            "x1": 45.7759895324707,
                            "x2": 442.0016784667969,
                            "y1": 493.9974670410156,
                            "y2": 642.3197021484375
                        },
                        "text": "There is also much previous work [9, 11, 13, 20, 21, 28, 34, 48, 70, 83, 90] exploit the sparsity of data and weights with pruning to reduce the NN computation and also the memory footprint to achieve high performance and efficiencies. ESE [28] proposes a pruning technique that compresses a large LSTM model by 20\u00d7 without sacrificing the prediction accuracy. DeltaRNN [21] utilizes the Delta Network algorithm to reduce MxV operations and corresponding weight fetches by skipping dispensable computations during inference of RNNs. It updates the output of a neuron only when the neuron\u2019s activation changes by more than a delta threshold. Bank-Balanced Sparsity [9] is proposed to achieve both high prediction accuracy of a pruned LSTM model and high hardware efficiency of the model running on FPGAs. It partitions weight matrix rows into banks for parallel computing and adopts fine-grained pruning inside each bank to maintain model accuracy. BLINK [11] designs the LSTM inference using a bit-sparse data representation. And it turns multiplications into bit-shift operation to improve the energy efficiency while maintaining the LSTM inference accuracy for real-time calcium image processing. The extension [12] proposes a"
                    },
                    {
                        "page": 19,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 20,
                        "region": {
                            "x1": 45.94485092163086,
                            "x2": 442.1626281738281,
                            "y1": 84.5328598022461,
                            "y2": 137.21112060546875
                        },
                        "text": "combination of the bit-sparse quantization and the pruning methods for energy-efficient LSTM inferences. More recently, Spartus [20] exploits spatio-temporal sparsity to achieve ultra-low-latency inference using Column-Balanced Targeted Dropout. These studies are orthogonal to our proposed approach and hardware architecture. These techniques can be complementary to our approaches to achieve even higher performance and efficiency of RNN/LSTM inferences using FPGAs."
                    },
                    {
                        "page": 20,
                        "region": {
                            "x1": 45.57524108886719,
                            "x2": 442.1665344238281,
                            "y1": 144.31141662597656,
                            "y2": 328.4900207519531
                        },
                        "text": "The Brainwave design [19] is a single-threaded SIMD architecture for running real-time AI, including persistent RNNs. It achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large memory intensive RNN models at a batch size of 1. It stores NN model weights on-chip for RNNs to get a necessary high memory read bandwidth to achieve higher performances. A coarse-grained reconfigurable architecture (CGRA)-based RNN accelerator is proposed [89] on top of Plasticine [56] with a set of techniques for performing cross-kernel optimization in RNN cells. AERO [38] is a single-threaded instruction-set-based processor using a versatile vector-processing unit customized for RNN inferences on resourcelimited FPGAs. A Brainwave-like NPU is proposed in Reference [51] with a single-threaded architecture. They also explore the potential of combining a TensorRAM with FPGAs to provide large high-speed memory for large memory intensive RNN sequence models. Besides, their late work [8] deploys the Brainwave-like NPU on the Stratix 10 NX that is Intel\u2019s new AI-optimized FPGA featured with AI tensor blocks. [57, 60] proposes a novel latency-hiding hardware architecture based on columnwise MVM and fine-grained tiling strategy to eliminate data dependency of RNNs, which improves the design throughput of RNNs/LSTMs. However, all these NPUs targeting RNNs/LSTMs are single threaded."
                    },
                    {
                        "page": 20,
                        "region": {
                            "x1": 45.574241638183594,
                            "x2": 442.16961669921875,
                            "y1": 335.599365234375,
                            "y2": 651.2825927734375
                        },
                        "text": "There is a large demand for architectural support of multi-DNN execution to maximize hardware utilization and reduce the costs of a large-scale production system. For example, TensorRT from Nvidia supports concurrent DNN execution for users to run multi-DNN workloads on the same GPU simultaneously. SMT-SA [71] presents a simultaneous multithreading approach for systolic arrays to solve the issue of underutilization because of zero-valued inputs. However, it is not able to deal with the underutilization issue that comes from the data dependencies in RNNs/LSTMs. A multi-threaded CGRA [3] has been proposed, but it is only for CNN accelerations. Reference [50] presents to generate in-order multi-threaded processing pipelined datapath automatically with the high-level specification of an unpipelined datapath. CUSTARD [16] presents a customisable multi-threaded FPGA soft processor with features including design space exploration and a compiler for automatic selection of custom instructions. More recently, PREMA [14] proposes a preemptible NPU with a preemptive scheduling algorithm to support multi-DNN execution. However, it does not consider the data dependencies between RNN/LSTM timesteps. AIMultiTasking [4] proposes to balance memory-intensive and compute-intensive tasks from different neural networks and process them in parallel by partitioning various layers into lots of identical sub-layers. The RNNs are handled just as FC layers in the scheduling scheme. However, RNNs are more complex than FC layers that have no data dependencies. In Reference [55], a dualcore accelerator for LSTM-RNN is proposed to execute multiple jobs simultaneously or have cores collaborate on a single job. However, they perform multithreading by utilizing the dual cores with one thread on one core, which is not an area / cost efficient way to add extra threads. Besides, there is some previous work targeting spatial multi-tenant execution. f-CNNx [78] employs multiple custom dedicated engines for the workloads with various CNN models. Reference [64] maps multiple LSTM models to an FPGA device by introducing a framework that alters their computational structure, opening opportunities for co-optimizing the memory requirements to the target architecture via applying compression schemes across multiple LSTM models. In Reference [40] HDAs are proposed to support the heterogeneous acceleration of multi-DNN workloads. More recently, Planaria [24] introduces the dynamical fission of a systolic-array-based DNN accelerator"
                    },
                    {
                        "page": 20,
                        "region": {
                            "x1": 45.900001525878906,
                            "x2": 440.4541320800781,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 45.77415084838867,
                            "x2": 442.00579833984375,
                            "y1": 84.5328598022461,
                            "y2": 173.07647705078125
                        },
                        "text": "at runtime to support spatial co-execution of multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations. This work designs a multi-threaded accelerator with fission to enable spatial and temporal co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations. It can handle multiple requests using multi-threaded mode with multiple sub-accelerator cores while still being able to run a task in a single-threaded mode with a large accelerator to attain higher performance and lower latency when targeting the workloads with high priority."
                    }
                ],
                "title": {
                    "page": 19,
                    "region": {
                        "x1": 45.77415084838867,
                        "x2": 139.76329040527344,
                        "y1": 84.7221450805664,
                        "y2": 89.3846435546875
                    },
                    "text": "6 RELATED WORK"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 21,
                        "region": {
                            "x1": 45.47726821899414,
                            "x2": 441.9931945800781,
                            "y1": 204.8810577392578,
                            "y2": 436.88494873046875
                        },
                        "text": "This work proposes Remarn, a multi-threaded multi-core NPU supporting spatial and temporal coexecution of RNN/LSTM inferences to improve the processing abilities of cloud-based NPUs. The Remarn can increase the throughput while improving the hardware utilization of the cloud-based NPUs. We have implemented the proposed multi-threaded multi-core accelerator architecture on Intel Stratix 10 FPGAs with superior performance and efficiency, which demonstrates the effectiveness of our approaches. Our study shows that both multithreading and multi sub-accelerator core techniques can address the underutilization issue (resource inefficiency) when targeting small sized RNN workloads. Besides, it also shows that multi sub-accelerator core is slightly better than multithreading for small workloads. The design of quad-core with single-thread (1-T-Q) achieves a better utilization than the design of single-core quad-thread (4-T) for the LSTM of Lh = 128, as shown in Figure 10. But this figure also shows that the single-core quad-thread (4-T) design gets a better performance for the LSTM of Lh = 256. In addition, both techniques do not bring benefits to large LSTM workloads, since the baseline design has achieved high utilization. Moreover, this study shows that multithreading targets temporal co-execution, while multi-core targets spatial co-execution, and the two techniques can be combined to achieve a much better performance. Further research includes exploring how Remarn can benefit from other enhancements such as dynamic scheduling and runtime power analysing, studying the tradeoffs of varying the number and the heterogeneity of Remarn cores, and automating the proposed approaches to enable rapid development of efficient Remarn designs for datacentres as well as edge processing and embedded systems."
                    }
                ],
                "title": {
                    "page": 21,
                    "region": {
                        "x1": 45.77415084838867,
                        "x2": 234.12307739257812,
                        "y1": 190.13047790527344,
                        "y2": 194.79296875
                    },
                    "text": "7 CONCLUSIONS AND FUTURE WORK"
                }
            },
            {
                "paragraphs": [
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472999572753906,
                            "x2": 441.16424560546875,
                            "y1": 467.6705627441406,
                            "y2": 471.552001953125
                        },
                        "text": "[1] Mohamed S. Abdelfattah, David Han, Andrew Bitar, Roberto DiCecco, Shane O\u2019Connell, Nitika Shanker, Joseph Chu,"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472198486328125,
                            "x2": 441.51983642578125,
                            "y1": 477.6331787109375,
                            "y2": 511.4024963378906
                        },
                        "text": "Ian Prins, Joshua Fender, Andrew C. Ling, et al. 2018. DLA: Compiler and FPGA overlay for neural network inference acceleration. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 411\u20134117. [2] Dario Amodei et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In Proceedings of"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.84682846069336,
                            "x2": 223.2610321044922,
                            "y1": 517.4837036132812,
                            "y2": 521.3651123046875
                        },
                        "text": "the International Conference on Machine Learning."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472999572753906,
                            "x2": 441.63934326171875,
                            "y1": 527.4463500976562,
                            "y2": 531.3277587890625
                        },
                        "text": "[3] Kota Ando, Shinya Takamaeda-Yamazaki, Masayuki Ikebe, Tetsuya Asai, and Masato Motomura. 2017. A multi-"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.84682846069336,
                            "x2": 363.32049560546875,
                            "y1": 537.4089965820312,
                            "y2": 541.2904052734375
                        },
                        "text": "threaded CGRA for convolutional neural network processing. Circ. Syst. 8, 6 (2017), 149\u2013170."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472999572753906,
                            "x2": 440.2733154296875,
                            "y1": 547.37158203125,
                            "y2": 551.2529907226562
                        },
                        "text": "[4] Eunjin Baek, Dongup Kwon, and Jangwoo Kim. 2020. A multi-neural network acceleration architecture. In Proceedings"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.84682846069336,
                            "x2": 405.69012451171875,
                            "y1": 557.334228515625,
                            "y2": 561.2156372070312
                        },
                        "text": "of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA\u201920). IEEE, 940\u2013953."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472999572753906,
                            "x2": 441.22808837890625,
                            "y1": 567.2968139648438,
                            "y2": 571.17822265625
                        },
                        "text": "[5] Erfan Bank-Tavakoli, Seyed Abolfazl Ghasemzadeh, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2019. Polar:"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.56787109375,
                            "x2": 374.1654357910156,
                            "y1": 577.2594604492188,
                            "y2": 581.140869140625
                        },
                        "text": "A pipelined/overlapped fpga-based lstm accelerator. IEEE Trans. VLSI Syst. 28, 3 (2019), 838\u2013842."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.472999572753906,
                            "x2": 440.2868347167969,
                            "y1": 587.222900390625,
                            "y2": 591.1043090820312
                        },
                        "text": "[6] John M. Borkenhagen et al. 2000. A multithreaded PowerPC processor for commercial servers. IBM J. Res. Dev. 44, 6"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.60454177856445,
                            "x2": 114.29220581054688,
                            "y1": 597.1854858398438,
                            "y2": 601.06689453125
                        },
                        "text": "(2000), 885\u2013898."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.47380065917969,
                            "x2": 440.25262451171875,
                            "y1": 607.1481323242188,
                            "y2": 611.029541015625
                        },
                        "text": "[7] Andrew Boutros et al. 2018. Embracing diversity: Enhanced DSP blocks for low-precision deep learning on FPGAs. In"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.847618103027344,
                            "x2": 397.3136291503906,
                            "y1": 617.1107788085938,
                            "y2": 620.9921875
                        },
                        "text": "Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 49.47380065917969,
                            "x2": 440.2796325683594,
                            "y1": 627.0734252929688,
                            "y2": 630.954833984375
                        },
                        "text": "[8] Andrew Boutros, Eriko Nurvitadhi, Rui Ma, Sergey Gribok, Zhipeng Zhao, James C. Hoe, Vaughn Betz, and Martin"
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 62.847618103027344,
                            "x2": 441.484619140625,
                            "y1": 637.0368041992188,
                            "y2": 650.8816528320312
                        },
                        "text": "Langhammer. 2020. Beyond peak performance: Comparing the real performance of AI-optimized FPGAs and GPUs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 10\u201319."
                    },
                    {
                        "page": 21,
                        "region": {
                            "x1": 45.49591827392578,
                            "x2": 440.0508117675781,
                            "y1": 671.3090209960938,
                            "y2": 675.1904296875
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 49.65299987792969,
                            "x2": 440.4795227050781,
                            "y1": 85.50357055664062,
                            "y2": 89.385009765625
                        },
                        "text": "[9] Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun Nie, Dechen Zhan, Yunxin Liu, Ming Wu, and"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.945308685302734,
                            "x2": 441.80401611328125,
                            "y1": 95.46615600585938,
                            "y2": 119.273681640625
                        },
                        "text": "Lintao Zhang. 2019. Efficient and effective sparse LSTM on FPGA with bank-balanced sparsity. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 63\u201372. [10] Andre Xian Ming Chang, Berin Martini, and Eugenio Culurciello. 2015. Recurrent neural networks hardware imple-"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.01805877685547,
                            "x2": 346.2196044921875,
                            "y1": 125.35488891601562,
                            "y2": 129.236328125
                        },
                        "text": "mentation on FPGA. arXiv:1511.05552. Retrieved from https://arxiv.org/abs/1511.05552."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.945308685302734,
                            "x2": 440.4444885253906,
                            "y1": 135.31747436523438,
                            "y2": 139.19891357421875
                        },
                        "text": "[11] Zhe Chen, Garrett J. Blair, Hugh T. Blair, and Jason Cong. 2020. BLINK: Bit-sparse LSTM inference kernel enabling"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.945308685302734,
                            "x2": 440.45172119140625,
                            "y1": 145.28012084960938,
                            "y2": 169.0867919921875
                        },
                        "text": "efficient calcium trace extraction for neurofeedback devices. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 217\u2013222. [12] Zhe Chen, Hugh T. Blair, and Jason Cong. 2022. Energy efficient LSTM inference accelerator for real-time causal"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 440.98724365234375,
                            "y1": 175.16799926757812,
                            "y2": 198.9754638671875
                        },
                        "text": "prediction. ACM Trans. Des. Autom. Electr. Syst. 27, 5, Article 44 (September 2022), 19 pages. https://doi.org/10.1145/ 349500 [13] Zhe Chen, Andrew Howe, Hugh T. Blair, and Jason Cong. 2018. CLINK: Compact LSTM inference kernel for energy"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.81695556640625,
                            "y1": 205.056640625,
                            "y2": 228.86334228515625
                        },
                        "text": "efficient neurofeedback devices. In Proceedings of the International Symposium on Low Power Electronics and Design. 1\u20136. [14] Yujeong Choi and Minsoo Rhu. 2020. PREMA: A predictive multi-task scheduling algorithm for preemptible neu-"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 62.53268051147461,
                            "x2": 440.4564514160156,
                            "y1": 234.94451904296875,
                            "y2": 248.78860473632812
                        },
                        "text": "ral processing units. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201920). IEEE, 220\u2013233."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 440.4676513671875,
                            "y1": 254.86978149414062,
                            "y2": 268.7138366699219
                        },
                        "text": "[15] Fran\u00e7ois Chollet et al. 2015. Keras: Deep Learning Library for theano and tensorflow. https://keras.io/k. [16] R. Dimond, O. Mencer, and W. Luk. 2006. Application-specific customisation of multi-threaded soft processors. IEE"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.01885986328125,
                            "x2": 223.32618713378906,
                            "y1": 274.7950439453125,
                            "y2": 278.6764831542969
                        },
                        "text": "Proc. Comput. Digit. Techn. 153, 3 (2006), 173\u2013180."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.3148498535156,
                            "y1": 284.7576599121094,
                            "y2": 288.63909912109375
                        },
                        "text": "[17] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.7978210449219,
                            "y1": 294.7115173339844,
                            "y2": 318.5181884765625
                        },
                        "text": "and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2625\u20132634. [18] Martin Ferianc, Zhiqiang Que, Hongxiang Fan, Wayne Luk, and Miguel Rodrigues. 2021. Optimizing Bayesian re-"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.27020263671875,
                            "y1": 324.5993957519531,
                            "y2": 348.40606689453125
                        },
                        "text": "current neural networks on an FPGA-based accelerator. In Proceedings of the International Conference on FieldProgrammable Technology (ICFPT\u201921). IEEE, 1\u201310. [19] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 440.4541015625,
                            "y1": 354.4872741699219,
                            "y2": 378.2939453125
                        },
                        "text": "Haselman, Logan Adams, Mahdi Ghandi, et al. 2018. A configurable cloud-scale DNN processor for real-time AI. In Proceedings of the 45th Annual International Symposium on Computer Architecture. IEEE Press, 1\u201314. [20] Chang Gao, Tobi Delbruck, and Shih-Chii Liu. 2021. Spartus: A 9.4 TOp/s FPGA-based LSTM accelerator exploiting"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.01885986328125,
                            "x2": 388.5671081542969,
                            "y1": 384.3751525878906,
                            "y2": 388.256591796875
                        },
                        "text": "spatio-temporal sparsity. IEEE Transactions on Neural Networks and Learning Systems. (Early Access)"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 440.45782470703125,
                            "y1": 394.3377685546875,
                            "y2": 398.2192077636719
                        },
                        "text": "[21] Chang Gao, Daniel Neil, Enea Ceolini, Shih-Chii Liu, and Tobi Delbruck. 2018. DeltaRNN: A power-efficient recurrent"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.27105712890625,
                            "y1": 404.3003845214844,
                            "y2": 428.1070861816406
                        },
                        "text": "neural network accelerator. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 21\u201330. [22] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency RNN inference with cellular batching. In Pro-"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.019649505615234,
                            "x2": 362.48492431640625,
                            "y1": 434.1882629394531,
                            "y2": 438.0697021484375
                        },
                        "text": "ceedings of the 13th European Conference on Computer Systems Conference (EuroSys\u201918). 1\u201315."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.9468994140625,
                            "x2": 441.4056091308594,
                            "y1": 444.15087890625,
                            "y2": 448.0323181152344
                        },
                        "text": "[23] Seyed Abolfazl Ghasemzadeh, Erfan Bank Tavakoli, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2021. BRDS:"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.9468994140625,
                            "x2": 441.4033508300781,
                            "y1": 454.113525390625,
                            "y2": 477.9201965332031
                        },
                        "text": "An FPGA-based LSTM accelerator with row-balanced dual-ratio sparsification. arXiv:2101.02667. Retrieved from https: //arxiv.org/abs/2101.02667. [24] Soroush Ghodrati, Byung Hoon Ahn, Joon Kyung Kim, Sean Kinzer, Brahmendra Reddy Yatham, Navateja Alla, Hardik"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.9468994140625,
                            "x2": 441.3603515625,
                            "y1": 484.0013732910156,
                            "y2": 517.7706909179688
                        },
                        "text": "Sharma, Mohammad Alian, Eiman Ebrahimi, Nam Sung Kim, et al. 2020. Planaria: Dynamic architecture fission for spatial multi-tenant acceleration of deep neural networks. In Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO\u201920). IEEE, 681\u2013697. [25] Yoav Goldberg. 2016. A primer on neural network models for natural language processing. J. Artif. Intell. Res. 57 (2016),"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.019649505615234,
                            "x2": 91.38284301757812,
                            "y1": 523.8519287109375,
                            "y2": 527.7333374023438
                        },
                        "text": "345\u2013420."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.9468994140625,
                            "x2": 440.46832275390625,
                            "y1": 533.8145141601562,
                            "y2": 537.6959228515625
                        },
                        "text": "[26] Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai Shi, Xi Chen, Guangyu Sun, Wei Zhang, and Jason"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.9468994140625,
                            "x2": 440.6851501464844,
                            "y1": 543.7683715820312,
                            "y2": 577.5376586914062
                        },
                        "text": "Cong. 2017. FP-DNN: An automated framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates. In Proceedings of the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201917). IEEE, 152\u2013159. [27] Yijin Guan, Zhihang Yuan, Guangyu Sun, and Jason Cong. 2017. FPGA-based accelerator for long short-term memory"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.3539123535156,
                            "y1": 583.618896484375,
                            "y2": 607.4263916015625
                        },
                        "text": "recurrent neural networks. In Proceedings of the 22nd Asia and South Pacific Design Automation Conference (ASPDAC\u201917). IEEE, 629\u2013634. [28] Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao, Yu Wang,"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.946109771728516,
                            "x2": 441.32666015625,
                            "y1": 613.507568359375,
                            "y2": 637.314208984375
                        },
                        "text": "et al. 2017. ESE: Efficient speech recognition engine with sparse LSTM on FPGA. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 75\u201384. [29] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,"
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 63.01885986328125,
                            "x2": 441.7091979980469,
                            "y1": 643.3954467773438,
                            "y2": 657.239501953125
                        },
                        "text": "Shubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv:1412.5567. Retrieved from https://arxiv.org/abs/1412.5567."
                    },
                    {
                        "page": 22,
                        "region": {
                            "x1": 45.90068054199219,
                            "x2": 440.4548034667969,
                            "y1": 671.2947998046875,
                            "y2": 675.1762084960938
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.2969970703125,
                            "y1": 85.50357055664062,
                            "y2": 99.34759521484375
                        },
                        "text": "[30] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput. 9, 8 (1997), 1735\u20131780. [31] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. 2018. Detecting"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84674835205078,
                            "x2": 440.3011779785156,
                            "y1": 105.42880249023438,
                            "y2": 119.273681640625
                        },
                        "text": "spacecraft anomalies using LSTM and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 387\u2013395."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77320098876953,
                            "x2": 440.261962890625,
                            "y1": 125.35488891601562,
                            "y2": 149.16156005859375
                        },
                        "text": "[32] Intel. [n.d.]. Understanding How Hyperflex Architecture Enables High Performance Systems. White Paper 01231. [33] Intel. 2020. Intel Agilex Variable Precision DSP Blocks User Guide. [34] Jingfei Jiang, Tao Xiao, Jinwei Xu, Dong Wen, Lei Gao, and Yong Dou. 2022. A low-latency LSTM accelerator using"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84674835205078,
                            "x2": 309.4521179199219,
                            "y1": 155.24270629882812,
                            "y2": 159.1241455078125
                        },
                        "text": "balanced sparsity based on FPGA. Microprocess. Microsyst. 89 (2022), 104417."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.2460021972656,
                            "y1": 165.20535278320312,
                            "y2": 169.0867919921875
                        },
                        "text": "[35] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.0893859863281,
                            "y1": 175.16799926757812,
                            "y2": 198.9754638671875
                        },
                        "text": "Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture. 1\u201312. [36] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA: An optimization framework for mapping multiple DNNs on"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.0765686035156,
                            "y1": 205.056640625,
                            "y2": 228.86334228515625
                        },
                        "text": "multiple accelerator cores. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201922). IEEE. [37] Kasem Khalil, Bappaditya Dey, Ashok Kumar, and Magdy Bayoumi. 2021. A reversible-logic based architecture for"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.4707336425781,
                            "y1": 234.94451904296875,
                            "y2": 258.751220703125
                        },
                        "text": "long short-term memory (LSTM) network. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201921). IEEE, 1\u20135. [38] Jinwon Kim, Jiho Kim, and Tae-Hwan Kim. 2021. AERO: A 1.28 MOP/s/LUT reconfigurable inference processor for"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84674835205078,
                            "x2": 339.1789855957031,
                            "y1": 264.8323974609375,
                            "y2": 268.7138366699219
                        },
                        "text": "recurrent neural networks in a resource-limited FPGA. Electronics 10, 11 (2021), 1249."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.620849609375,
                            "y1": 274.7950439453125,
                            "y2": 278.6764831542969
                        },
                        "text": "[39] Dongup Kwon, Suyeon Hur, Hamin Jang, Eriko Nurvitadhi, and Jangwoo Kim. 2020. Scalable multi-FPGA accelera-"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.6615905761719,
                            "y1": 284.7576599121094,
                            "y2": 308.5555725097656
                        },
                        "text": "tion for large RNNs with full parallelism levels. In Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC\u201920). IEEE, 1\u20136. [40] Hyoukjun Kwon, Liangzhen Lai, Michael Pellauer, Tushar Krishna, Yu-Hsin Chen, and Vikas Chandra. 2021. Het-"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77320098876953,
                            "x2": 440.3088684082031,
                            "y1": 314.6367492675781,
                            "y2": 338.4434509277344
                        },
                        "text": "erogeneous dataflow accelerators for multi-DNN workloads. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 71\u201383. [41] Martin Langhammer, Bogdan Pasca, Gregg Baeckler, and Sergey Gribok. 2019. Extracting INT8 multipliers from INT18"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.50933837890625,
                            "y1": 344.5246276855469,
                            "y2": 358.36871337890625
                        },
                        "text": "multipliers. In Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201919). IEEE. [42] Zhe Li, Caiwen Ding, Siyue Wang, Wujie Wen, Youwei Zhuo, Chang Liu, Qinru Qiu, Wenyao Xu, Xue Lin, Xuehai"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.29473876953125,
                            "y1": 364.44989013671875,
                            "y2": 388.256591796875
                        },
                        "text": "Qian, et al. 2019. E-RNN: Design optimization for efficient recurrent neural networks in FPGAs. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201919). IEEE, 69\u201380. [43] Yidong Liu, Leibo Liu, Fabrizio Lombardi, and Jie Han. 2019. An energy-efficient and noise-tolerant recurrent neural"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84516143798828,
                            "x2": 332.73602294921875,
                            "y1": 394.3377685546875,
                            "y2": 398.2192077636719
                        },
                        "text": "network using stochastic computing. IEEE Trans. VLSI Syst. 27, 9 (2019), 2213\u20132221."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.771610260009766,
                            "x2": 441.165771484375,
                            "y1": 404.3003845214844,
                            "y2": 408.18182373046875
                        },
                        "text": "[44] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi, David Sheffield, Rob Pelt, Martin Langhammer, Jaewoong Sim,"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.771610260009766,
                            "x2": 441.1539306640625,
                            "y1": 414.26300048828125,
                            "y2": 438.0697021484375
                        },
                        "text": "Aravind Dasu, and Derek Chiou. 2021. Specializing FGPU for persistent deep learning. ACM Trans. Reconfig. Technol. Syst. 14, 2 (2021), 1\u201323. [45] Andrew Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.30023193359375,
                            "y1": 444.15087890625,
                            "y2": 467.95758056640625
                        },
                        "text": "word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 142\u2013150. [46] Cameron McNairy and Rohit Bhatia. 2005. Montecito: A dual-core, dual-thread itanium processor. IEEE Micro 25, 2"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.60206985473633,
                            "x2": 106.86561584472656,
                            "y1": 474.03875732421875,
                            "y2": 477.9201965332031
                        },
                        "text": "(2020), 10\u201320."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.5141296386719,
                            "y1": 484.0013732910156,
                            "y2": 487.8828125
                        },
                        "text": "[47] Sparsh Mittal and Sumanth Umesh. 2021. A survey on hardware accelerators and optimization techniques for RNNs."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84516143798828,
                            "x2": 165.61317443847656,
                            "y1": 493.9640197753906,
                            "y2": 497.845458984375
                        },
                        "text": "J. Syst. Arch. 112 (2021), 101839."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.2754821777344,
                            "y1": 503.9266357421875,
                            "y2": 507.8080749511719
                        },
                        "text": "[48] Guocai Nan, Chenghua Wang, Weiqiang Liu, and Fabrizio Lombardi. 2020. DC-LSTM: Deep compressed LSTM with"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.1413269042969,
                            "y1": 513.8892822265625,
                            "y2": 537.6959228515625
                        },
                        "text": "low bit-width and structured matrices. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201920). IEEE, 1\u20135. [49] Eriko Nurvitadhi, Andrew Boutros, Prerna Budhkar, Ali Jafari, Dongup Kwon, David Sheffield, Abirami Prabhakaran,"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.2964172363281,
                            "y1": 543.7683715820312,
                            "y2": 577.5376586914062
                        },
                        "text": "Karthik Gururaj, Pranavi Appana, and Mishali Naik. 2019. Scalable low-latency persistent neural machine translation on CPU server with multiple FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 307\u2013310. [50] Eriko Nurvitadhi, James C. Hoe, Shih-Lien L. Lu, and Timothy Kam. 2010. Automatic multithreaded pipeline synthesis"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.84516143798828,
                            "x2": 419.836181640625,
                            "y1": 583.618896484375,
                            "y2": 587.5003051757812
                        },
                        "text": "from transactional datapath specifications. In Proceedings of the Design Automation Conference. IEEE, 314\u2013319."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.14605712890625,
                            "y1": 593.58154296875,
                            "y2": 597.4629516601562
                        },
                        "text": "[51] Eriko Nurvitadhi, Dongup Kwon, Ali Jafari, Andrew Boutros, Jaewoong Sim, Phillip Tomson, Huseyin Sumbul,"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.771610260009766,
                            "x2": 441.6607971191406,
                            "y1": 603.5449829101562,
                            "y2": 637.3150634765625
                        },
                        "text": "Gregory Chen, Phil Knag, Raghavan Kumar, et al. 2019. Why compete when you can work together: Fpga-asic integration for persistent rnns. In Proceedings of the IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201919). IEEE, 199\u2013207. [52] Eriko Nurvitadhi, Jaewoong Sim, David Sheffield, Asit Mishra, Srivatsan Krishnan, and Debbie Marr. 2016. Accelerat-"
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 62.8443603515625,
                            "x2": 440.2939147949219,
                            "y1": 643.396240234375,
                            "y2": 657.2579956054688
                        },
                        "text": "ing recurrent neural networks in analytics servers: Comparison of FPGA, CPU, GPU, and ASIC. In Proceedings of the 26th International Conference on Field Programmable Logic and Applications (FPL\u201916). IEEE, 1\u20134."
                    },
                    {
                        "page": 23,
                        "region": {
                            "x1": 45.495418548583984,
                            "x2": 440.0503234863281,
                            "y1": 671.3132934570312,
                            "y2": 675.1947021484375
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 440.4609375,
                            "y1": 85.50357055664062,
                            "y2": 89.385009765625
                        },
                        "text": "[53] Young H. Oh, Seonghak Kim, Yunho Jin, Sam Son, Jonghyun Bae, Jongsung Lee, Yeonhong Park, Dong Uk Kim, Tae Jun"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.6915588378906,
                            "y1": 95.46615600585938,
                            "y2": 129.236328125
                        },
                        "text": "Ham, and Jae W. Lee. 2021. Layerweaver: Maximizing resource utilization of neural processing units via layer-wise scheduling. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 584\u2013597. [54] Naebeom Park, Yulhwa Kim, Daehyun Ahn, Taesu Kim, and Jae-Joon Kim. 2020. Time-step interleaved weight reuse"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.8420715332031,
                            "y1": 135.31747436523438,
                            "y2": 159.1241455078125
                        },
                        "text": "for LSTM neural network computing. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 13\u201318. [55] Lu Peng, Wentao Shi, Jian Zhang, and Samuel Irving. 2019. Exploiting model-level parallelism in recurrent neural net-"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.2491760253906,
                            "y1": 165.20535278320312,
                            "y2": 189.0128173828125
                        },
                        "text": "work accelerators. In Proceedings of the IEEE 13th International Symposium on Embedded Multicore/Many-core Systemson-Chip (MCSoC\u201919). IEEE, 241\u2013248. [56] Raghu Prabhakar, Yaqi Zhang, David Koeplinger, Matt Feldman, Tian Zhao, Stefan Hadjis, Ardavan Pedram, Christos"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 440.47210693359375,
                            "y1": 195.09402465820312,
                            "y2": 218.90072631835938
                        },
                        "text": "Kozyrakis, and Kunle Olukotun. 2017. Plasticine: A reconfigurable architecture for parallel patterns. In Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA\u201917). IEEE, 389\u2013402. [57] Zhiqiang Que et al. 2020. Optimizing reconfigurable recurrent neural networks. In Proceedings of the IEEE 28th Annual"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 63.01774978637695,
                            "x2": 371.1768493652344,
                            "y1": 224.98190307617188,
                            "y2": 228.86334228515625
                        },
                        "text": "International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201920). IEEE."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 440.4528503417969,
                            "y1": 234.94451904296875,
                            "y2": 238.82595825195312
                        },
                        "text": "[58] Zhiqiang Que, Yanyang Liu, Ce Guo, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Real-time anomaly detection"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 440.4657897949219,
                            "y1": 244.90716552734375,
                            "y2": 268.7138366699219
                        },
                        "text": "for flight testing using AutoEncoder and LSTM. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 379\u2013382. [59] Zhiqiang Que, Hiroki Nakahara, Hongxiang Fan, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.3193054199219,
                            "y1": 274.7950439453125,
                            "y2": 298.59295654296875
                        },
                        "text": "Wayne Luk. 2020. A reconfigurable multithreaded accelerator for recurrent neural networks. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 20\u201328. [60] Zhiqiang Que, Hiroki Nakahara, Eriko Nurvitadhi, Andrew Boutros, Hongxiang Fan, Chenglong Zeng, Jiuxi Meng,"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94419860839844,
                            "x2": 441.79364013671875,
                            "y1": 304.67413330078125,
                            "y2": 328.4808349609375
                        },
                        "text": "Kuen Hung Tsoi, Xinyu Niu, and Wayne Luk. 2022. Recurrent neural networks with column-wise matrix-vector multiplication on FPGAs. IEEE Trans. VLSI Syst. (2022). [61] Zhiqiang Que, Thomas Nugent, Shuanglong Liu, Li Tian, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Efficient"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.3401184082031,
                            "y1": 334.56201171875,
                            "y2": 358.36871337890625
                        },
                        "text": "weight reuse for large LSTMs. In Proceedings of the IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201919), Vol. 2160. IEEE, 17\u201324. [62] Zhiqiang Que, Erwei Wang, Umar Marikar, Eric Moreno, Jennifer Ngadiuba, Hamza Javed, Bart\u0142omiej Borzyszkowski,"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.81732177734375,
                            "y1": 364.44989013671875,
                            "y2": 398.2192077636719
                        },
                        "text": "Thea Aarrestad, Vladimir Loncar, Sioni Summers, Maurizio Pierini, Peter Y. Cheung, and Wayne Luk. 2021. Accelerating recurrent neural networks for gravitational wave experiments. In Proceedings of the 32th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201921). IEEE. [63] Zhiqiang Que, Yongxin Zhu, Hongxiang Fan, Jiuxi Meng, Xinyu Niu, and Wayne Luk. 2020. Mapping large LSTMs to"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 63.018550872802734,
                            "x2": 285.3540954589844,
                            "y1": 404.3003845214844,
                            "y2": 408.18182373046875
                        },
                        "text": "FPGAs with weight reuse. J. Sign. Process. Syst. 92, 9 (2020), 965\u2013979."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 440.47296142578125,
                            "y1": 414.26300048828125,
                            "y2": 418.1444396972656
                        },
                        "text": "[64] Stefano Ribes, Pedro Trancoso, Ioannis Sourdis, and Christos-Savvas Bouganis. 2020. Mapping multiple LSTM models"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 63.018550872802734,
                            "x2": 429.6423034667969,
                            "y1": 424.22564697265625,
                            "y2": 428.1070861816406
                        },
                        "text": "on FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 1\u20139."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.76800537109375,
                            "y1": 434.1882629394531,
                            "y2": 438.0697021484375
                        },
                        "text": "[65] Michalis Rizakis, Stylianos I. Venieris, Alexandros Kouris, and Christos-Savvas Bouganis. 2018. Approximate FPGA-"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.2380676269531,
                            "y1": 444.15087890625,
                            "y2": 467.95758056640625
                        },
                        "text": "based LSTMs under computation time constraints. In Proceedings of the International Symposium on Applied Reconfigurable Computing. Springer, 3\u201315. [66] Vladimir Rybalkin, Alessandro Pappalardo, Muhammad Mohsin Ghaffar, Giulio Gambardella, Norbert Wehn, and"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.8220520019531,
                            "y1": 474.03875732421875,
                            "y2": 507.8080749511719
                        },
                        "text": "Michaela Blott. 2018. FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE. [67] Vladimir Rybalkin, Chirag Sudarshan, Christian Weis, Jan Lappas, Norbert Wehn, and Li Cheng. 2020. Efficient hard-"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 62.72126007080078,
                            "x2": 380.0071105957031,
                            "y1": 513.8892822265625,
                            "y2": 517.7706909179688
                        },
                        "text": "ware architectures for 1D-and MD-LSTM networks. J. Sign. Process. Syst. 92, 11 (2020), 1219\u20131245."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.8029479980469,
                            "y1": 523.8519287109375,
                            "y2": 527.7333374023438
                        },
                        "text": "[68] Vladimir Rybalkin and Norbert Wehn. 2020. When massive GPU parallelism ain\u2019t enough: A novel hardware architec-"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 441.8118591308594,
                            "y1": 533.8145141601562,
                            "y2": 557.6124267578125
                        },
                        "text": "ture of 2D-LSTM neural network. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. [69] Vladimir Rybalkin, Norbert Wehn, Mohammad Reza Yousefi, and Didier Stricker. 2017. Hardware architecture of bidi-"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94580078125,
                            "x2": 440.4704284667969,
                            "y1": 563.6936645507812,
                            "y2": 587.5003051757812
                        },
                        "text": "rectional long short-term memory neural network for optical character recognition. In Proceedings of the Conference on Design, Automation & Test in Europe. 1394\u20131399. [70] Runbin Shi, Junjie Liu, K.-H. Hayden So, Shuo Wang, and Yun Liang. 2019. E-LSTM: Efficient inference of sparse LSTM"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94499969482422,
                            "x2": 441.7003173828125,
                            "y1": 593.58154296875,
                            "y2": 617.3889770507812
                        },
                        "text": "on embedded heterogeneous system. In Proceedings of the 56th ACM/IEEE Design Automation Conference (DAC\u201919). IEEE, 1\u20136. [71] Gil Shomron, Tal Horowitz, and Uri Weiser. 2019. SMT-SA: Simultaneous multithreading in systolic arrays. IEEE"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 63.01774978637695,
                            "x2": 191.43832397460938,
                            "y1": 623.47021484375,
                            "y2": 627.3516235351562
                        },
                        "text": "Comput. Arch. Lett. 18, 2 (2019), 99\u2013102."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.94419860839844,
                            "x2": 440.46728515625,
                            "y1": 633.4328002929688,
                            "y2": 637.314208984375
                        },
                        "text": "[72] Franyell Silfa, Jose Maria Arnau, and Antonio Gonzalez. 2020. E-BATCH: Energy-efficient and high-throughput RNN"
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 63.01694869995117,
                            "x2": 364.66937255859375,
                            "y1": 643.3954467773438,
                            "y2": 647.27685546875
                        },
                        "text": "batching. ACM Transactions on Architecture and Code Optimization (TACO) 19, 1 (2020), 1\u201323."
                    },
                    {
                        "page": 24,
                        "region": {
                            "x1": 45.89876937866211,
                            "x2": 440.452880859375,
                            "y1": 671.2955932617188,
                            "y2": 675.177001953125
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 440.3161315917969,
                            "y1": 85.50357055664062,
                            "y2": 99.34759521484375
                        },
                        "text": "[73] Burton J. Smith. 1986. A pipelined, shared resource MIMD computer. In Advanced Computer Architecture. 39\u201341. [74] Yuxi Sun, Akram Ben Ahmed, and Hideharu Amano. 2019. Acceleration of deep recurrent neural networks with an"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 441.490234375,
                            "y1": 105.42880249023438,
                            "y2": 129.236328125
                        },
                        "text": "FPGA cluster. In Proceedings of the 10th International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies. 1\u20134. [75] Zhanrui Sun, Yongxin Zhu, Yu Zheng, Hao Wu, Zihao Cao, Peng Xiong, Junjie Hou, Tian Huang, and Zhiqiang Que."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77320098876953,
                            "x2": 440.5663757324219,
                            "y1": 135.31747436523438,
                            "y2": 159.1241455078125
                        },
                        "text": "2018. FPGA acceleration of LSTM based on data for test flight. In Proceedings of the IEEE International Conference on Smart Cloud (SmartCloud\u201918). IEEE, 1\u20136. [76] Tian Tan, Eriko Nurvitadhi, David Shih, and Derek Chiou. 2018. Evaluating the highly-pipelined intel stratix 10 FPGA"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77320098876953,
                            "x2": 441.090087890625,
                            "y1": 165.20535278320312,
                            "y2": 189.0128173828125
                        },
                        "text": "architecture using open-source benchmarks. In Proceedings of the International Conference on Field-Programmable Technology (FPT\u201918). IEEE, 206\u2013213. [77] James E. Thornton. 1964. Parallel operation in the control data 6600. In Proceedings of the Fall Joint Computer Confer-"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.845951080322266,
                            "x2": 240.35919189453125,
                            "y1": 195.09402465820312,
                            "y2": 198.9754638671875
                        },
                        "text": "ence, Part II: Very High Speed Computer Systems. 33\u201340."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.2820739746094,
                            "y1": 205.056640625,
                            "y2": 208.93807983398438
                        },
                        "text": "[78] Stylianos I. Venieris and Christos-Savvas Bouganis. 2018. f-CNNx: A toolflow for mapping multiple convolutional"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.067138671875,
                            "y1": 215.019287109375,
                            "y2": 238.82595825195312
                        },
                        "text": "neural networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 381\u20133817. [79] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84516143798828,
                            "x2": 398.6622009277344,
                            "y1": 244.90716552734375,
                            "y2": 248.78860473632812
                        },
                        "text": "generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3156\u20133164."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.2763366699219,
                            "y1": 254.86978149414062,
                            "y2": 258.751220703125
                        },
                        "text": "[80] Shuo Wang, Zhe Li, Caiwen Ding, Bo Yuan, Qinru Qiu, Yanzhi Wang, and Yun Liang. 2018. C-LSTM: Enabling efficient"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 440.28192138671875,
                            "y1": 264.8323974609375,
                            "y2": 288.63909912109375
                        },
                        "text": "LSTM using structured compression techniques on FPGAs. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 11\u201320. [81] Zhisheng Wang, Jun Lin, and Zhongfeng Wang. 2017. Accelerating recurrent neural networks: A memory-efficient"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84516143798828,
                            "x2": 248.16346740722656,
                            "y1": 294.7115173339844,
                            "y2": 298.59295654296875
                        },
                        "text": "approach. IEEE Trans. VLSI Syst. 25, 10 (2017), 2763\u20132775."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77241134643555,
                            "x2": 441.6407470703125,
                            "y1": 304.67413330078125,
                            "y2": 308.5555725097656
                        },
                        "text": "[82] Zhao Wang, Guangyu Sun, Jingchen Zhu, Zhe Zhou, Yijiang Guo, and Zhihang Yuan. 2021. METRO: A software-"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.771610260009766,
                            "x2": 441.2280578613281,
                            "y1": 314.6367492675781,
                            "y2": 338.4434509277344
                        },
                        "text": "hardware co-design of interconnections for spatial DNN accelerators. arXiv:2108.10570 [cs.AR]. Retrieved from https: //arxiv.org/abs/2108.10570. [83] Jiaquan Wu, Feiteng Li, Zhijian Chen, and Xiaoyan Xiang. 2019. A 3.89-GOPS/mW scalable recurrent neural network"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.8443603515625,
                            "x2": 429.2515563964844,
                            "y1": 344.5246276855469,
                            "y2": 348.40606689453125
                        },
                        "text": "processor with improved efficiency on memory and computation. IEEE Trans. VLSI Syst. 27, 12 (2019), 2939\u20132943."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.770809173583984,
                            "x2": 440.8192138671875,
                            "y1": 354.4872741699219,
                            "y2": 358.36871337890625
                        },
                        "text": "[84] Xilinx. 2017. Deep Learning with INT8 Optimization on Xilinx Devices. Retrieved from https://www.xilinx.com/"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84355926513672,
                            "x2": 285.71783447265625,
                            "y1": 364.44989013671875,
                            "y2": 368.3313293457031
                        },
                        "text": "support/documentation/whitepapers/wp486-deep-learning-int8.pdf."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.770809173583984,
                            "x2": 440.2763671875,
                            "y1": 374.4125061035156,
                            "y2": 378.2939453125
                        },
                        "text": "[85] Krishna Praveen Yalamarthy et al. 2019. Low-complexity distributed-arithmetic-based pipelined architecture for an"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84355926513672,
                            "x2": 255.7310791015625,
                            "y1": 384.3751525878906,
                            "y2": 388.256591796875
                        },
                        "text": "LSTM network. IEEE Trans. VLSI Syst. 28, 2 (2019), 329\u2013338."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.770809173583984,
                            "x2": 441.6368713378906,
                            "y1": 394.3377685546875,
                            "y2": 398.2192077636719
                        },
                        "text": "[86] Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria Arnau, and Antonio Gonz\u00e1lez. 2019. LSTM-"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.770809173583984,
                            "x2": 440.297119140625,
                            "y1": 404.3003845214844,
                            "y2": 428.1070861816406
                        },
                        "text": "sharp: An adaptable, energy-efficient hardware accelerator for long short-term memory. arXiv:1911.01258. Retrieved from https://arxiv.org/abs/1911.01258. [87] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.770809173583984,
                            "x2": 441.5107727050781,
                            "y1": 434.1882629394531,
                            "y2": 457.9949645996094
                        },
                        "text": "Toderici. 2015. Beyond short snippets: Deep networks for video classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4694\u20134702. [88] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv:1409.2329."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84355926513672,
                            "x2": 215.01348876953125,
                            "y1": 464.0761413574219,
                            "y2": 467.95758056640625
                        },
                        "text": "Retrieved from https://arxiv.org/abs/1409.2329."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77001190185547,
                            "x2": 441.62158203125,
                            "y1": 474.03875732421875,
                            "y2": 477.9201965332031
                        },
                        "text": "[89] Tian Zhao, Yaqi Zhang, and Kunle Olukotun. 2019. Serving recurrent neural networks efficiently with a spatial accel-"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.842769622802734,
                            "x2": 220.1900177001953,
                            "y1": 484.0013732910156,
                            "y2": 487.8828125
                        },
                        "text": "erator. Proc. Mach. Learn. Syst. 1 (2019), 166\u2013177."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.76921844482422,
                            "x2": 441.6160583496094,
                            "y1": 493.9640197753906,
                            "y2": 497.845458984375
                        },
                        "text": "[90] Yong Zheng, Haigang Yang, Yiping Jia, and Zhihong Huang. 2021. PermLSTM: A high energy-efficiency LSTM accel-"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 62.84196853637695,
                            "x2": 216.26156616210938,
                            "y1": 503.9266357421875,
                            "y2": 507.8080749511719
                        },
                        "text": "erator architecture. Electronics 10, 8 (2021), 882."
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.77399826049805,
                            "x2": 320.1117858886719,
                            "y1": 522.7783813476562,
                            "y2": 527.14501953125
                        },
                        "text": "Received 2 September 2021; revised 18 February 2022; accepted 1 May 2022"
                    },
                    {
                        "page": 25,
                        "region": {
                            "x1": 45.494998931884766,
                            "x2": 440.0498962402344,
                            "y1": 671.3134765625,
                            "y2": 675.1948852539062
                        },
                        "text": "ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022."
                    }
                ],
                "title": {
                    "page": 21,
                    "region": {
                        "x1": 45.77444839477539,
                        "x2": 108.54777526855469,
                        "y1": 453.93890380859375,
                        "y2": 458.6014099121094
                    },
                    "text": "REFERENCES"
                }
            }
        ]
    },
    "structured_data": {
        "abstract": "",
        "introduction": "Recurrent Neural Network (RNN) has been the key component of artificial intelligence (AI) applications where the inputs are sequential, such as natural language processing [25], speech recognition [2, 28], and video analysis [17, 87]. Long Short-Term Memory (LSTM) is the most popular type of RNNs. Since low latency is key for a seamless user experience in such applications, efficient and real-time acceleration of RNNs/LSTMs is required. FPGAs have been used to speed up the inference of LSTMs [19, 27, 51, 52, 75], showing the benefits of low latency and low power consumption compared to CPUs or GPUs. However, existing RNN/LSTM accelerators cannot support cost-effective multi-RNN execution. Cloud providers must minimize their huge operation costs by running as many applications on a given server as possible, while satisfying the quality of each service. In Google data centers, Convolutional Neural Networks (CNNs) and Multi-Layer Perceptions (MLP) comprise 5% and 61% of the workload, respectively, while LSTMs makes up 29% [35]. However, most of the existing LSTM accelerators are only able to perform one inference at a time [19, 27, 28, 47, 51, 57]. These accelerators can process multi-LSTM tasks by executing one by one in sequence, resulting in inefficiency when multiple requests come at the same time as shown in Figure 1. It may make later tasks wait for a long time before a hardware core is available, since earlier LSTM tasks may have a large number of timesteps involving many iterations, e.g., an LSTM layer in DeepSpeech [29] has 1,500 timesteps. Besides, some applications employ not one but multiple LSTMs to collaboratively achieve satisfactory performance. A spacecraft anomalies detection system [31] even contains over hundreds of LSTM models, each modeling a single telemetry channel and predicting values for that channel, which demonstrates the necessity of supporting multi-LSTM execution. Furthermore, conventional LSTM accelerators are often implemented by deploying all computing resources to support a single computational engine on a large scale, leveraging data-level parallelism. For instance, Brainwave [19] devised by Microsoft is a single-threaded neural processing unit (NPU) that involves 96,000 processing elements (PEs). However, when the workload of a targeted LSTM task is small, these hardware resources will not be fully utilized, e.g., the hardware utilization is lower than 1% [19] for Brainwave and lower than 15% for the Brainwave-like NPU [51] when running an LSTM model (ht =256). It is challenging to design an accelerator to support cost-effective multi-LSTM execution. This article introduces a reconfigurable multi-threaded multi-core NPU for accelerating RNN/ LSTM inferences by increasing the hardware utilization for better performance. It improves the processing abilities of cloud-based NPUs as well as the Quality of Service. Our primary goal is to efficiently enhance the scalability potential of NPU cores. The most area- /cost-efficient way to add logical cores is multithreading. Essentially, multithreading retrieves unused performance (where computational cores are idle because of events) by switching to another thread. Multithreading also does not affect peak performance when working in a single-threaded mode. Usually, the execution of multiple neural networks has the potential to mitigate the idle issues, because layers from different neural networks can be scheduled freely without any issue of dependencies. Running multiple tasks can also be realized by batch techniques that provide multiple requests to a neural network to produce multiple results together. However, the batch techniques can harm latency, ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. because different requests may not arrive at the same time [22], which means that a newly arrived request must wait until the batch is formed, which brings a significant latency penalty as shown in Figure 2. Remarn is inspired by coarse-grained multithreading utilized in modern CPUs such as IBM RS64-IV [6] and Intel Montecito [46], which makes a core switch to a different hardware context when a thread is stalled due to some events. Remarn consists of a custom coarse-grained multi-threaded (CGMT) LSTM hardware architecture that switches tasks among threads when LSTM computational engines meet data hazard. When one logical NPU core is stalled, the other can make progress. Coarse-grained multithreading is a mature technique in modern CPU designs. However, few studies concern combining the CGMT and NPUs, especially for RNNs/LSTMs. There is also fine-grained multi-threading [73, 77] that switches the context every cycle, but it brings more hardware complexity than CGMT. Unlike CNNs that do not have memory cells and can run different layers from different neural networks iteratively, RNNs/LSTMs contains many memory cells, which makes it difficult to process different timesteps from different RNN layers or models, since they have different cell memory statuses when using a single-threaded NPU. It has to finish the running of the preceding RNN inference or layer until the next inference or layer can start. The existence of inter-timestep dependencies within an RNN model prevents the following timesteps from even starting their execution until the current timestep\u2019s completion, leading to hardware underutilization and inefficiency. To address this challenge, this work proposes the CGMT-LSTM, which can intelligently switch to an alternate thread of computation when the data hazard happens, e.g., the inter-timestep dependencies of RNNs, to increase hardware utilization and boost design performance. Besides, instead of deploying all computing resources to form a single physical core on a large scale like Brainwave [19] and our previous design [59], we design an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, and each core can accept new RNN requests. The multiple LSTM models used in the application of spacecraft anomaly detection system [31] have only a small hidden vector size that is less than 128, but they have 250 timesteps that are large. LSTM models that have a large number of timesteps but utilize a small size are the most tangible examples, since they require dealing with lots of dependencies, as well as the parallel task of matrix-vector multiplications (MVMs) [86]. The Brainwave [19] involves big MVM \u201ctile engines\u201d that can effectively process a 400\u00d7240 matrix in parallel. Besides, our previous NPU [59] is based on a tile size of 1, 024 \u00d7 16, resulting in 16,384 effective PEs. Any MVM that does not map to this dimension will leave some resources idle and small MVMs even ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. require zero paddings, resulting in low utilization. Splitting a large accelerator into several small sub-accelerator cores can not only help to find a better mapping to improve hardware utilization but also provide more physical cores to spatially co-locate multiple RNN inferences on the same hardware. As Reference [19] mentions, RNN programs have a critical loop-carry dependence on the ht vector. If the full pipeline cannot return ht to the vector register file in time for the next timestep computation, then the MVM unit will stall until ht is available. Thus, even if a large MVM engine finishes the LSTM gates operations in a short period, e.g., one cycle, then the design still needs to wait for the ht returned from the pipeline, which shows the limitation of the architecture using a large engine. This work splits the accelerator with a single large engine into several smaller sub-accelerator cores, each of them processing small LSTM models more efficiently by adopting the tiling-based columnwise MVM approach [57]. It addresses the challenge of accelerating a large number of small LSTM models with large timesteps. Please note that these sub-accelerator cores can work together as a single large accelerator when necessary to deal with the high priority workloads that require the lowest end to end latency. To the best of our knowledge, Remarn is the first coarse-grained multi-threaded and multi-core LSTM accelerator architecture capable of achieving high performance and efficient multi-LSTM execution. Our contributions are the following: \u2022 A novel reconfigurable multi-threaded multi-core neural processing unit to enable effective and efficient multi-neural network execution for LSTM inferences. \u2022 A custom CGMT LSTM accelerator architecture that also can be partitioned into several full- fledged sub-accelerator cores, which significantly improves hardware utilization and design performance. \u2022 A custom tiling method for LSTMs, which minimizes the intermediate results buffers when combining CGMT and partition, thereby increasing the accelerator area efficiency. \u2022 A comprehensive evaluation on the proposed methods and hardware architecture. Relationship to Prior Publications: This article expands on a conference paper [59] with the baseline design proposed in Reference [57]. The baseline design [57] involves a novel latency-hiding hardware architecture based on columnwise matrix-vector multiplication. It has much higher performance and hardware utilization than other designs for RNN models with different sizes, but it still suffers from underutilization when the model size is small. Reference [59] addresses the underutilization issue by introducing CGMT that enables temporal multi-neural network execution to improve the performance when the RNN models are small while still maintaining the ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. performance of RNN models with medium to large sizes. It mainly time-multiplexes a large RNN accelerator across various RNNs workloads. This article addresses a limitation of the work described in our previous papers, that they do not allow the large computation engine to be partitioned into several smaller ones, so they do not support spatial co-execution of multiple RNN inferences. This limitation brings a severe hardware underutilization issue when targeting acceleration for a large number of small RNN models that are commonly used in many applications [27, 31, 62, 88]. This work introduces an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, combining multithreading with multi-core techniques to enable temporal and spatial multi-neural network execution on cloud-based NPUs. The proposed novel dimension of optimization allows us to obtain significant improvement in throughput while reducing latency over the previous design [59]. This article adds the design of multiple sub-accelerator cores for spatial co-execution of RNNs in Section 3.3. Sections 4.1\u20134.3 describe a revised hardware architecture, and Sections 5.3\u20135.6 contain new evaluation results.",
        "background and preliminaries": "RNNs/LSTMs have been shown to have useful properties with many significant applications. Among the many RNN variants, the most popular one is the LSTM that was initially proposed in 1997 [30]. This study follows the standard LSTM cell [17, 19, 27, 51]. Figure 3 contains a diagram of an LSTM cell. It utilizes the following equations to compute the gates and produce the results for the next time step. it = \u03c3 (Wi [xt ,ht\u22121] + bi ), ft = \u03c3 (Wf [xt ,ht\u22121] + bf ) \u0434t = tanh(W\u0434[xt ,ht\u22121] + bu ), ot = \u03c3 (Wo[xt ,ht\u22121] + bo ) (1) ct = ft ct\u22121 + it \u0434t , ht = ot tanh(ct ). Here, \u03c3 and tanh represent the sigmoid function and hyperbolic tangent function. Both are activation functions; it , ft ,\u0434t , and ot denote the output of the input gate (i-gate), forget gate (f - gate), input modulation gate (\u0434-gate), and output gate (o-gate) at timestep t , respectively. The \u0434gate is often considered as a sub-part of the i-gate. Each LSTM gate consists of a MVM unit and a corresponding activation function unit, as shown in Figure 3. The operator denotes an elementwise multiplication.W is the weight matrix for both input and hidden units, since the input vector ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. and hidden vector are combined in the equations. The b terms denote the bias vectors. ct is the internal memory cell status at timestep t while ht is the hidden vector that is the output of the cell and passed to the next timestep calculation or next LSTM layer. The LSTM information flow is controlled by these four gates with details shown in Figure 3. The i-gate decides what new information is to be written into the internal memory cell; the \u0434-gate modulates the information processed by i-gate via adding non-linearity. Note that only \u0434-gate utilizes hyperbolic tangent as its activation function while all the other three gates utilize sigmoid. The f -gate decides what old information is no longer needed and can be discarded so there are element-wise multiplications between the output of f -gate and memory cell status in the previous timestep ct\u22121. Its output will be added to the products of the outputs from i-gate and \u0434-gate to form the current status of the internal memory cell. The o-gate decides what the value of the current hidden vector (ht ) should be by multiplying the current status of the memory cell after the hyperbolic tangent function, as shown in the LSTM-Tail in Figure 3. Our work focuses on the optimization of RNN inferences involving standard LSTMs, but the proposed techniques can be applied to other deep neural networks (DNN) inferences.",
        "design and optimization methodology": "Accelerating RNN/LSTM inferences efficiently is challenging because of their recurrent nature and data dependencies. This section first presents the coarse-grained multithreading for accelerating RNN/LSTM and then introduces a partitioning strategy of LSTM weight matrix to apply sub-layer granularity scheduling with fine-grained tasks for Remarn architecture. Finally, we present the multi-core accelerator to enable spatial co-execution of RNN models or layers to improve the design performance. Some design parameters are defined in Table 1. 3.1 Multithreading for Recurrent Neural Processing Unit There is a large demand for architectural support of multi-DNN execution to maximize the hardware utilization and reduce the costs of running a large-scale production system. However, most of the existing LSTM accelerators can only run one task at a time [19, 27, 28, 47, 51, 57]. This work proposes a CGMT LSTM NPU that switches on the event when the data hazard of a computational unit happens in the LSTM computation. The general idea is when a thread is stalled because of some events, e.g., cache misses, the multi-threaded core can switch to a different hardware context. In the proposed CGMT LSTM accelerator, the event is the hazard caused by the data dependency between the timesteps of sequential calculation in LSTMs. We propose to maintain multiple thread contexts in a recurrent NPU core so that when the first thread stalls, the second one can carry on, as shown in Figure 4. Thus, it utilizes computational resources more efficiently than a single thread core. It can increase the design performance by utilizing thread-level parallelism and enhance the NPU hardware utilization. Besides, a preceding LSTM model or layer may have thousands of sequences (timesteps), which occupies the NPU core for a long time, leading to a long waiting time for the latter requests before they have an available core to run. However, some services have strict latency constraints, since ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. the response time directly relates to user experience, e.g., intelligent personal assistants are one of the examples where real-time DL is utilized to handle user speech and output smart responses. The conventional single-threaded NPUs can only store the old thread\u2019s context to memory and retrieve the new thread\u2019s context using preemption mechanisms [14] to run another task. However, it will have a large context switch penalty. In our multi-threaded NPU for LSTMs, the new task can execute from another thread as soon as it comes, as shown in Figure 4. Please note that a particular thread may still stall due to data hazard, but the physical core is not stalled, since multiple threads share the same computational physical core, which leads to \u201cVirtual idle\u201d as shown in this figure. We believe that further optimizations, e.g., simultaneous multithreading (SMT) can be adopted to our NPU design to gain even higher performance. We leave it for our future work, because it does not have a big impact on the conclusions we draw from this work. 3.2 Weights Matrix Blocking Strategy The LSTM calculation of one timestep in an LSTM layer has four MVM operations according to the Equations (1). Besides, these four MVM are independent and share the same size. Since the four matrices of i, f ,o,u gates in LSTMs have the same size, these matrices can be combined into a single large matrix [1, 57]. Figure 5 illustrates the basic idea. Thus, in the calculation of one timestep of an LSTM layer, we can only focus on the optimizations of one large matrix multiplying one vector for the whole LSTM cell rather than four small matrices multiplied by one vector. This is a general optimization that can be utilized for any MVM operations that share the same input vector. Because each matrix from LSTM gates has a size of Lh \u00d7 (Lx + Lh), the large fused matrix has a size of (4 \u00d7 Lh) \u00d7 (Lx + Lh). Usually, deep neural networks have many compute intensive operations involving large weight matrices, which can be accelerated by running in parallel. However, when deploying on FPGAs, the parallelism is constrained by the limited hardware resources on the targeted FPGAs. It means that the whole MVM computation may not be fully unrolled and performed at once, especially for the large LSTMs. To use the computational resources efficiently, the combined weight matrix of an LSTM layer is partitioned into multiple sub-layers in advance depending on the configuration of the accelerator cores and LSTM layer sizes. Specifically, an LSTM layer of one timestep is partitioned into a number of sub-layers with equal size, as shown in Figure 6(a). For simplicity, the number of the sub-layers in the example is set as 2 to illustrate the idea, but a real design could have more sub-layers, and it depends on the design constraints. Then, Element-based Parallelism ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. (EP) and Vector-based Parallelism (VP) are applied to exploit the available parallelism [57]. The number of the sub-layers is determined by (4\u00d7Lh) V P . The sub-layer is then divided into many small tiles, each having a size of (EP ,VP ), as shown in Figure 6(b). In each cycle, our Remarn core is able to process the MVM of a tile and a sub-vector of [xt ,ht\u22121] with a size of EP . To increase the design parallelism, VP should be chosen to be as large as possible. However, the largest value of VP equals Hw , which is 4 \u00d7 Lh, since there are only four gates in an LSTM. Thus, the smallest number of sub-layers is one when VP is 4 \u00d7 Lh. In this study, sub-layer granularity scheduling is adopted with fine-grain tasks of sub-layers for multi-LSTM execution. Besides, we interleave the rows of the four weight matrices, so that the related elements from the four gates output are adjacent in the result vector. Thus, the LSTM gate outputs can multiply with each other easily using the element-wise operations in the LSTM-tail unit. It also means that there is no data dependency between these sub-layers. The optimization of interleaving also removes the requirement to buffer large intermediate outputs from four LSTM gates, since the result sub-vectors from the sub-layers are not related and will be reduced in the tail unit soon. Each sub-vector of the MVM output can be handled individually in the LSTM-tail units. There is no need to wait for other sub-layer results to get the LSTM memory cell status and hidden vector of the current sub-layer. The proposed CGMT core will always finish processing all the sub-layers in one timestep before it switches to another thread, because the data hazard in the LSTMs happens when timestep changes. Compared with a fine-grained multithreading scheme that switches the context between threads in every cycle, we can avoid buffering these large intermediate MVM operation values as well as element-wise operation values, since these values will finally form the sub-vectors of LSTM memory cells and hidden units. Only the thread dedicated buffers are needed to be added for storing the LSTM memory cells and hidden units in each timestep, since different threads process different LSTM tasks. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. 3.3 Multi-core for Recurrent Neural Processing Unit Many RNN/LSTM accelerators deploy all the computing resources to form a single accelerator core with a large scale, leveraging data-level parallelism. However, this type of accelerator is only able to process one task at a time, resulting in potential underutilization and inefficiency when multiple tasks come at the same time, especially in large data centers. Multithreading described in Section 3.1 can address the issue partially but it still cannot easily deal with a large number of small workloads. Recently, spatial architectures have become a popular solution to build high throughput accelerators for DNNs [24, 78, 82]. Generally, multiple PEs are grouped as an engine, and these engines are connected with an on-chip interconnection to enable data communication during processing. f-CNNx [78] proposes multiple custom dedicated engines for the workloads with various CNN models. Planaria [24] proposes to dynamically fission a systolic array-based DNN accelerator at runtime to spatially co-locate multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations that are absent from accelerators targeting CNNs and fully connected (FC) layers. This work adopts a similar idea to Reference [24]. But instead of using a systolic two-dimensional matrix-matrix multiplication array, our architecture uses matrix-vector multiplication engines, since RNNs and MLPs are dominated by matrix-vector multiplication operations. With multiple sub-accelerator cores, it enables spatial co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations. In addition to inter-model parallelism, which is naturally supported by using multiple subaccelerator cores, such as co-execution of multiple models, our design also enables intra-model parallelism among different layers and timesteps of the RNN layers. Generally, in an RNN model, a latter layer or timestep should wait until its preceding layer or timestep finishes, since some of its inputs are the output of the preceding layer or timestep, as shown in Figure 7(a). For example, the computation that requires c00 and h00 cannot start until they are available from the preceding layer or timestep. Using a single-threaded single-core accelerator, two LSTM workloads will run as Figure 7(c) shows. The accelerator will process the LSTM layers and timesteps iteratively. The temporal inefficiency comes from the stall of data dependencies and the spatial inefficiency comes from mapping a small network to a large core leaving some resources idle. With a multi-threaded accelerator, each layer of the LSTM can run on different threads to remove the stall or the temporal inefficiency that comes from the RNN nature data dependencies. Please note, a special computation order, as shown in Figure 7(b) is required to achieve stall-free processing using a multi-threaded single-core accelerators, as shown in Figure 7(d). However, when LSTMs are small, many resources are left unused when mapping them to a large core. In such cases, a small core will get a similar latency to the one using a large core. Besides, with multiple sub-accelerator cores, the processing of the cascaded LSTM layers can be overlapped in a layerwise coarse-grained pipeline as shown in Figure 7(e). The second layer does not need to wait for the whole sequence of hidden vectors of the first layer to be ready. Just one hidden vector from the preceding LSTM layer is sufficient to start the calculation of the next LSTM layer. It helps to reduce the overall design latency. Besides, since the output of the preceding layer sinks directly in the following sub-accelerator core, there is no need for buffering large intermediate outputs, which could help to improve the area efficiency. We can further optimize the processing after combining the multithreading with multi-core. Figure 7(f) shows two LSTMs are mapping to a dual-thread three-core accelerator, which achieves the best total latency. While the multi-core scheme leads to high performance, it also presents a new challenge: In some situations it cannot provide sufficient bus bandwidth per core. In Remarn, the multiple cores share the same front side bus. But this will not affect on the bus bandwidth, because we do not add ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. new Remarn big cores but only split the original big core into several small ones, e.g., four small cores. When the four small cores co-operate to handle a large network, the quad-core requires the same bandwidth as the one with a big monolithic core. The total bus bandwidth they require is the same as the large core from which they are derived, which is smaller than that for four large cores. For example, the bandwidth requirement of four small LSTM models with Lh = 64 is much lower than the one of one big LSTM model with Lh = 1,024.",
        "hardware architecture": "Based on the optimization techniques introduced above, we implement the proposed Remarn on top of a state-of-the-art (SOTA) RNN accelerator [57] for low-latency cloud-based applications. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. In Section 4.2, we outline the main components of the architecture and detail the necessary hardware modifications required to support our Remarn unit. Several FPGA-specific optimizations are also introduced. 4.1 System Overview A high-level block diagram of Remarn accelerator is shown in Figures 8(a) and 8(b). The original monolithic accelerator with all the MVM kernels in our previous work [59] has been split into N (e.g., N = 4 in the figure) sub-accelerator cores, each working in a big-little engines architecture. All these N cores are connected via the switch for hidden vectors and partial sum data movement. When N is small as in this case, the switch can be implemented by a crossbar. When N is large, then a network-on-chip could be more effective. Hence, in one extreme, all four cores can be attached together to construct a large logical accelerator, running only one RNN inference using the entire accelerator. Alternatively, it can also provide four stand-alone sub-accelerator cores, spatially co-executing 4 different RNNs simultaneously. Combining with the CGMT of four threads, it can support up to 16 different RNN inferences simultaneously. In this work, all the sub-accelerator cores are identical. But the design can be extended easily to employ a different accelerator hardware architecture, e.g., a systolic array-based one, for some of the cores to support the heterogeneous acceleration of multi-DNN workloads [40]. This work utilizes a general switch as the interconnection between the cores, which brings a slight performance drop when fusing as a large accelerator compared to a monolithic one, since the computational engines now need one more hop to share the partial results. We leave that for future work, since it has a limited impact on the conclusions we draw from our study in this article. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. 4.2 The Big-Little Engines Architecture This work proposes a hardware architecture with big-little engines for the DNN accelerators, which is illustrated in Figure 8(a). The proposed accelerator core consists of a big engine, a little engine, an adapter unit, the crossbar as well as weight and vector buffers. The big engine is for the matrix operations that are computation intensive while the little one is for the small but essential components in the neural networks, such as ReLU, pooling, batch normalization, activation, and so on. The big-little engines architecture follows a wait-free single-producer/single-consumer mechanism using the adapter unit, which adjusts the data parallelism between the two engines. In this work, the big engine unit has VP MVM kernels to perform matrix-vector multiplications for LSTM gates operations. It occupies the major computational resources, such as DSPs. Practically, the design will deploy a large number of hardware resources in the big engine to increase the data parallelism to improve the processing ability. However, since the big engine produces one output vector after multiple accumulation cycles, the little engine does not require a large parallelization factor by deploying many LSTM tail units like the one in the big engine unit. The adapter unit can convert the parallelism between the two engines. With a proper adapting factor between the big engine output and little engine input, the little engine can be fully occupied without a stall or wait. The little engine unit has a vector activation function unit and EP tail units that execute the element-wise operations in the LSTM computation. It does not contain many computation resources like the big engine, but it is essential for running the whole neural networks on-chip. Besides, with different hardware components in the little engine, such as ReLU, pooling, batch normalization, and so on, our design can easily be extended to support other types of neural networks, such as CNNs. The Brainwave [19] accelerate and serve CNN models using its MVM engines and custom multi-function units. Sometimes, to accelerate DNN designs, the big engine can be a large systolic array supporting large matrix operations and the little engine utilizes a SIMD vector unit to support general vector operations [24] for the other operations in NNs. The LSTMs are much more challenging to accelerate, exhibiting lower available parallelism and more data dependencies than two-dimensional (2D) CNNs [19]. This design focuses on accelerating RNN/LSTM inferences using a Brainwave-like architecture, but the proposed optimizations and hardware architecture can be easily extended to support other neural networks, which will be our future work. 4.3 Detailed Hardware Components in One Sub-accelerator Core 4.3.1 Overview of the Sub-accelerator Core. Each sub-accelerator core is a full-fledged RNN neural processing unit with coarse-grained multithreading. It has many logical NPUs that share nearly all resources of the physical NPU, e.g., weights buffer, kernel units, activation function units, element-wise units, as shown in Figure 8(a). The core has VP MVM kernels, each having EP PEs, resulting in VP \u00d7 EP available PEs. The VP and EP values are determined via the design space exploration described in Reference [57]. In this article, each PE is one fully pipelined multiplier. The kernels are used to perform the matrix-vector multiplications between the weights and xt as well as ht\u22121 that is required in LSTM gates operations shown in Equations (1). 4.3.2 The MVM Kernel Units. Generally, the row-wise MVM is based on an architecture with parallel multipliers followed by a balanced adder tree. Accumulating the products of these multiplications is usually achieved using a balanced adder tree structure so that a number of related additions can be scheduled in parallel, which minimizes the overall latency of the system. This architecture of the kernel unit is commonplace in FPGA-based designs of RNNs [26, 69]. Since the elements in the partial result vector are not related, we adopt the columnwise MVM [57] that is based on the architecture of parallel multipliers followed by parallel accumulators. Besides, to support element-based parallelism, a small balanced adder tree is placed between the multipliers ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. and the accumulators as shown in Figure 8(c). This adder tree can help to balance EP and VP to improve parallelism. In our proposed multi-threaded NPU core, all the logical cores share all the MVM kernel units. When one logical core (thread) is stalled, the other can still conduct calculations using the same kernel units. Thus, the proposed CGMT NPU core can utilize the resources of MVM kernels efficiently to improve the NPU performance and hardware utilization. Since each thread requires its own input vector xt and ht\u22121, the design has to maintain multiple contexts by using buffers in the thread-aware vector manager and buffers unit. The thread selection logic is necessary to choose the required buffer to retrieve the data and conduct the remaining computations. 4.3.3 The Activation Function Unit. The\u03c3 / tanh unit performs the vector-based sigmoid (\u03c3 ) and hyperbolic tangent (tanh) operations. They are implemented using programmable lookup tables with size of 2,048 similar to References [28, 51]. The implementation using lookup tables brings several benefits. First, our NPU can run a trained model with custom activation functions (e.g., a hard sigmoid from Keras [15]) from our users without re-training of the model. Because the equations of the custom sigmoid/tanh are not changed in the model, re-training is unnecessary. More importantly, we do not touch the sensitive data of users, which is vital for many users. Second, the lookup table has a fixed latency (e.g., one cycle) but other implementations, e.g., a piecewise linear approximation, may involve multiplications that have a much larger pipe stage and latency. 4.3.4 The LSTM Tail Units. Figure 8(d) illustrates the LSTM-tail units that perform the elementwise operations in the Equations (1). The LSTM memory cell FIFOs are employed to temporarily store the status of the running LSTMs, since one thread may be switched due to data hazard before the design finish the calculation of the current LSTM layer with multiple timesteps. Because these threads are performing different LSTM layers or models, the design must keep multiple contexts of cell status of LSTMs, as shown in Figure 8(d). Other hardware contexts, such as the input data and hidden units, are maintained in the thread-aware vector manager and buffer unit with the same mechanism. 4.4 FPGA-Specific Optimizations Since the proposed accelerator core can process a small tile with a size of (EP ,VP ) in each cycle, all the MVM kernel units in one core share the same input of a sub-vector of (xt ,ht\u22121), which has EP elements. These EP elements are broadcasted to all the MVM kernels for the computation. In addition, the design also needs to broadcast a single address to all the weight buffers units to fetch the weights. To alleviate the issue of large fan-out, the tree-shaped [51] inter-connection is adopted to decrease the fan-out of each node with pipeline stages between the source and destination. The RTL code is carefully written to enable the use of HyperFlex registers on Stratix 10 to get high operating frequency [32, 76]. The DSP blocks in modern FPGAs, which are highly configurable, are often underutilized when implementing 8-bit DNN systems. [84] illustrates methods to extract two INT8 multipliers from Xilinx DSP48E2 Blocks that contain a 27\u00d718 multiplier. Reference [41] introduces a method to pack 2 INT8 multiplications into one INT18 multiplier with extra ALMs. Both the methods require two multiplications to share one input operand. In the columnwise architecture [57], the computation order of MVM is different from the one in row-wise MVM. With the columnwise MVM used in RNN designs, one column of the weights matrix naturally shares the same element of the input vector and conducts the multiplications at the same time. Thus, these multiplications share one input operand, which helps us to pack four INT8 multiplications into one DSP block on Intel FPGAs [41] to decrease the hardware resources. In addition, this would not be a restriction (and will come ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. at a lower cost) if we apply a novel DSP similar to the one that was presented in Reference [7] and will be adopted in the next generation Agilex devices [33].",
        "experimental results and analysis": "This section introduces the experimental results on an Intel Stratix 10 FPGA that show the advances of the Remarn for RNN/LSTM accelerations. 5.1 Experimental Setup Table 2 lists some benchmark workloads that are chosen from several typical LSTM applications to make a fair and direct comparison of our design with others. Multi-tasked LSTM workloads are constructed randomly from these LSTM workloads. We evaluate the Remarn on an Intel Stratix 10 2800 (S10) and compare the results with other work. The Remarn runs the inferences of persistent LSTM models that store the weights in on-chip memory [19, 51, 57, 59, 66\u201368]. Remarn is designed using Verilog RTL. And Quartus Prime Pro 19.4 is used for the compilation of FPGA designs. The choice of the number of sub-core is based on the size of the benchmark LSTM workloads and the accelerator hardware architecture. The value of EP , explored in Reference [57], is set to 16, the same size as the previous designs [57, 59]. The value of VP should be chosen to be as large as possible. However, the largest effective value of VP is 4Lh as discussed in Section 3.2. Hence, the VP = 256 in this work, since the Lh = 64 for the smallest LSTM benchmark as shown in Table 2, resulting in 256\u00d716 = 4, 096 effective PEs for each sub-accelerator core. To make a fair comparison, the number of sub-accelerator cores, N , is set to 4 so that Remarn will have 16,386 PEs, which is the same as the number of total PEs in our previous designs [57, 59]. 5.2 Resource Utilization Table 3 shows the resource utilization of our designs with two configurations on FPGAs. The first design is a baseline design that is a single-threaded design without partition. It utilizes the parameter of (EP ,VP ) as (16, 1024), which includes 16,384 8-bit multipliers in the MVM kernels implemented using DSP blocks with extra ALMs. The second design is a dual-threaded quadcore Remarn that has four sub-accelerator cores. Each core utilizes the parameter of (EP ,VP ) as (16, 256). Thus, it has the same number of multipliers as the baseline design. The dual-threaded quad-core design consumes 5.0% more of total logic (ALMs) resources and 0.8% more block ram (M20K) than the baseline design. Since dual threads share the same physical core it consumes the same DSP resources as the single-threaded one. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. 5.3 Performance and Efficiency Comparison with CPUs and GPUs To compare the performance of the proposed design on FPGA with other platforms, the DeepBench published results [89] on an Intel Xeon Syklake CPU and NVIDIA Tesla V100 GPU are used for comparison. TensorFlow is used for both CPU and GPU. Besides, the CPU with AVX2 Vector instructions enabled is utilized while the CuDNN libraries are enabled for the GPU. cuDNN is a GPU-accelerator Library from NVIDIA, which is specialized for deep learning. Both CPU and GPU implementations run with a batch size of 1, which provides the lowest service latency of cloud, since requests are required to be processed as soon as they arrive. For a fair comparison with the throughput of our dual-threaded Remarn, the throughput of the CPU and GPU have been doubled in Table 4. The GPU is significantly underutilized even when cuDNN library API calls, since it is designed for throughput-oriented workloads. It prefers BLAS level-3 (matrix-matrix) operations that are not common in RNN computations [89]. Our FPGA design of dual-threaded Remarn achieves 6.5 times higher performance and 15.6 times higher power efficiency, respectively, than the one running on the Tesla V100 GPU as shown in Table 4. 5.4 Performance and Efficiency Comparison with the Baseline To show the benefits of the proposed approaches, we compare the multi-threaded multi-core Remarn with the baseline design [57] in Figures 9 and 10. The baseline is a single-threaded singlecore accelerator. Hardware utilization is the percentage of the peak Tera Operations Per Second (TOPS) achieved for each LSTM, as compared with the peak TOPS of the design with all the PEs. Figure 9 shows the speedup of the quad-core Remarn over the baseline design [57]. With the proposed approaches of CGMT and multi-core, the quad-core Remarn designs of a single thread (1-T-Q), dual threads (2-T-Q) and quad threads (4-T-Q) achieve 1.82, 2.91, and 3.16 times higher performance respectively than the baseline when targeting mixed requests from LSTM workloads with h = 64, 128, and 256. Please note that Remarn has the same number of PEs as the baseline, and they consume the same DSP blocks on FPGAs. Particularly, with four threads (4-T-Q), Remarn achieves 14.44 times higher performance than the baseline [57] when targeting the small LSTMs (Lh = 64). When only targeting large LSTMs, the performance gain of the quad-core Remarn is small, since the baseline design [57] already achieves high hardware utilization for these LSTMs. However, the baseline still suffers from low utilization when targeting small-sized LSTMs that are commonly used in many applications [17, 55]. LSTM models that have a large number of timesteps but use small sizes are the most tangible examples that require dealing with lots of dependencies, as well as the parallel task of MVMs [86]. Our proposed approach and hardware architecture can alleviate ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. this problem by leveraging the coarse-grained multithreading combining the spatial co-execution using multiple sub-accelerator cores. With the demand for high-performance designs, it is vital to maximize the hardware utilization to attain the best possible effective performance and power efficiency. When compared to our previous work [59] with only multithreading, Remarn can attain much higher hardware utilization and performance. Remarn achieves higher than 50% utilization for all the LSTM cases with Lh = 128 as shown in Figure 10(b) while the previous work can only achieve up to 36.4%. Particularly, with dual threads, the previous work achieves the utilization lower than 25% for LSTMs with Lh = 128; however, the proposed Remarn can achieve higher than 75%. Compared to our previous work [59], the Remarn proposed in this work achieves up to 3.61 times higher performance when targeting the small LSTMs, as shown in Figure 11, which ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. shows the advances of the proposed architecture using multiple sub-accelerator cores. There is a small reduction from a linear speedup for the model size of 64, which is due to control logic added to each small cores when splitting the big engine. The impact is negligible when the workload is large with a long runtime. We believe this can be optimized with more efforts but doing so has little impact on the conclusions we draw from this study. The design with multithreading alone shows a steady increase from 1-T to 2-T and to 4-T [59] for the model size of 128. However, the additional speedups for the quad-core design dropped when it is from 2-T to 4-T threads as shown in Figure 11. It is because with 2-T on small cores, the design can already remove most of the stalls caused by the RNN data dependencies for the model size of 128. Hence, the quad-core design with 2-T and 4-T achieves a similar performance and runtime hardware utilization as shown in Figure 10(b). But with 2-T on the big core, the design still suffers from some stalls so the performance of 4-T on the big core is much better than 2-T. Hence, the additional speedups drop when going from 2-T to 4-T threads on the quad-core design. Moreover, when compared to References [57, 59], which has a single large core, the quad-core Remarn has a slight performance drop (less than 2%) when all 4 cores are working together as a big accelerator to run a large workload, as shown in Tables 4 and 5. It is because the multi-core design can lead to synchronization costs than the one using a monolithic accelerator. When four cores are running the same large workload, a synchronization mechanism is required to maintain the correct data dependencies in RNNs. It is a design tradeoff. With slight performance loss on the large workloads, the design can achieve much higher performance on the small workloads, which can result in a much better performance gain for all the workloads. Besides, the users can re-configure the FPGA into a big and monolithic accelerator when most of large RNN workloads are large, which can achieve the best performance. The evaluation results also show that Remarn gets a low utilization when running small LSTMs (h = 64), as shown in Figure 10(b). Even with four threads, the design still stalls, because the cycles of processing MVMs in the other threads still cannot completely hide the whole pipeline latency to get the ht available before it is required for the next timestep. The number of cycles to process the MVM of the LSTM gates is given by Lx+Lh EP . Different choices of (EP , VP ) impact the performance and utilization of the designs that run LSTMs with different sizes. There is a tradeoff between the extra performance that can be achieved and the hardware design complexity as well as extra hardware resources of supporting various sets of (EP ,VP ) at runtime. It is our future work to enhance this accelerator architecture to support various sets of (EP ,VP ), since different sizes of LSTMs may prefer different optimal EP and VP parameters. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. 5.5 Performance and Efficiency Comparison with Other FPGA-based Designs Table 5 shows the comparison between our Remarn and the existing LSTM accelerator implementations using a Stratix 10 GX 2800 FPGA. For a fair comparison, we only show the previous work with a detailed implementation of the LSTM system using the same FPGA. We list the model storage, precision, DSP used, runtime frequency, power, throughput and power efficiency as well as the hardware (HW) utilization. The thermal design power (TDP) that is 125 W is utilized for a fair comparison, since it is reported in all the listed work. Overall, Remarn provides over 4.87 times higher performance and 5.71 times higher hardware utilization than the SOTA design [51] when targeting LSTM models with ht =256, as shown in Table 5. Compared with the baseline that is a single-threaded single-core design [57], Remarn achieves 1.45 to 14.4 higher throughput and HW utilization. Since multithreading also ensures there is no downside to peak performance in a single-threaded mode, the Remarn can achieve a similar peak performance with the one in Reference [57], which is the highest with respect to the SOTA FPGA-based RNN/LSTM designs with a commonly used INT8 precision. The only prior work that has a higher peak performance is Reference [19] that employs an 8-bit block floating-point representation. However, when targeting the small LSTM models, the throughput of the proposed Remarn is 18.82 times higher than [19] as shown in Table 5. Moreover, Remarn achieves the highest hardware utilization among all the listed designs across various LSTM models. This work focuses on maximizing throughput and minimizing latency by increasing HW utilization. 5.6 Scheduling The scheduling strategy for multi-core accelerators for DNN workloads is a new and rarely explored topic. PREMA [14] introduces a preemptive scheduling algorithm on a single-core NPU to support multi-DNN execution. AI-MultiTasking [4] proposes to process different neural networks by partitioning their various layers into identical sub-layers in compile time. It also develops a heuristic of mapping DNN jobs to the accelerator. In Reference [40], Heterogeneous Dataflow accelerators (HDAs) are proposed to support the heterogeneous acceleration of multiDNN workloads using manual-designed mapping of jobs to sub-accelerators and compile time layerwise scheduling. LayerWeaver [53] introduces a layerwise time-multiplexing scheduler on single NPU by interweaving layer execution to increase the hardware utilization. Besides, MAGMA [36] proposes a novel framework for mapping multiple DNNs on multiple accelerator cores. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. The focus of this article is on the accelerator architecture, and on examples of optimal utilization of that architecture based on fine-grained scheduling of tasks made up of LSTM sub-layers running on sub-cores. We exploit the intra-model data dependencies of both timesteps and layers in LSTMs, as shown in Figure 7(a). Both time step and layer dependencies have a linear dependence chain in the model. For LSTM inference, it is processed from the first layer to the last layer sequentially and from the first timestep to the last timestep in each layer. Besides, layers from different LSTMs are independent. Inspired by References [4, 40], which rely on manually designed heuristics, this work develops a set of heuristics that exploit the characteristics of LSTM inference workloads to reduce the scheduling overhead for the proposed homogeneous multi-core accelerator. We do not include algorithmic details (e.g., flowchart or pseudocode) about scheduling of these sub-layers tasks, other than mentioning related research on scheduling strategies [4, 14, 40] and LayerWeaver [53] as well as the custom mapping algorithm MAGMA [36]. Architectural tradeoffs of these scheduling strategies, based on queuing theory and other techniques, are important and would be best addressed by future work. Besides, further research will cover fine-grained dynamic scheduling for our accelerator, such as using Gang scheduling that schedules a gang of jobs onto free cores. Furthermore, other sophisticated scheduling strategies relevant to our approach will be studied and tested. 5.7 Power Consumption and Optimizations The proposed extension of the multi-core feature is based on splitting a large monolithic accelerator into several sub-accelerator cores, which does not change the total number of existing PEs in hardware. There is only a small bump in the hardware resources for the extended accelerator because of supporting new features, which may cause a little more power/energy consumption. However, the extra power consumption caused by extra FPGA hardware resources is small, since the new design only has 5% more area than the baseline [57] but can get 1.45 to 14.4 times higher performance, resulting in high power efficiency. Besides, the architectural extensions lead to higher runtime dynamic power, since these extensions provide more parallelism and higher hardware utilization. In general, the performance and power are positively correlated; the more accelerators used, the higher the power consumption. However, higher utilization also means that our architecture wastes less power on the underutilized execution resources than the baseline [57], which increases the overall power/energy efficiency. The overall increase in utilization also means the proposed design completes the same work in a shorter period of time, resulting in a potential lower energy consumption. Besides, with multiple sub-accelerator cores, the design has the potential to totally turn off some of the cores when there is only a small amount of workload to save power, which is not possible in the baseline where only a big and monolithic core is used. In future work, we will build an architecture-level power model targeting multi-threaded multi-core DNN accelerators so that we can quantify the impact of these architectural extensions on power/energy consumption. There are some potential optimizations for the power consumption targeting our architecture. Since the proposed design achieves a high hardware utilization, it may not require so many PEs to reach a design performance goal. Reducing the number of total PEs or reducing the hardware resources in design time can help to reduce the total power consumption. However, the design causes large dynamic power consumption because of the high utilization. Hence, one possible optimization is to control dynamic power consumption by gracefully degrading the core activities when a power consumption threshold is exceeded. It can be done by a feedback of the current power consumption of the cores, which may be estimated from some performance counters or read from some on-die sensors. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022.",
        "related work": "There are many previous studies about FPGA-based accelerations of persistent RNN/LSTM with weights stored in on-chip memory [19, 51, 58, 66\u201369]. Rybalkin et al. [69] introduces the hardware architecture of BiLSTM targeting OCR applications. All the weights and activations are quantized to 5-bit and stored in on-chip memory. [66] quantizes the design using 1-8 bits and achieves an accuracy that surpasses the one using floating-point on a given dataset. Besides, their later work [67, 68] proposes a novel hardware architecture of 2D-LSTM and investigates the tradeoff between precision and performance. There are also many previous studies about LSTM implementations with weights stored in off-chip memory on FPGAs [10, 26, 27, 54, 61], which has been recognized as the performance bottleneck. In LSTM computations, a weights matrix tends to be repeatedly loaded from off-chip memory if the size of on-chip memory is not large enough to store the entire matrix, which brings large latency. Guan et al. [27] proposes a smart memory organization with on-chip double buffers to overlap computations with data transfers. And References [22, 72] apply the batching technique to increase the throughput of LSTM inferences. For example, E-BATCH is proposed [72] for RNNs, which increases throughput while also improving energy efficiency on an ASIC-based accelerator. Apart from batching, References [54, 61, 63] introduce novel LSTM weights reuse schemes that utilizes the weights sharing characteristics in different timestep computations in one inference. These schemes reduce the access of the off-chip memory and decrease the energy cost as well as improve the design throughput. Some of the previous work [42, 80, 81, 85] adopts circulant matrix to optimize LSTMs by reducing the weight matrices of LSTM inferences. Besides, an approximate computing scheme is deployed for LSTMs using small tiles [65]. And stochastic computing is used to improve the energy efficiency of the RNN inference [43]. Reference [37] proposes an LSTM architecture based on reversible logic gates for low power circuit designs. POLAR [5] and BRDS [23] present FPGA-based pipelined and overlapped architecture for dense and sparse LSTM inferences, respectively. A multiFPGA approach [74] is proposed to accelerate multi-layer RNNs. It achieves a single-layer latency targeting deep RNNs with arbitrarily multiple layers using an FPGA-based cluster. Reference [49] presents a multi-FPGA-based architecture to accelerate neural machine translation. Reference [39] explores various partitioning strategies of large RNN inferences to achieve scalable multi-FPGA acceleration. It also analyses the performance impact of software pipelining and collective communications. PDL-FGPU [44], a specialized FPGA-based GPU overlay architecture is proposed to run persistent Deep Learning, including various RNN variants. Initiation interval (II) balancing [62] is proposed with a layerwise implementation of LSTMs to achieve ultra low latency. The layerwise implementation of RNNs is also used in accelerating Bayesian RNNs [18]. There is also much previous work [9, 11, 13, 20, 21, 28, 34, 48, 70, 83, 90] exploit the sparsity of data and weights with pruning to reduce the NN computation and also the memory footprint to achieve high performance and efficiencies. ESE [28] proposes a pruning technique that compresses a large LSTM model by 20\u00d7 without sacrificing the prediction accuracy. DeltaRNN [21] utilizes the Delta Network algorithm to reduce MxV operations and corresponding weight fetches by skipping dispensable computations during inference of RNNs. It updates the output of a neuron only when the neuron\u2019s activation changes by more than a delta threshold. Bank-Balanced Sparsity [9] is proposed to achieve both high prediction accuracy of a pruned LSTM model and high hardware efficiency of the model running on FPGAs. It partitions weight matrix rows into banks for parallel computing and adopts fine-grained pruning inside each bank to maintain model accuracy. BLINK [11] designs the LSTM inference using a bit-sparse data representation. And it turns multiplications into bit-shift operation to improve the energy efficiency while maintaining the LSTM inference accuracy for real-time calcium image processing. The extension [12] proposes a ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. combination of the bit-sparse quantization and the pruning methods for energy-efficient LSTM inferences. More recently, Spartus [20] exploits spatio-temporal sparsity to achieve ultra-low-latency inference using Column-Balanced Targeted Dropout. These studies are orthogonal to our proposed approach and hardware architecture. These techniques can be complementary to our approaches to achieve even higher performance and efficiency of RNN/LSTM inferences using FPGAs. The Brainwave design [19] is a single-threaded SIMD architecture for running real-time AI, including persistent RNNs. It achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large memory intensive RNN models at a batch size of 1. It stores NN model weights on-chip for RNNs to get a necessary high memory read bandwidth to achieve higher performances. A coarse-grained reconfigurable architecture (CGRA)-based RNN accelerator is proposed [89] on top of Plasticine [56] with a set of techniques for performing cross-kernel optimization in RNN cells. AERO [38] is a single-threaded instruction-set-based processor using a versatile vector-processing unit customized for RNN inferences on resourcelimited FPGAs. A Brainwave-like NPU is proposed in Reference [51] with a single-threaded architecture. They also explore the potential of combining a TensorRAM with FPGAs to provide large high-speed memory for large memory intensive RNN sequence models. Besides, their late work [8] deploys the Brainwave-like NPU on the Stratix 10 NX that is Intel\u2019s new AI-optimized FPGA featured with AI tensor blocks. [57, 60] proposes a novel latency-hiding hardware architecture based on columnwise MVM and fine-grained tiling strategy to eliminate data dependency of RNNs, which improves the design throughput of RNNs/LSTMs. However, all these NPUs targeting RNNs/LSTMs are single threaded. There is a large demand for architectural support of multi-DNN execution to maximize hardware utilization and reduce the costs of a large-scale production system. For example, TensorRT from Nvidia supports concurrent DNN execution for users to run multi-DNN workloads on the same GPU simultaneously. SMT-SA [71] presents a simultaneous multithreading approach for systolic arrays to solve the issue of underutilization because of zero-valued inputs. However, it is not able to deal with the underutilization issue that comes from the data dependencies in RNNs/LSTMs. A multi-threaded CGRA [3] has been proposed, but it is only for CNN accelerations. Reference [50] presents to generate in-order multi-threaded processing pipelined datapath automatically with the high-level specification of an unpipelined datapath. CUSTARD [16] presents a customisable multi-threaded FPGA soft processor with features including design space exploration and a compiler for automatic selection of custom instructions. More recently, PREMA [14] proposes a preemptible NPU with a preemptive scheduling algorithm to support multi-DNN execution. However, it does not consider the data dependencies between RNN/LSTM timesteps. AIMultiTasking [4] proposes to balance memory-intensive and compute-intensive tasks from different neural networks and process them in parallel by partitioning various layers into lots of identical sub-layers. The RNNs are handled just as FC layers in the scheduling scheme. However, RNNs are more complex than FC layers that have no data dependencies. In Reference [55], a dualcore accelerator for LSTM-RNN is proposed to execute multiple jobs simultaneously or have cores collaborate on a single job. However, they perform multithreading by utilizing the dual cores with one thread on one core, which is not an area / cost efficient way to add extra threads. Besides, there is some previous work targeting spatial multi-tenant execution. f-CNNx [78] employs multiple custom dedicated engines for the workloads with various CNN models. Reference [64] maps multiple LSTM models to an FPGA device by introducing a framework that alters their computational structure, opening opportunities for co-optimizing the memory requirements to the target architecture via applying compression schemes across multiple LSTM models. In Reference [40] HDAs are proposed to support the heterogeneous acceleration of multi-DNN workloads. More recently, Planaria [24] introduces the dynamical fission of a systolic-array-based DNN accelerator ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. at runtime to support spatial co-execution of multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations. This work designs a multi-threaded accelerator with fission to enable spatial and temporal co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations. It can handle multiple requests using multi-threaded mode with multiple sub-accelerator cores while still being able to run a task in a single-threaded mode with a large accelerator to attain higher performance and lower latency when targeting the workloads with high priority.",
        "conclusion": "This work proposes Remarn, a multi-threaded multi-core NPU supporting spatial and temporal coexecution of RNN/LSTM inferences to improve the processing abilities of cloud-based NPUs. The Remarn can increase the throughput while improving the hardware utilization of the cloud-based NPUs. We have implemented the proposed multi-threaded multi-core accelerator architecture on Intel Stratix 10 FPGAs with superior performance and efficiency, which demonstrates the effectiveness of our approaches. Our study shows that both multithreading and multi sub-accelerator core techniques can address the underutilization issue (resource inefficiency) when targeting small sized RNN workloads. Besides, it also shows that multi sub-accelerator core is slightly better than multithreading for small workloads. The design of quad-core with single-thread (1-T-Q) achieves a better utilization than the design of single-core quad-thread (4-T) for the LSTM of Lh = 128, as shown in Figure 10. But this figure also shows that the single-core quad-thread (4-T) design gets a better performance for the LSTM of Lh = 256. In addition, both techniques do not bring benefits to large LSTM workloads, since the baseline design has achieved high utilization. Moreover, this study shows that multithreading targets temporal co-execution, while multi-core targets spatial co-execution, and the two techniques can be combined to achieve a much better performance. Further research includes exploring how Remarn can benefit from other enhancements such as dynamic scheduling and runtime power analysing, studying the tradeoffs of varying the number and the heterogeneity of Remarn cores, and automating the proposed approaches to enable rapid development of efficient Remarn designs for datacentres as well as edge processing and embedded systems.",
        "references": "[1] Mohamed S. Abdelfattah, David Han, Andrew Bitar, Roberto DiCecco, Shane O\u2019Connell, Nitika Shanker, Joseph Chu, Ian Prins, Joshua Fender, Andrew C. Ling, et al. 2018. DLA: Compiler and FPGA overlay for neural network inference acceleration. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 411\u20134117. [2] Dario Amodei et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In Proceedings of the International Conference on Machine Learning. [3] Kota Ando, Shinya Takamaeda-Yamazaki, Masayuki Ikebe, Tetsuya Asai, and Masato Motomura. 2017. A multi- threaded CGRA for convolutional neural network processing. Circ. Syst. 8, 6 (2017), 149\u2013170. [4] Eunjin Baek, Dongup Kwon, and Jangwoo Kim. 2020. A multi-neural network acceleration architecture. In Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA\u201920). IEEE, 940\u2013953. [5] Erfan Bank-Tavakoli, Seyed Abolfazl Ghasemzadeh, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2019. Polar: A pipelined/overlapped fpga-based lstm accelerator. IEEE Trans. VLSI Syst. 28, 3 (2019), 838\u2013842. [6] John M. Borkenhagen et al. 2000. A multithreaded PowerPC processor for commercial servers. IBM J. Res. Dev. 44, 6 (2000), 885\u2013898. [7] Andrew Boutros et al. 2018. Embracing diversity: Enhanced DSP blocks for low-precision deep learning on FPGAs. In Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE. [8] Andrew Boutros, Eriko Nurvitadhi, Rui Ma, Sergey Gribok, Zhipeng Zhao, James C. Hoe, Vaughn Betz, and Martin Langhammer. 2020. Beyond peak performance: Comparing the real performance of AI-optimized FPGAs and GPUs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 10\u201319. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [9] Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun Nie, Dechen Zhan, Yunxin Liu, Ming Wu, and Lintao Zhang. 2019. Efficient and effective sparse LSTM on FPGA with bank-balanced sparsity. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 63\u201372. [10] Andre Xian Ming Chang, Berin Martini, and Eugenio Culurciello. 2015. Recurrent neural networks hardware imple- mentation on FPGA. arXiv:1511.05552. Retrieved from https://arxiv.org/abs/1511.05552. [11] Zhe Chen, Garrett J. Blair, Hugh T. Blair, and Jason Cong. 2020. BLINK: Bit-sparse LSTM inference kernel enabling efficient calcium trace extraction for neurofeedback devices. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 217\u2013222. [12] Zhe Chen, Hugh T. Blair, and Jason Cong. 2022. Energy efficient LSTM inference accelerator for real-time causal prediction. ACM Trans. Des. Autom. Electr. Syst. 27, 5, Article 44 (September 2022), 19 pages. https://doi.org/10.1145/ 349500 [13] Zhe Chen, Andrew Howe, Hugh T. Blair, and Jason Cong. 2018. CLINK: Compact LSTM inference kernel for energy efficient neurofeedback devices. In Proceedings of the International Symposium on Low Power Electronics and Design. 1\u20136. [14] Yujeong Choi and Minsoo Rhu. 2020. PREMA: A predictive multi-task scheduling algorithm for preemptible neu- ral processing units. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201920). IEEE, 220\u2013233. [15] Fran\u00e7ois Chollet et al. 2015. Keras: Deep Learning Library for theano and tensorflow. https://keras.io/k. [16] R. Dimond, O. Mencer, and W. Luk. 2006. Application-specific customisation of multi-threaded soft processors. IEE Proc. Comput. Digit. Techn. 153, 3 (2006), 173\u2013180. [17] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2625\u20132634. [18] Martin Ferianc, Zhiqiang Que, Hongxiang Fan, Wayne Luk, and Miguel Rodrigues. 2021. Optimizing Bayesian re- current neural networks on an FPGA-based accelerator. In Proceedings of the International Conference on FieldProgrammable Technology (ICFPT\u201921). IEEE, 1\u201310. [19] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. 2018. A configurable cloud-scale DNN processor for real-time AI. In Proceedings of the 45th Annual International Symposium on Computer Architecture. IEEE Press, 1\u201314. [20] Chang Gao, Tobi Delbruck, and Shih-Chii Liu. 2021. Spartus: A 9.4 TOp/s FPGA-based LSTM accelerator exploiting spatio-temporal sparsity. IEEE Transactions on Neural Networks and Learning Systems. (Early Access) [21] Chang Gao, Daniel Neil, Enea Ceolini, Shih-Chii Liu, and Tobi Delbruck. 2018. DeltaRNN: A power-efficient recurrent neural network accelerator. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 21\u201330. [22] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency RNN inference with cellular batching. In Pro- ceedings of the 13th European Conference on Computer Systems Conference (EuroSys\u201918). 1\u201315. [23] Seyed Abolfazl Ghasemzadeh, Erfan Bank Tavakoli, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2021. BRDS: An FPGA-based LSTM accelerator with row-balanced dual-ratio sparsification. arXiv:2101.02667. Retrieved from https: //arxiv.org/abs/2101.02667. [24] Soroush Ghodrati, Byung Hoon Ahn, Joon Kyung Kim, Sean Kinzer, Brahmendra Reddy Yatham, Navateja Alla, Hardik Sharma, Mohammad Alian, Eiman Ebrahimi, Nam Sung Kim, et al. 2020. Planaria: Dynamic architecture fission for spatial multi-tenant acceleration of deep neural networks. In Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO\u201920). IEEE, 681\u2013697. [25] Yoav Goldberg. 2016. A primer on neural network models for natural language processing. J. Artif. Intell. Res. 57 (2016), 345\u2013420. [26] Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai Shi, Xi Chen, Guangyu Sun, Wei Zhang, and Jason Cong. 2017. FP-DNN: An automated framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates. In Proceedings of the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201917). IEEE, 152\u2013159. [27] Yijin Guan, Zhihang Yuan, Guangyu Sun, and Jason Cong. 2017. FPGA-based accelerator for long short-term memory recurrent neural networks. In Proceedings of the 22nd Asia and South Pacific Design Automation Conference (ASPDAC\u201917). IEEE, 629\u2013634. [28] Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao, Yu Wang, et al. 2017. ESE: Efficient speech recognition engine with sparse LSTM on FPGA. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 75\u201384. [29] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv:1412.5567. Retrieved from https://arxiv.org/abs/1412.5567. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [30] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput. 9, 8 (1997), 1735\u20131780. [31] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. 2018. Detecting spacecraft anomalies using LSTM and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 387\u2013395. [32] Intel. [n.d.]. Understanding How Hyperflex Architecture Enables High Performance Systems. White Paper 01231. [33] Intel. 2020. Intel Agilex Variable Precision DSP Blocks User Guide. [34] Jingfei Jiang, Tao Xiao, Jinwei Xu, Dong Wen, Lei Gao, and Yong Dou. 2022. A low-latency LSTM accelerator using balanced sparsity based on FPGA. Microprocess. Microsyst. 89 (2022), 104417. [35] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture. 1\u201312. [36] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA: An optimization framework for mapping multiple DNNs on multiple accelerator cores. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201922). IEEE. [37] Kasem Khalil, Bappaditya Dey, Ashok Kumar, and Magdy Bayoumi. 2021. A reversible-logic based architecture for long short-term memory (LSTM) network. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201921). IEEE, 1\u20135. [38] Jinwon Kim, Jiho Kim, and Tae-Hwan Kim. 2021. AERO: A 1.28 MOP/s/LUT reconfigurable inference processor for recurrent neural networks in a resource-limited FPGA. Electronics 10, 11 (2021), 1249. [39] Dongup Kwon, Suyeon Hur, Hamin Jang, Eriko Nurvitadhi, and Jangwoo Kim. 2020. Scalable multi-FPGA accelera- tion for large RNNs with full parallelism levels. In Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC\u201920). IEEE, 1\u20136. [40] Hyoukjun Kwon, Liangzhen Lai, Michael Pellauer, Tushar Krishna, Yu-Hsin Chen, and Vikas Chandra. 2021. Het- erogeneous dataflow accelerators for multi-DNN workloads. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 71\u201383. [41] Martin Langhammer, Bogdan Pasca, Gregg Baeckler, and Sergey Gribok. 2019. Extracting INT8 multipliers from INT18 multipliers. In Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201919). IEEE. [42] Zhe Li, Caiwen Ding, Siyue Wang, Wujie Wen, Youwei Zhuo, Chang Liu, Qinru Qiu, Wenyao Xu, Xue Lin, Xuehai Qian, et al. 2019. E-RNN: Design optimization for efficient recurrent neural networks in FPGAs. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201919). IEEE, 69\u201380. [43] Yidong Liu, Leibo Liu, Fabrizio Lombardi, and Jie Han. 2019. An energy-efficient and noise-tolerant recurrent neural network using stochastic computing. IEEE Trans. VLSI Syst. 27, 9 (2019), 2213\u20132221. [44] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi, David Sheffield, Rob Pelt, Martin Langhammer, Jaewoong Sim, Aravind Dasu, and Derek Chiou. 2021. Specializing FGPU for persistent deep learning. ACM Trans. Reconfig. Technol. Syst. 14, 2 (2021), 1\u201323. [45] Andrew Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 142\u2013150. [46] Cameron McNairy and Rohit Bhatia. 2005. Montecito: A dual-core, dual-thread itanium processor. IEEE Micro 25, 2 (2020), 10\u201320. [47] Sparsh Mittal and Sumanth Umesh. 2021. A survey on hardware accelerators and optimization techniques for RNNs. J. Syst. Arch. 112 (2021), 101839. [48] Guocai Nan, Chenghua Wang, Weiqiang Liu, and Fabrizio Lombardi. 2020. DC-LSTM: Deep compressed LSTM with low bit-width and structured matrices. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201920). IEEE, 1\u20135. [49] Eriko Nurvitadhi, Andrew Boutros, Prerna Budhkar, Ali Jafari, Dongup Kwon, David Sheffield, Abirami Prabhakaran, Karthik Gururaj, Pranavi Appana, and Mishali Naik. 2019. Scalable low-latency persistent neural machine translation on CPU server with multiple FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 307\u2013310. [50] Eriko Nurvitadhi, James C. Hoe, Shih-Lien L. Lu, and Timothy Kam. 2010. Automatic multithreaded pipeline synthesis from transactional datapath specifications. In Proceedings of the Design Automation Conference. IEEE, 314\u2013319. [51] Eriko Nurvitadhi, Dongup Kwon, Ali Jafari, Andrew Boutros, Jaewoong Sim, Phillip Tomson, Huseyin Sumbul, Gregory Chen, Phil Knag, Raghavan Kumar, et al. 2019. Why compete when you can work together: Fpga-asic integration for persistent rnns. In Proceedings of the IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201919). IEEE, 199\u2013207. [52] Eriko Nurvitadhi, Jaewoong Sim, David Sheffield, Asit Mishra, Srivatsan Krishnan, and Debbie Marr. 2016. Accelerat- ing recurrent neural networks in analytics servers: Comparison of FPGA, CPU, GPU, and ASIC. In Proceedings of the 26th International Conference on Field Programmable Logic and Applications (FPL\u201916). IEEE, 1\u20134. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [53] Young H. Oh, Seonghak Kim, Yunho Jin, Sam Son, Jonghyun Bae, Jongsung Lee, Yeonhong Park, Dong Uk Kim, Tae Jun Ham, and Jae W. Lee. 2021. Layerweaver: Maximizing resource utilization of neural processing units via layer-wise scheduling. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 584\u2013597. [54] Naebeom Park, Yulhwa Kim, Daehyun Ahn, Taesu Kim, and Jae-Joon Kim. 2020. Time-step interleaved weight reuse for LSTM neural network computing. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 13\u201318. [55] Lu Peng, Wentao Shi, Jian Zhang, and Samuel Irving. 2019. Exploiting model-level parallelism in recurrent neural net- work accelerators. In Proceedings of the IEEE 13th International Symposium on Embedded Multicore/Many-core Systemson-Chip (MCSoC\u201919). IEEE, 241\u2013248. [56] Raghu Prabhakar, Yaqi Zhang, David Koeplinger, Matt Feldman, Tian Zhao, Stefan Hadjis, Ardavan Pedram, Christos Kozyrakis, and Kunle Olukotun. 2017. Plasticine: A reconfigurable architecture for parallel patterns. In Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA\u201917). IEEE, 389\u2013402. [57] Zhiqiang Que et al. 2020. Optimizing reconfigurable recurrent neural networks. In Proceedings of the IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201920). IEEE. [58] Zhiqiang Que, Yanyang Liu, Ce Guo, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Real-time anomaly detection for flight testing using AutoEncoder and LSTM. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 379\u2013382. [59] Zhiqiang Que, Hiroki Nakahara, Hongxiang Fan, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and Wayne Luk. 2020. A reconfigurable multithreaded accelerator for recurrent neural networks. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 20\u201328. [60] Zhiqiang Que, Hiroki Nakahara, Eriko Nurvitadhi, Andrew Boutros, Hongxiang Fan, Chenglong Zeng, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, and Wayne Luk. 2022. Recurrent neural networks with column-wise matrix-vector multiplication on FPGAs. IEEE Trans. VLSI Syst. (2022). [61] Zhiqiang Que, Thomas Nugent, Shuanglong Liu, Li Tian, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Efficient weight reuse for large LSTMs. In Proceedings of the IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201919), Vol. 2160. IEEE, 17\u201324. [62] Zhiqiang Que, Erwei Wang, Umar Marikar, Eric Moreno, Jennifer Ngadiuba, Hamza Javed, Bart\u0142omiej Borzyszkowski, Thea Aarrestad, Vladimir Loncar, Sioni Summers, Maurizio Pierini, Peter Y. Cheung, and Wayne Luk. 2021. Accelerating recurrent neural networks for gravitational wave experiments. In Proceedings of the 32th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201921). IEEE. [63] Zhiqiang Que, Yongxin Zhu, Hongxiang Fan, Jiuxi Meng, Xinyu Niu, and Wayne Luk. 2020. Mapping large LSTMs to FPGAs with weight reuse. J. Sign. Process. Syst. 92, 9 (2020), 965\u2013979. [64] Stefano Ribes, Pedro Trancoso, Ioannis Sourdis, and Christos-Savvas Bouganis. 2020. Mapping multiple LSTM models on FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 1\u20139. [65] Michalis Rizakis, Stylianos I. Venieris, Alexandros Kouris, and Christos-Savvas Bouganis. 2018. Approximate FPGA- based LSTMs under computation time constraints. In Proceedings of the International Symposium on Applied Reconfigurable Computing. Springer, 3\u201315. [66] Vladimir Rybalkin, Alessandro Pappalardo, Muhammad Mohsin Ghaffar, Giulio Gambardella, Norbert Wehn, and Michaela Blott. 2018. FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE. [67] Vladimir Rybalkin, Chirag Sudarshan, Christian Weis, Jan Lappas, Norbert Wehn, and Li Cheng. 2020. Efficient hard- ware architectures for 1D-and MD-LSTM networks. J. Sign. Process. Syst. 92, 11 (2020), 1219\u20131245. [68] Vladimir Rybalkin and Norbert Wehn. 2020. When massive GPU parallelism ain\u2019t enough: A novel hardware architec- ture of 2D-LSTM neural network. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. [69] Vladimir Rybalkin, Norbert Wehn, Mohammad Reza Yousefi, and Didier Stricker. 2017. Hardware architecture of bidi- rectional long short-term memory neural network for optical character recognition. In Proceedings of the Conference on Design, Automation & Test in Europe. 1394\u20131399. [70] Runbin Shi, Junjie Liu, K.-H. Hayden So, Shuo Wang, and Yun Liang. 2019. E-LSTM: Efficient inference of sparse LSTM on embedded heterogeneous system. In Proceedings of the 56th ACM/IEEE Design Automation Conference (DAC\u201919). IEEE, 1\u20136. [71] Gil Shomron, Tal Horowitz, and Uri Weiser. 2019. SMT-SA: Simultaneous multithreading in systolic arrays. IEEE Comput. Arch. Lett. 18, 2 (2019), 99\u2013102. [72] Franyell Silfa, Jose Maria Arnau, and Antonio Gonzalez. 2020. E-BATCH: Energy-efficient and high-throughput RNN batching. ACM Transactions on Architecture and Code Optimization (TACO) 19, 1 (2020), 1\u201323. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [73] Burton J. Smith. 1986. A pipelined, shared resource MIMD computer. In Advanced Computer Architecture. 39\u201341. [74] Yuxi Sun, Akram Ben Ahmed, and Hideharu Amano. 2019. Acceleration of deep recurrent neural networks with an FPGA cluster. In Proceedings of the 10th International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies. 1\u20134. [75] Zhanrui Sun, Yongxin Zhu, Yu Zheng, Hao Wu, Zihao Cao, Peng Xiong, Junjie Hou, Tian Huang, and Zhiqiang Que. 2018. FPGA acceleration of LSTM based on data for test flight. In Proceedings of the IEEE International Conference on Smart Cloud (SmartCloud\u201918). IEEE, 1\u20136. [76] Tian Tan, Eriko Nurvitadhi, David Shih, and Derek Chiou. 2018. Evaluating the highly-pipelined intel stratix 10 FPGA architecture using open-source benchmarks. In Proceedings of the International Conference on Field-Programmable Technology (FPT\u201918). IEEE, 206\u2013213. [77] James E. Thornton. 1964. Parallel operation in the control data 6600. In Proceedings of the Fall Joint Computer Confer- ence, Part II: Very High Speed Computer Systems. 33\u201340. [78] Stylianos I. Venieris and Christos-Savvas Bouganis. 2018. f-CNNx: A toolflow for mapping multiple convolutional neural networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 381\u20133817. [79] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3156\u20133164. [80] Shuo Wang, Zhe Li, Caiwen Ding, Bo Yuan, Qinru Qiu, Yanzhi Wang, and Yun Liang. 2018. C-LSTM: Enabling efficient LSTM using structured compression techniques on FPGAs. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 11\u201320. [81] Zhisheng Wang, Jun Lin, and Zhongfeng Wang. 2017. Accelerating recurrent neural networks: A memory-efficient approach. IEEE Trans. VLSI Syst. 25, 10 (2017), 2763\u20132775. [82] Zhao Wang, Guangyu Sun, Jingchen Zhu, Zhe Zhou, Yijiang Guo, and Zhihang Yuan. 2021. METRO: A software- hardware co-design of interconnections for spatial DNN accelerators. arXiv:2108.10570 [cs.AR]. Retrieved from https: //arxiv.org/abs/2108.10570. [83] Jiaquan Wu, Feiteng Li, Zhijian Chen, and Xiaoyan Xiang. 2019. A 3.89-GOPS/mW scalable recurrent neural network processor with improved efficiency on memory and computation. IEEE Trans. VLSI Syst. 27, 12 (2019), 2939\u20132943. [84] Xilinx. 2017. Deep Learning with INT8 Optimization on Xilinx Devices. Retrieved from https://www.xilinx.com/ support/documentation/whitepapers/wp486-deep-learning-int8.pdf. [85] Krishna Praveen Yalamarthy et al. 2019. Low-complexity distributed-arithmetic-based pipelined architecture for an LSTM network. IEEE Trans. VLSI Syst. 28, 2 (2019), 329\u2013338. [86] Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria Arnau, and Antonio Gonz\u00e1lez. 2019. LSTM- sharp: An adaptable, energy-efficient hardware accelerator for long short-term memory. arXiv:1911.01258. Retrieved from https://arxiv.org/abs/1911.01258. [87] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. 2015. Beyond short snippets: Deep networks for video classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4694\u20134702. [88] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv:1409.2329. Retrieved from https://arxiv.org/abs/1409.2329. [89] Tian Zhao, Yaqi Zhang, and Kunle Olukotun. 2019. Serving recurrent neural networks efficiently with a spatial accel- erator. Proc. Mach. Learn. Syst. 1 (2019), 166\u2013177. [90] Yong Zheng, Haigang Yang, Yiping Jia, and Zhihong Huang. 2021. PermLSTM: A high energy-efficiency LSTM accel- erator architecture. Electronics 10, 8 (2021), 882. Received 2 September 2021; revised 18 February 2022; accepted 1 May 2022 ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022.",
        "title": "Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks"
    },
    "full_text": "Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks\n\n4 Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks ZHIQIANG QUE, Imperial College London, UK HIROKI NAKAHARA, Tokyo Institute of Technology, Japan HONGXIANG FAN, Imperial College London, UK HE LI, University of Cambridge, UK JIUXI MENG, Imperial College London, UK KUEN HUNG TSOI and XINYU NIU, Corerain Technologies Ltd., China ERIKO NURVITADHI, Intel Corporation, USA WAYNE LUK, Imperial College London, UK This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters. CCS Concepts: \u2022 Hardware \u2192 Hardware accelerators; Application specific processors; \u2022 Computer systems organization\u2192Multicore architectures; Neural networks; Special purpose systems; Additional Key Words and Phrases: Accelerator architecture, recurrent neural networks, multi-tenant execution The support of the United Kingdom EPSRC (grant numbers EP/V028251/1, EP/L016796/1, EP/N031768/1, EP/P010040/1, and EP/S030069/1), Corerain and Intel is gratefully acknowledged. Authors\u2019 addresses: Z. Que, H. Fan, J. Meng, and W. Luk, Imperial College London, Exhibition Rd, South Kensington, London SW7 2BX, UK; emails: {z.que, h.fan17, jiuxi.meng16, w.luk}@imperial.ac.uk; H. Nakahara, Tokyo Institute of Technology, Ohokayama 1-21-2, Tokyo, 1528550, Japan; email: nakahara.h.ad@m.titech.ac.jp; H. Li, University of Cambridge, Cambridge CB2 1TN, UK; email: he.li@ieee.org; K. H. Tsoi and X. Niu, Corerain Technologies Ltd.,14F Changfu Jinmao Building (CFC), Shenzhou, China; email: {kuenhung.tsoi, xinyu.niu}@corerain.com; E. Nurvitadhi, Intel Corporation, Jones Farm Campus, Hillsboro, OR, 97124-6463, USA; email: eriko.nurvitadhi@intel.com. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2022 Association for Computing Machinery. 1936-7406/2022/12-ART4 $15.00 https://doi.org/10.1145/3534969 ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. ACM Reference format: Zhiqiang Que, Hiroki Nakahara, Hongxiang Fan, He Li, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and Wayne Luk. 2022. Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks. ACM Trans. Reconfig. Technol. Syst. 16, 1, Article 4 (December 2022), 26 pages. https://doi.org/10.1145/3534969 \n\n1 INTRODUCTION\n\nRecurrent Neural Network (RNN) has been the key component of artificial intelligence (AI) applications where the inputs are sequential, such as natural language processing [25], speech recognition [2, 28], and video analysis [17, 87]. Long Short-Term Memory (LSTM) is the most popular type of RNNs. Since low latency is key for a seamless user experience in such applications, efficient and real-time acceleration of RNNs/LSTMs is required. FPGAs have been used to speed up the inference of LSTMs [19, 27, 51, 52, 75], showing the benefits of low latency and low power consumption compared to CPUs or GPUs. However, existing RNN/LSTM accelerators cannot support cost-effective multi-RNN execution. Cloud providers must minimize their huge operation costs by running as many applications on a given server as possible, while satisfying the quality of each service. In Google data centers, Convolutional Neural Networks (CNNs) and Multi-Layer Perceptions (MLP) comprise 5% and 61% of the workload, respectively, while LSTMs makes up 29% [35]. However, most of the existing LSTM accelerators are only able to perform one inference at a time [19, 27, 28, 47, 51, 57]. These accelerators can process multi-LSTM tasks by executing one by one in sequence, resulting in inefficiency when multiple requests come at the same time as shown in Figure 1. It may make later tasks wait for a long time before a hardware core is available, since earlier LSTM tasks may have a large number of timesteps involving many iterations, e.g., an LSTM layer in DeepSpeech [29] has 1,500 timesteps. Besides, some applications employ not one but multiple LSTMs to collaboratively achieve satisfactory performance. A spacecraft anomalies detection system [31] even contains over hundreds of LSTM models, each modeling a single telemetry channel and predicting values for that channel, which demonstrates the necessity of supporting multi-LSTM execution. Furthermore, conventional LSTM accelerators are often implemented by deploying all computing resources to support a single computational engine on a large scale, leveraging data-level parallelism. For instance, Brainwave [19] devised by Microsoft is a single-threaded neural processing unit (NPU) that involves 96,000 processing elements (PEs). However, when the workload of a targeted LSTM task is small, these hardware resources will not be fully utilized, e.g., the hardware utilization is lower than 1% [19] for Brainwave and lower than 15% for the Brainwave-like NPU [51] when running an LSTM model (ht =256). It is challenging to design an accelerator to support cost-effective multi-LSTM execution. This article introduces a reconfigurable multi-threaded multi-core NPU for accelerating RNN/ LSTM inferences by increasing the hardware utilization for better performance. It improves the processing abilities of cloud-based NPUs as well as the Quality of Service. Our primary goal is to efficiently enhance the scalability potential of NPU cores. The most area- /cost-efficient way to add logical cores is multithreading. Essentially, multithreading retrieves unused performance (where computational cores are idle because of events) by switching to another thread. Multithreading also does not affect peak performance when working in a single-threaded mode. Usually, the execution of multiple neural networks has the potential to mitigate the idle issues, because layers from different neural networks can be scheduled freely without any issue of dependencies. Running multiple tasks can also be realized by batch techniques that provide multiple requests to a neural network to produce multiple results together. However, the batch techniques can harm latency, ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. because different requests may not arrive at the same time [22], which means that a newly arrived request must wait until the batch is formed, which brings a significant latency penalty as shown in Figure 2. Remarn is inspired by coarse-grained multithreading utilized in modern CPUs such as IBM RS64-IV [6] and Intel Montecito [46], which makes a core switch to a different hardware context when a thread is stalled due to some events. Remarn consists of a custom coarse-grained multi-threaded (CGMT) LSTM hardware architecture that switches tasks among threads when LSTM computational engines meet data hazard. When one logical NPU core is stalled, the other can make progress. Coarse-grained multithreading is a mature technique in modern CPU designs. However, few studies concern combining the CGMT and NPUs, especially for RNNs/LSTMs. There is also fine-grained multi-threading [73, 77] that switches the context every cycle, but it brings more hardware complexity than CGMT. Unlike CNNs that do not have memory cells and can run different layers from different neural networks iteratively, RNNs/LSTMs contains many memory cells, which makes it difficult to process different timesteps from different RNN layers or models, since they have different cell memory statuses when using a single-threaded NPU. It has to finish the running of the preceding RNN inference or layer until the next inference or layer can start. The existence of inter-timestep dependencies within an RNN model prevents the following timesteps from even starting their execution until the current timestep\u2019s completion, leading to hardware underutilization and inefficiency. To address this challenge, this work proposes the CGMT-LSTM, which can intelligently switch to an alternate thread of computation when the data hazard happens, e.g., the inter-timestep dependencies of RNNs, to increase hardware utilization and boost design performance. Besides, instead of deploying all computing resources to form a single physical core on a large scale like Brainwave [19] and our previous design [59], we design an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, and each core can accept new RNN requests. The multiple LSTM models used in the application of spacecraft anomaly detection system [31] have only a small hidden vector size that is less than 128, but they have 250 timesteps that are large. LSTM models that have a large number of timesteps but utilize a small size are the most tangible examples, since they require dealing with lots of dependencies, as well as the parallel task of matrix-vector multiplications (MVMs) [86]. The Brainwave [19] involves big MVM \u201ctile engines\u201d that can effectively process a 400\u00d7240 matrix in parallel. Besides, our previous NPU [59] is based on a tile size of 1, 024 \u00d7 16, resulting in 16,384 effective PEs. Any MVM that does not map to this dimension will leave some resources idle and small MVMs even ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. require zero paddings, resulting in low utilization. Splitting a large accelerator into several small sub-accelerator cores can not only help to find a better mapping to improve hardware utilization but also provide more physical cores to spatially co-locate multiple RNN inferences on the same hardware. As Reference [19] mentions, RNN programs have a critical loop-carry dependence on the ht vector. If the full pipeline cannot return ht to the vector register file in time for the next timestep computation, then the MVM unit will stall until ht is available. Thus, even if a large MVM engine finishes the LSTM gates operations in a short period, e.g., one cycle, then the design still needs to wait for the ht returned from the pipeline, which shows the limitation of the architecture using a large engine. This work splits the accelerator with a single large engine into several smaller sub-accelerator cores, each of them processing small LSTM models more efficiently by adopting the tiling-based columnwise MVM approach [57]. It addresses the challenge of accelerating a large number of small LSTM models with large timesteps. Please note that these sub-accelerator cores can work together as a single large accelerator when necessary to deal with the high priority workloads that require the lowest end to end latency. To the best of our knowledge, Remarn is the first coarse-grained multi-threaded and multi-core LSTM accelerator architecture capable of achieving high performance and efficient multi-LSTM execution. Our contributions are the following: \u2022 A novel reconfigurable multi-threaded multi-core neural processing unit to enable effective and efficient multi-neural network execution for LSTM inferences. \u2022 A custom CGMT LSTM accelerator architecture that also can be partitioned into several full- fledged sub-accelerator cores, which significantly improves hardware utilization and design performance. \u2022 A custom tiling method for LSTMs, which minimizes the intermediate results buffers when combining CGMT and partition, thereby increasing the accelerator area efficiency. \u2022 A comprehensive evaluation on the proposed methods and hardware architecture. Relationship to Prior Publications: This article expands on a conference paper [59] with the baseline design proposed in Reference [57]. The baseline design [57] involves a novel latency-hiding hardware architecture based on columnwise matrix-vector multiplication. It has much higher performance and hardware utilization than other designs for RNN models with different sizes, but it still suffers from underutilization when the model size is small. Reference [59] addresses the underutilization issue by introducing CGMT that enables temporal multi-neural network execution to improve the performance when the RNN models are small while still maintaining the ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. performance of RNN models with medium to large sizes. It mainly time-multiplexes a large RNN accelerator across various RNNs workloads. This article addresses a limitation of the work described in our previous papers, that they do not allow the large computation engine to be partitioned into several smaller ones, so they do not support spatial co-execution of multiple RNN inferences. This limitation brings a severe hardware underutilization issue when targeting acceleration for a large number of small RNN models that are commonly used in many applications [27, 31, 62, 88]. This work introduces an accelerator hardware architecture that can be partitioned into multiple full-fledged sub-accelerator cores, combining multithreading with multi-core techniques to enable temporal and spatial multi-neural network execution on cloud-based NPUs. The proposed novel dimension of optimization allows us to obtain significant improvement in throughput while reducing latency over the previous design [59]. This article adds the design of multiple sub-accelerator cores for spatial co-execution of RNNs in Section 3.3. Sections 4.1\u20134.3 describe a revised hardware architecture, and Sections 5.3\u20135.6 contain new evaluation results. \n\n2 BACKGROUND AND PRELIMINARIES\n\nRNNs/LSTMs have been shown to have useful properties with many significant applications. Among the many RNN variants, the most popular one is the LSTM that was initially proposed in 1997 [30]. This study follows the standard LSTM cell [17, 19, 27, 51]. Figure 3 contains a diagram of an LSTM cell. It utilizes the following equations to compute the gates and produce the results for the next time step. it = \u03c3 (Wi [xt ,ht\u22121] + bi ), ft = \u03c3 (Wf [xt ,ht\u22121] + bf ) \u0434t = tanh(W\u0434[xt ,ht\u22121] + bu ), ot = \u03c3 (Wo[xt ,ht\u22121] + bo ) (1) ct = ft ct\u22121 + it \u0434t , ht = ot tanh(ct ). Here, \u03c3 and tanh represent the sigmoid function and hyperbolic tangent function. Both are activation functions; it , ft ,\u0434t , and ot denote the output of the input gate (i-gate), forget gate (f - gate), input modulation gate (\u0434-gate), and output gate (o-gate) at timestep t , respectively. The \u0434gate is often considered as a sub-part of the i-gate. Each LSTM gate consists of a MVM unit and a corresponding activation function unit, as shown in Figure 3. The operator denotes an elementwise multiplication.W is the weight matrix for both input and hidden units, since the input vector ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. and hidden vector are combined in the equations. The b terms denote the bias vectors. ct is the internal memory cell status at timestep t while ht is the hidden vector that is the output of the cell and passed to the next timestep calculation or next LSTM layer. The LSTM information flow is controlled by these four gates with details shown in Figure 3. The i-gate decides what new information is to be written into the internal memory cell; the \u0434-gate modulates the information processed by i-gate via adding non-linearity. Note that only \u0434-gate utilizes hyperbolic tangent as its activation function while all the other three gates utilize sigmoid. The f -gate decides what old information is no longer needed and can be discarded so there are element-wise multiplications between the output of f -gate and memory cell status in the previous timestep ct\u22121. Its output will be added to the products of the outputs from i-gate and \u0434-gate to form the current status of the internal memory cell. The o-gate decides what the value of the current hidden vector (ht ) should be by multiplying the current status of the memory cell after the hyperbolic tangent function, as shown in the LSTM-Tail in Figure 3. Our work focuses on the optimization of RNN inferences involving standard LSTMs, but the proposed techniques can be applied to other deep neural networks (DNN) inferences. \n\n3 DESIGN AND OPTIMIZATION METHODOLOGY\n\nAccelerating RNN/LSTM inferences efficiently is challenging because of their recurrent nature and data dependencies. This section first presents the coarse-grained multithreading for accelerating RNN/LSTM and then introduces a partitioning strategy of LSTM weight matrix to apply sub-layer granularity scheduling with fine-grained tasks for Remarn architecture. Finally, we present the multi-core accelerator to enable spatial co-execution of RNN models or layers to improve the design performance. Some design parameters are defined in Table 1. \n\n3.1 Multithreading for Recurrent Neural Processing Unit\n\nThere is a large demand for architectural support of multi-DNN execution to maximize the hardware utilization and reduce the costs of running a large-scale production system. However, most of the existing LSTM accelerators can only run one task at a time [19, 27, 28, 47, 51, 57]. This work proposes a CGMT LSTM NPU that switches on the event when the data hazard of a computational unit happens in the LSTM computation. The general idea is when a thread is stalled because of some events, e.g., cache misses, the multi-threaded core can switch to a different hardware context. In the proposed CGMT LSTM accelerator, the event is the hazard caused by the data dependency between the timesteps of sequential calculation in LSTMs. We propose to maintain multiple thread contexts in a recurrent NPU core so that when the first thread stalls, the second one can carry on, as shown in Figure 4. Thus, it utilizes computational resources more efficiently than a single thread core. It can increase the design performance by utilizing thread-level parallelism and enhance the NPU hardware utilization. Besides, a preceding LSTM model or layer may have thousands of sequences (timesteps), which occupies the NPU core for a long time, leading to a long waiting time for the latter requests before they have an available core to run. However, some services have strict latency constraints, since ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. the response time directly relates to user experience, e.g., intelligent personal assistants are one of the examples where real-time DL is utilized to handle user speech and output smart responses. The conventional single-threaded NPUs can only store the old thread\u2019s context to memory and retrieve the new thread\u2019s context using preemption mechanisms [14] to run another task. However, it will have a large context switch penalty. In our multi-threaded NPU for LSTMs, the new task can execute from another thread as soon as it comes, as shown in Figure 4. Please note that a particular thread may still stall due to data hazard, but the physical core is not stalled, since multiple threads share the same computational physical core, which leads to \u201cVirtual idle\u201d as shown in this figure. We believe that further optimizations, e.g., simultaneous multithreading (SMT) can be adopted to our NPU design to gain even higher performance. We leave it for our future work, because it does not have a big impact on the conclusions we draw from this work. \n\n3.2 Weights Matrix Blocking Strategy\n\nThe LSTM calculation of one timestep in an LSTM layer has four MVM operations according to the Equations (1). Besides, these four MVM are independent and share the same size. Since the four matrices of i, f ,o,u gates in LSTMs have the same size, these matrices can be combined into a single large matrix [1, 57]. Figure 5 illustrates the basic idea. Thus, in the calculation of one timestep of an LSTM layer, we can only focus on the optimizations of one large matrix multiplying one vector for the whole LSTM cell rather than four small matrices multiplied by one vector. This is a general optimization that can be utilized for any MVM operations that share the same input vector. Because each matrix from LSTM gates has a size of Lh \u00d7 (Lx + Lh), the large fused matrix has a size of (4 \u00d7 Lh) \u00d7 (Lx + Lh). Usually, deep neural networks have many compute intensive operations involving large weight matrices, which can be accelerated by running in parallel. However, when deploying on FPGAs, the parallelism is constrained by the limited hardware resources on the targeted FPGAs. It means that the whole MVM computation may not be fully unrolled and performed at once, especially for the large LSTMs. To use the computational resources efficiently, the combined weight matrix of an LSTM layer is partitioned into multiple sub-layers in advance depending on the configuration of the accelerator cores and LSTM layer sizes. Specifically, an LSTM layer of one timestep is partitioned into a number of sub-layers with equal size, as shown in Figure 6(a). For simplicity, the number of the sub-layers in the example is set as 2 to illustrate the idea, but a real design could have more sub-layers, and it depends on the design constraints. Then, Element-based Parallelism ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. (EP) and Vector-based Parallelism (VP) are applied to exploit the available parallelism [57]. The number of the sub-layers is determined by (4\u00d7Lh) V P . The sub-layer is then divided into many small tiles, each having a size of (EP ,VP ), as shown in Figure 6(b). In each cycle, our Remarn core is able to process the MVM of a tile and a sub-vector of [xt ,ht\u22121] with a size of EP . To increase the design parallelism, VP should be chosen to be as large as possible. However, the largest value of VP equals Hw , which is 4 \u00d7 Lh, since there are only four gates in an LSTM. Thus, the smallest number of sub-layers is one when VP is 4 \u00d7 Lh. In this study, sub-layer granularity scheduling is adopted with fine-grain tasks of sub-layers for multi-LSTM execution. Besides, we interleave the rows of the four weight matrices, so that the related elements from the four gates output are adjacent in the result vector. Thus, the LSTM gate outputs can multiply with each other easily using the element-wise operations in the LSTM-tail unit. It also means that there is no data dependency between these sub-layers. The optimization of interleaving also removes the requirement to buffer large intermediate outputs from four LSTM gates, since the result sub-vectors from the sub-layers are not related and will be reduced in the tail unit soon. Each sub-vector of the MVM output can be handled individually in the LSTM-tail units. There is no need to wait for other sub-layer results to get the LSTM memory cell status and hidden vector of the current sub-layer. The proposed CGMT core will always finish processing all the sub-layers in one timestep before it switches to another thread, because the data hazard in the LSTMs happens when timestep changes. Compared with a fine-grained multithreading scheme that switches the context between threads in every cycle, we can avoid buffering these large intermediate MVM operation values as well as element-wise operation values, since these values will finally form the sub-vectors of LSTM memory cells and hidden units. Only the thread dedicated buffers are needed to be added for storing the LSTM memory cells and hidden units in each timestep, since different threads process different LSTM tasks. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. \n\n3.3 Multi-core for Recurrent Neural Processing Unit\n\nMany RNN/LSTM accelerators deploy all the computing resources to form a single accelerator core with a large scale, leveraging data-level parallelism. However, this type of accelerator is only able to process one task at a time, resulting in potential underutilization and inefficiency when multiple tasks come at the same time, especially in large data centers. Multithreading described in Section 3.1 can address the issue partially but it still cannot easily deal with a large number of small workloads. Recently, spatial architectures have become a popular solution to build high throughput accelerators for DNNs [24, 78, 82]. Generally, multiple PEs are grouped as an engine, and these engines are connected with an on-chip interconnection to enable data communication during processing. f-CNNx [78] proposes multiple custom dedicated engines for the workloads with various CNN models. Planaria [24] proposes to dynamically fission a systolic array-based DNN accelerator at runtime to spatially co-locate multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations that are absent from accelerators targeting CNNs and fully connected (FC) layers. This work adopts a similar idea to Reference [24]. But instead of using a systolic two-dimensional matrix-matrix multiplication array, our architecture uses matrix-vector multiplication engines, since RNNs and MLPs are dominated by matrix-vector multiplication operations. With multiple sub-accelerator cores, it enables spatial co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations. In addition to inter-model parallelism, which is naturally supported by using multiple subaccelerator cores, such as co-execution of multiple models, our design also enables intra-model parallelism among different layers and timesteps of the RNN layers. Generally, in an RNN model, a latter layer or timestep should wait until its preceding layer or timestep finishes, since some of its inputs are the output of the preceding layer or timestep, as shown in Figure 7(a). For example, the computation that requires c00 and h00 cannot start until they are available from the preceding layer or timestep. Using a single-threaded single-core accelerator, two LSTM workloads will run as Figure 7(c) shows. The accelerator will process the LSTM layers and timesteps iteratively. The temporal inefficiency comes from the stall of data dependencies and the spatial inefficiency comes from mapping a small network to a large core leaving some resources idle. With a multi-threaded accelerator, each layer of the LSTM can run on different threads to remove the stall or the temporal inefficiency that comes from the RNN nature data dependencies. Please note, a special computation order, as shown in Figure 7(b) is required to achieve stall-free processing using a multi-threaded single-core accelerators, as shown in Figure 7(d). However, when LSTMs are small, many resources are left unused when mapping them to a large core. In such cases, a small core will get a similar latency to the one using a large core. Besides, with multiple sub-accelerator cores, the processing of the cascaded LSTM layers can be overlapped in a layerwise coarse-grained pipeline as shown in Figure 7(e). The second layer does not need to wait for the whole sequence of hidden vectors of the first layer to be ready. Just one hidden vector from the preceding LSTM layer is sufficient to start the calculation of the next LSTM layer. It helps to reduce the overall design latency. Besides, since the output of the preceding layer sinks directly in the following sub-accelerator core, there is no need for buffering large intermediate outputs, which could help to improve the area efficiency. We can further optimize the processing after combining the multithreading with multi-core. Figure 7(f) shows two LSTMs are mapping to a dual-thread three-core accelerator, which achieves the best total latency. While the multi-core scheme leads to high performance, it also presents a new challenge: In some situations it cannot provide sufficient bus bandwidth per core. In Remarn, the multiple cores share the same front side bus. But this will not affect on the bus bandwidth, because we do not add ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. new Remarn big cores but only split the original big core into several small ones, e.g., four small cores. When the four small cores co-operate to handle a large network, the quad-core requires the same bandwidth as the one with a big monolithic core. The total bus bandwidth they require is the same as the large core from which they are derived, which is smaller than that for four large cores. For example, the bandwidth requirement of four small LSTM models with Lh = 64 is much lower than the one of one big LSTM model with Lh = 1,024. \n\n4 HARDWARE ARCHITECTURE\n\nBased on the optimization techniques introduced above, we implement the proposed Remarn on top of a state-of-the-art (SOTA) RNN accelerator [57] for low-latency cloud-based applications. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. In Section 4.2, we outline the main components of the architecture and detail the necessary hardware modifications required to support our Remarn unit. Several FPGA-specific optimizations are also introduced. \n\n4.1 System Overview\n\nA high-level block diagram of Remarn accelerator is shown in Figures 8(a) and 8(b). The original monolithic accelerator with all the MVM kernels in our previous work [59] has been split into N (e.g., N = 4 in the figure) sub-accelerator cores, each working in a big-little engines architecture. All these N cores are connected via the switch for hidden vectors and partial sum data movement. When N is small as in this case, the switch can be implemented by a crossbar. When N is large, then a network-on-chip could be more effective. Hence, in one extreme, all four cores can be attached together to construct a large logical accelerator, running only one RNN inference using the entire accelerator. Alternatively, it can also provide four stand-alone sub-accelerator cores, spatially co-executing 4 different RNNs simultaneously. Combining with the CGMT of four threads, it can support up to 16 different RNN inferences simultaneously. In this work, all the sub-accelerator cores are identical. But the design can be extended easily to employ a different accelerator hardware architecture, e.g., a systolic array-based one, for some of the cores to support the heterogeneous acceleration of multi-DNN workloads [40]. This work utilizes a general switch as the interconnection between the cores, which brings a slight performance drop when fusing as a large accelerator compared to a monolithic one, since the computational engines now need one more hop to share the partial results. We leave that for future work, since it has a limited impact on the conclusions we draw from our study in this article. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. \n\n4.2 The Big-Little Engines Architecture\n\nThis work proposes a hardware architecture with big-little engines for the DNN accelerators, which is illustrated in Figure 8(a). The proposed accelerator core consists of a big engine, a little engine, an adapter unit, the crossbar as well as weight and vector buffers. The big engine is for the matrix operations that are computation intensive while the little one is for the small but essential components in the neural networks, such as ReLU, pooling, batch normalization, activation, and so on. The big-little engines architecture follows a wait-free single-producer/single-consumer mechanism using the adapter unit, which adjusts the data parallelism between the two engines. In this work, the big engine unit has VP MVM kernels to perform matrix-vector multiplications for LSTM gates operations. It occupies the major computational resources, such as DSPs. Practically, the design will deploy a large number of hardware resources in the big engine to increase the data parallelism to improve the processing ability. However, since the big engine produces one output vector after multiple accumulation cycles, the little engine does not require a large parallelization factor by deploying many LSTM tail units like the one in the big engine unit. The adapter unit can convert the parallelism between the two engines. With a proper adapting factor between the big engine output and little engine input, the little engine can be fully occupied without a stall or wait. The little engine unit has a vector activation function unit and EP tail units that execute the element-wise operations in the LSTM computation. It does not contain many computation resources like the big engine, but it is essential for running the whole neural networks on-chip. Besides, with different hardware components in the little engine, such as ReLU, pooling, batch normalization, and so on, our design can easily be extended to support other types of neural networks, such as CNNs. The Brainwave [19] accelerate and serve CNN models using its MVM engines and custom multi-function units. Sometimes, to accelerate DNN designs, the big engine can be a large systolic array supporting large matrix operations and the little engine utilizes a SIMD vector unit to support general vector operations [24] for the other operations in NNs. The LSTMs are much more challenging to accelerate, exhibiting lower available parallelism and more data dependencies than two-dimensional (2D) CNNs [19]. This design focuses on accelerating RNN/LSTM inferences using a Brainwave-like architecture, but the proposed optimizations and hardware architecture can be easily extended to support other neural networks, which will be our future work. \n\n4.3 Detailed Hardware Components in One Sub-accelerator Core\n\n4.3.1 Overview of the Sub-accelerator Core. Each sub-accelerator core is a full-fledged RNN neural processing unit with coarse-grained multithreading. It has many logical NPUs that share nearly all resources of the physical NPU, e.g., weights buffer, kernel units, activation function units, element-wise units, as shown in Figure 8(a). The core has VP MVM kernels, each having EP PEs, resulting in VP \u00d7 EP available PEs. The VP and EP values are determined via the design space exploration described in Reference [57]. In this article, each PE is one fully pipelined multiplier. The kernels are used to perform the matrix-vector multiplications between the weights and xt as well as ht\u22121 that is required in LSTM gates operations shown in Equations (1). 4.3.2 The MVM Kernel Units. Generally, the row-wise MVM is based on an architecture with parallel multipliers followed by a balanced adder tree. Accumulating the products of these multiplications is usually achieved using a balanced adder tree structure so that a number of related additions can be scheduled in parallel, which minimizes the overall latency of the system. This architecture of the kernel unit is commonplace in FPGA-based designs of RNNs [26, 69]. Since the elements in the partial result vector are not related, we adopt the columnwise MVM [57] that is based on the architecture of parallel multipliers followed by parallel accumulators. Besides, to support element-based parallelism, a small balanced adder tree is placed between the multipliers ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. and the accumulators as shown in Figure 8(c). This adder tree can help to balance EP and VP to improve parallelism. In our proposed multi-threaded NPU core, all the logical cores share all the MVM kernel units. When one logical core (thread) is stalled, the other can still conduct calculations using the same kernel units. Thus, the proposed CGMT NPU core can utilize the resources of MVM kernels efficiently to improve the NPU performance and hardware utilization. Since each thread requires its own input vector xt and ht\u22121, the design has to maintain multiple contexts by using buffers in the thread-aware vector manager and buffers unit. The thread selection logic is necessary to choose the required buffer to retrieve the data and conduct the remaining computations. 4.3.3 The Activation Function Unit. The\u03c3 / tanh unit performs the vector-based sigmoid (\u03c3 ) and hyperbolic tangent (tanh) operations. They are implemented using programmable lookup tables with size of 2,048 similar to References [28, 51]. The implementation using lookup tables brings several benefits. First, our NPU can run a trained model with custom activation functions (e.g., a hard sigmoid from Keras [15]) from our users without re-training of the model. Because the equations of the custom sigmoid/tanh are not changed in the model, re-training is unnecessary. More importantly, we do not touch the sensitive data of users, which is vital for many users. Second, the lookup table has a fixed latency (e.g., one cycle) but other implementations, e.g., a piecewise linear approximation, may involve multiplications that have a much larger pipe stage and latency. 4.3.4 The LSTM Tail Units. Figure 8(d) illustrates the LSTM-tail units that perform the elementwise operations in the Equations (1). The LSTM memory cell FIFOs are employed to temporarily store the status of the running LSTMs, since one thread may be switched due to data hazard before the design finish the calculation of the current LSTM layer with multiple timesteps. Because these threads are performing different LSTM layers or models, the design must keep multiple contexts of cell status of LSTMs, as shown in Figure 8(d). Other hardware contexts, such as the input data and hidden units, are maintained in the thread-aware vector manager and buffer unit with the same mechanism. \n\n4.4 FPGA-Specific Optimizations\n\nSince the proposed accelerator core can process a small tile with a size of (EP ,VP ) in each cycle, all the MVM kernel units in one core share the same input of a sub-vector of (xt ,ht\u22121), which has EP elements. These EP elements are broadcasted to all the MVM kernels for the computation. In addition, the design also needs to broadcast a single address to all the weight buffers units to fetch the weights. To alleviate the issue of large fan-out, the tree-shaped [51] inter-connection is adopted to decrease the fan-out of each node with pipeline stages between the source and destination. The RTL code is carefully written to enable the use of HyperFlex registers on Stratix 10 to get high operating frequency [32, 76]. The DSP blocks in modern FPGAs, which are highly configurable, are often underutilized when implementing 8-bit DNN systems. [84] illustrates methods to extract two INT8 multipliers from Xilinx DSP48E2 Blocks that contain a 27\u00d718 multiplier. Reference [41] introduces a method to pack 2 INT8 multiplications into one INT18 multiplier with extra ALMs. Both the methods require two multiplications to share one input operand. In the columnwise architecture [57], the computation order of MVM is different from the one in row-wise MVM. With the columnwise MVM used in RNN designs, one column of the weights matrix naturally shares the same element of the input vector and conducts the multiplications at the same time. Thus, these multiplications share one input operand, which helps us to pack four INT8 multiplications into one DSP block on Intel FPGAs [41] to decrease the hardware resources. In addition, this would not be a restriction (and will come ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. at a lower cost) if we apply a novel DSP similar to the one that was presented in Reference [7] and will be adopted in the next generation Agilex devices [33]. \n\n5 EXPERIMENTAL RESULTS AND ANALYSIS\n\nThis section introduces the experimental results on an Intel Stratix 10 FPGA that show the advances of the Remarn for RNN/LSTM accelerations. \n\n5.1 Experimental Setup\n\nTable 2 lists some benchmark workloads that are chosen from several typical LSTM applications to make a fair and direct comparison of our design with others. Multi-tasked LSTM workloads are constructed randomly from these LSTM workloads. We evaluate the Remarn on an Intel Stratix 10 2800 (S10) and compare the results with other work. The Remarn runs the inferences of persistent LSTM models that store the weights in on-chip memory [19, 51, 57, 59, 66\u201368]. Remarn is designed using Verilog RTL. And Quartus Prime Pro 19.4 is used for the compilation of FPGA designs. The choice of the number of sub-core is based on the size of the benchmark LSTM workloads and the accelerator hardware architecture. The value of EP , explored in Reference [57], is set to 16, the same size as the previous designs [57, 59]. The value of VP should be chosen to be as large as possible. However, the largest effective value of VP is 4Lh as discussed in Section 3.2. Hence, the VP = 256 in this work, since the Lh = 64 for the smallest LSTM benchmark as shown in Table 2, resulting in 256\u00d716 = 4, 096 effective PEs for each sub-accelerator core. To make a fair comparison, the number of sub-accelerator cores, N , is set to 4 so that Remarn will have 16,386 PEs, which is the same as the number of total PEs in our previous designs [57, 59]. \n\n5.2 Resource Utilization\n\nTable 3 shows the resource utilization of our designs with two configurations on FPGAs. The first design is a baseline design that is a single-threaded design without partition. It utilizes the parameter of (EP ,VP ) as (16, 1024), which includes 16,384 8-bit multipliers in the MVM kernels implemented using DSP blocks with extra ALMs. The second design is a dual-threaded quadcore Remarn that has four sub-accelerator cores. Each core utilizes the parameter of (EP ,VP ) as (16, 256). Thus, it has the same number of multipliers as the baseline design. The dual-threaded quad-core design consumes 5.0% more of total logic (ALMs) resources and 0.8% more block ram (M20K) than the baseline design. Since dual threads share the same physical core it consumes the same DSP resources as the single-threaded one. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. \n\n5.3 Performance and Efficiency Comparison with CPUs and GPUs\n\nTo compare the performance of the proposed design on FPGA with other platforms, the DeepBench published results [89] on an Intel Xeon Syklake CPU and NVIDIA Tesla V100 GPU are used for comparison. TensorFlow is used for both CPU and GPU. Besides, the CPU with AVX2 Vector instructions enabled is utilized while the CuDNN libraries are enabled for the GPU. cuDNN is a GPU-accelerator Library from NVIDIA, which is specialized for deep learning. Both CPU and GPU implementations run with a batch size of 1, which provides the lowest service latency of cloud, since requests are required to be processed as soon as they arrive. For a fair comparison with the throughput of our dual-threaded Remarn, the throughput of the CPU and GPU have been doubled in Table 4. The GPU is significantly underutilized even when cuDNN library API calls, since it is designed for throughput-oriented workloads. It prefers BLAS level-3 (matrix-matrix) operations that are not common in RNN computations [89]. Our FPGA design of dual-threaded Remarn achieves 6.5 times higher performance and 15.6 times higher power efficiency, respectively, than the one running on the Tesla V100 GPU as shown in Table 4. \n\n5.4 Performance and Efficiency Comparison with the Baseline\n\nTo show the benefits of the proposed approaches, we compare the multi-threaded multi-core Remarn with the baseline design [57] in Figures 9 and 10. The baseline is a single-threaded singlecore accelerator. Hardware utilization is the percentage of the peak Tera Operations Per Second (TOPS) achieved for each LSTM, as compared with the peak TOPS of the design with all the PEs. Figure 9 shows the speedup of the quad-core Remarn over the baseline design [57]. With the proposed approaches of CGMT and multi-core, the quad-core Remarn designs of a single thread (1-T-Q), dual threads (2-T-Q) and quad threads (4-T-Q) achieve 1.82, 2.91, and 3.16 times higher performance respectively than the baseline when targeting mixed requests from LSTM workloads with h = 64, 128, and 256. Please note that Remarn has the same number of PEs as the baseline, and they consume the same DSP blocks on FPGAs. Particularly, with four threads (4-T-Q), Remarn achieves 14.44 times higher performance than the baseline [57] when targeting the small LSTMs (Lh = 64). When only targeting large LSTMs, the performance gain of the quad-core Remarn is small, since the baseline design [57] already achieves high hardware utilization for these LSTMs. However, the baseline still suffers from low utilization when targeting small-sized LSTMs that are commonly used in many applications [17, 55]. LSTM models that have a large number of timesteps but use small sizes are the most tangible examples that require dealing with lots of dependencies, as well as the parallel task of MVMs [86]. Our proposed approach and hardware architecture can alleviate ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. this problem by leveraging the coarse-grained multithreading combining the spatial co-execution using multiple sub-accelerator cores. With the demand for high-performance designs, it is vital to maximize the hardware utilization to attain the best possible effective performance and power efficiency. When compared to our previous work [59] with only multithreading, Remarn can attain much higher hardware utilization and performance. Remarn achieves higher than 50% utilization for all the LSTM cases with Lh = 128 as shown in Figure 10(b) while the previous work can only achieve up to 36.4%. Particularly, with dual threads, the previous work achieves the utilization lower than 25% for LSTMs with Lh = 128; however, the proposed Remarn can achieve higher than 75%. Compared to our previous work [59], the Remarn proposed in this work achieves up to 3.61 times higher performance when targeting the small LSTMs, as shown in Figure 11, which ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. shows the advances of the proposed architecture using multiple sub-accelerator cores. There is a small reduction from a linear speedup for the model size of 64, which is due to control logic added to each small cores when splitting the big engine. The impact is negligible when the workload is large with a long runtime. We believe this can be optimized with more efforts but doing so has little impact on the conclusions we draw from this study. The design with multithreading alone shows a steady increase from 1-T to 2-T and to 4-T [59] for the model size of 128. However, the additional speedups for the quad-core design dropped when it is from 2-T to 4-T threads as shown in Figure 11. It is because with 2-T on small cores, the design can already remove most of the stalls caused by the RNN data dependencies for the model size of 128. Hence, the quad-core design with 2-T and 4-T achieves a similar performance and runtime hardware utilization as shown in Figure 10(b). But with 2-T on the big core, the design still suffers from some stalls so the performance of 4-T on the big core is much better than 2-T. Hence, the additional speedups drop when going from 2-T to 4-T threads on the quad-core design. Moreover, when compared to References [57, 59], which has a single large core, the quad-core Remarn has a slight performance drop (less than 2%) when all 4 cores are working together as a big accelerator to run a large workload, as shown in Tables 4 and 5. It is because the multi-core design can lead to synchronization costs than the one using a monolithic accelerator. When four cores are running the same large workload, a synchronization mechanism is required to maintain the correct data dependencies in RNNs. It is a design tradeoff. With slight performance loss on the large workloads, the design can achieve much higher performance on the small workloads, which can result in a much better performance gain for all the workloads. Besides, the users can re-configure the FPGA into a big and monolithic accelerator when most of large RNN workloads are large, which can achieve the best performance. The evaluation results also show that Remarn gets a low utilization when running small LSTMs (h = 64), as shown in Figure 10(b). Even with four threads, the design still stalls, because the cycles of processing MVMs in the other threads still cannot completely hide the whole pipeline latency to get the ht available before it is required for the next timestep. The number of cycles to process the MVM of the LSTM gates is given by Lx+Lh EP . Different choices of (EP , VP ) impact the performance and utilization of the designs that run LSTMs with different sizes. There is a tradeoff between the extra performance that can be achieved and the hardware design complexity as well as extra hardware resources of supporting various sets of (EP ,VP ) at runtime. It is our future work to enhance this accelerator architecture to support various sets of (EP ,VP ), since different sizes of LSTMs may prefer different optimal EP and VP parameters. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. \n\n5.5 Performance and Efficiency Comparison with Other FPGA-based Designs\n\nTable 5 shows the comparison between our Remarn and the existing LSTM accelerator implementations using a Stratix 10 GX 2800 FPGA. For a fair comparison, we only show the previous work with a detailed implementation of the LSTM system using the same FPGA. We list the model storage, precision, DSP used, runtime frequency, power, throughput and power efficiency as well as the hardware (HW) utilization. The thermal design power (TDP) that is 125 W is utilized for a fair comparison, since it is reported in all the listed work. Overall, Remarn provides over 4.87 times higher performance and 5.71 times higher hardware utilization than the SOTA design [51] when targeting LSTM models with ht =256, as shown in Table 5. Compared with the baseline that is a single-threaded single-core design [57], Remarn achieves 1.45 to 14.4 higher throughput and HW utilization. Since multithreading also ensures there is no downside to peak performance in a single-threaded mode, the Remarn can achieve a similar peak performance with the one in Reference [57], which is the highest with respect to the SOTA FPGA-based RNN/LSTM designs with a commonly used INT8 precision. The only prior work that has a higher peak performance is Reference [19] that employs an 8-bit block floating-point representation. However, when targeting the small LSTM models, the throughput of the proposed Remarn is 18.82 times higher than [19] as shown in Table 5. Moreover, Remarn achieves the highest hardware utilization among all the listed designs across various LSTM models. This work focuses on maximizing throughput and minimizing latency by increasing HW utilization. \n\n5.6 Scheduling\n\nThe scheduling strategy for multi-core accelerators for DNN workloads is a new and rarely explored topic. PREMA [14] introduces a preemptive scheduling algorithm on a single-core NPU to support multi-DNN execution. AI-MultiTasking [4] proposes to process different neural networks by partitioning their various layers into identical sub-layers in compile time. It also develops a heuristic of mapping DNN jobs to the accelerator. In Reference [40], Heterogeneous Dataflow accelerators (HDAs) are proposed to support the heterogeneous acceleration of multiDNN workloads using manual-designed mapping of jobs to sub-accelerators and compile time layerwise scheduling. LayerWeaver [53] introduces a layerwise time-multiplexing scheduler on single NPU by interweaving layer execution to increase the hardware utilization. Besides, MAGMA [36] proposes a novel framework for mapping multiple DNNs on multiple accelerator cores. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. The focus of this article is on the accelerator architecture, and on examples of optimal utilization of that architecture based on fine-grained scheduling of tasks made up of LSTM sub-layers running on sub-cores. We exploit the intra-model data dependencies of both timesteps and layers in LSTMs, as shown in Figure 7(a). Both time step and layer dependencies have a linear dependence chain in the model. For LSTM inference, it is processed from the first layer to the last layer sequentially and from the first timestep to the last timestep in each layer. Besides, layers from different LSTMs are independent. Inspired by References [4, 40], which rely on manually designed heuristics, this work develops a set of heuristics that exploit the characteristics of LSTM inference workloads to reduce the scheduling overhead for the proposed homogeneous multi-core accelerator. We do not include algorithmic details (e.g., flowchart or pseudocode) about scheduling of these sub-layers tasks, other than mentioning related research on scheduling strategies [4, 14, 40] and LayerWeaver [53] as well as the custom mapping algorithm MAGMA [36]. Architectural tradeoffs of these scheduling strategies, based on queuing theory and other techniques, are important and would be best addressed by future work. Besides, further research will cover fine-grained dynamic scheduling for our accelerator, such as using Gang scheduling that schedules a gang of jobs onto free cores. Furthermore, other sophisticated scheduling strategies relevant to our approach will be studied and tested. \n\n5.7 Power Consumption and Optimizations\n\nThe proposed extension of the multi-core feature is based on splitting a large monolithic accelerator into several sub-accelerator cores, which does not change the total number of existing PEs in hardware. There is only a small bump in the hardware resources for the extended accelerator because of supporting new features, which may cause a little more power/energy consumption. However, the extra power consumption caused by extra FPGA hardware resources is small, since the new design only has 5% more area than the baseline [57] but can get 1.45 to 14.4 times higher performance, resulting in high power efficiency. Besides, the architectural extensions lead to higher runtime dynamic power, since these extensions provide more parallelism and higher hardware utilization. In general, the performance and power are positively correlated; the more accelerators used, the higher the power consumption. However, higher utilization also means that our architecture wastes less power on the underutilized execution resources than the baseline [57], which increases the overall power/energy efficiency. The overall increase in utilization also means the proposed design completes the same work in a shorter period of time, resulting in a potential lower energy consumption. Besides, with multiple sub-accelerator cores, the design has the potential to totally turn off some of the cores when there is only a small amount of workload to save power, which is not possible in the baseline where only a big and monolithic core is used. In future work, we will build an architecture-level power model targeting multi-threaded multi-core DNN accelerators so that we can quantify the impact of these architectural extensions on power/energy consumption. There are some potential optimizations for the power consumption targeting our architecture. Since the proposed design achieves a high hardware utilization, it may not require so many PEs to reach a design performance goal. Reducing the number of total PEs or reducing the hardware resources in design time can help to reduce the total power consumption. However, the design causes large dynamic power consumption because of the high utilization. Hence, one possible optimization is to control dynamic power consumption by gracefully degrading the core activities when a power consumption threshold is exceeded. It can be done by a feedback of the current power consumption of the cores, which may be estimated from some performance counters or read from some on-die sensors. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. \n\n6 RELATED WORK\n\nThere are many previous studies about FPGA-based accelerations of persistent RNN/LSTM with weights stored in on-chip memory [19, 51, 58, 66\u201369]. Rybalkin et al. [69] introduces the hardware architecture of BiLSTM targeting OCR applications. All the weights and activations are quantized to 5-bit and stored in on-chip memory. [66] quantizes the design using 1-8 bits and achieves an accuracy that surpasses the one using floating-point on a given dataset. Besides, their later work [67, 68] proposes a novel hardware architecture of 2D-LSTM and investigates the tradeoff between precision and performance. There are also many previous studies about LSTM implementations with weights stored in off-chip memory on FPGAs [10, 26, 27, 54, 61], which has been recognized as the performance bottleneck. In LSTM computations, a weights matrix tends to be repeatedly loaded from off-chip memory if the size of on-chip memory is not large enough to store the entire matrix, which brings large latency. Guan et al. [27] proposes a smart memory organization with on-chip double buffers to overlap computations with data transfers. And References [22, 72] apply the batching technique to increase the throughput of LSTM inferences. For example, E-BATCH is proposed [72] for RNNs, which increases throughput while also improving energy efficiency on an ASIC-based accelerator. Apart from batching, References [54, 61, 63] introduce novel LSTM weights reuse schemes that utilizes the weights sharing characteristics in different timestep computations in one inference. These schemes reduce the access of the off-chip memory and decrease the energy cost as well as improve the design throughput. Some of the previous work [42, 80, 81, 85] adopts circulant matrix to optimize LSTMs by reducing the weight matrices of LSTM inferences. Besides, an approximate computing scheme is deployed for LSTMs using small tiles [65]. And stochastic computing is used to improve the energy efficiency of the RNN inference [43]. Reference [37] proposes an LSTM architecture based on reversible logic gates for low power circuit designs. POLAR [5] and BRDS [23] present FPGA-based pipelined and overlapped architecture for dense and sparse LSTM inferences, respectively. A multiFPGA approach [74] is proposed to accelerate multi-layer RNNs. It achieves a single-layer latency targeting deep RNNs with arbitrarily multiple layers using an FPGA-based cluster. Reference [49] presents a multi-FPGA-based architecture to accelerate neural machine translation. Reference [39] explores various partitioning strategies of large RNN inferences to achieve scalable multi-FPGA acceleration. It also analyses the performance impact of software pipelining and collective communications. PDL-FGPU [44], a specialized FPGA-based GPU overlay architecture is proposed to run persistent Deep Learning, including various RNN variants. Initiation interval (II) balancing [62] is proposed with a layerwise implementation of LSTMs to achieve ultra low latency. The layerwise implementation of RNNs is also used in accelerating Bayesian RNNs [18]. There is also much previous work [9, 11, 13, 20, 21, 28, 34, 48, 70, 83, 90] exploit the sparsity of data and weights with pruning to reduce the NN computation and also the memory footprint to achieve high performance and efficiencies. ESE [28] proposes a pruning technique that compresses a large LSTM model by 20\u00d7 without sacrificing the prediction accuracy. DeltaRNN [21] utilizes the Delta Network algorithm to reduce MxV operations and corresponding weight fetches by skipping dispensable computations during inference of RNNs. It updates the output of a neuron only when the neuron\u2019s activation changes by more than a delta threshold. Bank-Balanced Sparsity [9] is proposed to achieve both high prediction accuracy of a pruned LSTM model and high hardware efficiency of the model running on FPGAs. It partitions weight matrix rows into banks for parallel computing and adopts fine-grained pruning inside each bank to maintain model accuracy. BLINK [11] designs the LSTM inference using a bit-sparse data representation. And it turns multiplications into bit-shift operation to improve the energy efficiency while maintaining the LSTM inference accuracy for real-time calcium image processing. The extension [12] proposes a ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. combination of the bit-sparse quantization and the pruning methods for energy-efficient LSTM inferences. More recently, Spartus [20] exploits spatio-temporal sparsity to achieve ultra-low-latency inference using Column-Balanced Targeted Dropout. These studies are orthogonal to our proposed approach and hardware architecture. These techniques can be complementary to our approaches to achieve even higher performance and efficiency of RNN/LSTM inferences using FPGAs. The Brainwave design [19] is a single-threaded SIMD architecture for running real-time AI, including persistent RNNs. It achieves more than an order of magnitude improvement in latency and throughput over state-of-the-art GPUs on large memory intensive RNN models at a batch size of 1. It stores NN model weights on-chip for RNNs to get a necessary high memory read bandwidth to achieve higher performances. A coarse-grained reconfigurable architecture (CGRA)-based RNN accelerator is proposed [89] on top of Plasticine [56] with a set of techniques for performing cross-kernel optimization in RNN cells. AERO [38] is a single-threaded instruction-set-based processor using a versatile vector-processing unit customized for RNN inferences on resourcelimited FPGAs. A Brainwave-like NPU is proposed in Reference [51] with a single-threaded architecture. They also explore the potential of combining a TensorRAM with FPGAs to provide large high-speed memory for large memory intensive RNN sequence models. Besides, their late work [8] deploys the Brainwave-like NPU on the Stratix 10 NX that is Intel\u2019s new AI-optimized FPGA featured with AI tensor blocks. [57, 60] proposes a novel latency-hiding hardware architecture based on columnwise MVM and fine-grained tiling strategy to eliminate data dependency of RNNs, which improves the design throughput of RNNs/LSTMs. However, all these NPUs targeting RNNs/LSTMs are single threaded. There is a large demand for architectural support of multi-DNN execution to maximize hardware utilization and reduce the costs of a large-scale production system. For example, TensorRT from Nvidia supports concurrent DNN execution for users to run multi-DNN workloads on the same GPU simultaneously. SMT-SA [71] presents a simultaneous multithreading approach for systolic arrays to solve the issue of underutilization because of zero-valued inputs. However, it is not able to deal with the underutilization issue that comes from the data dependencies in RNNs/LSTMs. A multi-threaded CGRA [3] has been proposed, but it is only for CNN accelerations. Reference [50] presents to generate in-order multi-threaded processing pipelined datapath automatically with the high-level specification of an unpipelined datapath. CUSTARD [16] presents a customisable multi-threaded FPGA soft processor with features including design space exploration and a compiler for automatic selection of custom instructions. More recently, PREMA [14] proposes a preemptible NPU with a preemptive scheduling algorithm to support multi-DNN execution. However, it does not consider the data dependencies between RNN/LSTM timesteps. AIMultiTasking [4] proposes to balance memory-intensive and compute-intensive tasks from different neural networks and process them in parallel by partitioning various layers into lots of identical sub-layers. The RNNs are handled just as FC layers in the scheduling scheme. However, RNNs are more complex than FC layers that have no data dependencies. In Reference [55], a dualcore accelerator for LSTM-RNN is proposed to execute multiple jobs simultaneously or have cores collaborate on a single job. However, they perform multithreading by utilizing the dual cores with one thread on one core, which is not an area / cost efficient way to add extra threads. Besides, there is some previous work targeting spatial multi-tenant execution. f-CNNx [78] employs multiple custom dedicated engines for the workloads with various CNN models. Reference [64] maps multiple LSTM models to an FPGA device by introducing a framework that alters their computational structure, opening opportunities for co-optimizing the memory requirements to the target architecture via applying compression schemes across multiple LSTM models. In Reference [40] HDAs are proposed to support the heterogeneous acceleration of multi-DNN workloads. More recently, Planaria [24] introduces the dynamical fission of a systolic-array-based DNN accelerator ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. at runtime to support spatial co-execution of multiple DNN inferences. However, none of them targets RNNs, and they do not consider the recurrent nature and data dependency in RNN computations. This work designs a multi-threaded accelerator with fission to enable spatial and temporal co-execution of multiple RNN/LSTM inferences on the same hardware and offers simultaneous multi-RNN accelerations. It can handle multiple requests using multi-threaded mode with multiple sub-accelerator cores while still being able to run a task in a single-threaded mode with a large accelerator to attain higher performance and lower latency when targeting the workloads with high priority. \n\n7 CONCLUSIONS AND FUTURE WORK\n\nThis work proposes Remarn, a multi-threaded multi-core NPU supporting spatial and temporal coexecution of RNN/LSTM inferences to improve the processing abilities of cloud-based NPUs. The Remarn can increase the throughput while improving the hardware utilization of the cloud-based NPUs. We have implemented the proposed multi-threaded multi-core accelerator architecture on Intel Stratix 10 FPGAs with superior performance and efficiency, which demonstrates the effectiveness of our approaches. Our study shows that both multithreading and multi sub-accelerator core techniques can address the underutilization issue (resource inefficiency) when targeting small sized RNN workloads. Besides, it also shows that multi sub-accelerator core is slightly better than multithreading for small workloads. The design of quad-core with single-thread (1-T-Q) achieves a better utilization than the design of single-core quad-thread (4-T) for the LSTM of Lh = 128, as shown in Figure 10. But this figure also shows that the single-core quad-thread (4-T) design gets a better performance for the LSTM of Lh = 256. In addition, both techniques do not bring benefits to large LSTM workloads, since the baseline design has achieved high utilization. Moreover, this study shows that multithreading targets temporal co-execution, while multi-core targets spatial co-execution, and the two techniques can be combined to achieve a much better performance. Further research includes exploring how Remarn can benefit from other enhancements such as dynamic scheduling and runtime power analysing, studying the tradeoffs of varying the number and the heterogeneity of Remarn cores, and automating the proposed approaches to enable rapid development of efficient Remarn designs for datacentres as well as edge processing and embedded systems. \n\nREFERENCES\n\n[1] Mohamed S. Abdelfattah, David Han, Andrew Bitar, Roberto DiCecco, Shane O\u2019Connell, Nitika Shanker, Joseph Chu, Ian Prins, Joshua Fender, Andrew C. Ling, et al. 2018. DLA: Compiler and FPGA overlay for neural network inference acceleration. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 411\u20134117. [2] Dario Amodei et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In Proceedings of the International Conference on Machine Learning. [3] Kota Ando, Shinya Takamaeda-Yamazaki, Masayuki Ikebe, Tetsuya Asai, and Masato Motomura. 2017. A multi- threaded CGRA for convolutional neural network processing. Circ. Syst. 8, 6 (2017), 149\u2013170. [4] Eunjin Baek, Dongup Kwon, and Jangwoo Kim. 2020. A multi-neural network acceleration architecture. In Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA\u201920). IEEE, 940\u2013953. [5] Erfan Bank-Tavakoli, Seyed Abolfazl Ghasemzadeh, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2019. Polar: A pipelined/overlapped fpga-based lstm accelerator. IEEE Trans. VLSI Syst. 28, 3 (2019), 838\u2013842. [6] John M. Borkenhagen et al. 2000. A multithreaded PowerPC processor for commercial servers. IBM J. Res. Dev. 44, 6 (2000), 885\u2013898. [7] Andrew Boutros et al. 2018. Embracing diversity: Enhanced DSP blocks for low-precision deep learning on FPGAs. In Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE. [8] Andrew Boutros, Eriko Nurvitadhi, Rui Ma, Sergey Gribok, Zhipeng Zhao, James C. Hoe, Vaughn Betz, and Martin Langhammer. 2020. Beyond peak performance: Comparing the real performance of AI-optimized FPGAs and GPUs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 10\u201319. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [9] Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun Nie, Dechen Zhan, Yunxin Liu, Ming Wu, and Lintao Zhang. 2019. Efficient and effective sparse LSTM on FPGA with bank-balanced sparsity. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 63\u201372. [10] Andre Xian Ming Chang, Berin Martini, and Eugenio Culurciello. 2015. Recurrent neural networks hardware imple- mentation on FPGA. arXiv:1511.05552. Retrieved from https://arxiv.org/abs/1511.05552. [11] Zhe Chen, Garrett J. Blair, Hugh T. Blair, and Jason Cong. 2020. BLINK: Bit-sparse LSTM inference kernel enabling efficient calcium trace extraction for neurofeedback devices. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 217\u2013222. [12] Zhe Chen, Hugh T. Blair, and Jason Cong. 2022. Energy efficient LSTM inference accelerator for real-time causal prediction. ACM Trans. Des. Autom. Electr. Syst. 27, 5, Article 44 (September 2022), 19 pages. https://doi.org/10.1145/ 349500 [13] Zhe Chen, Andrew Howe, Hugh T. Blair, and Jason Cong. 2018. CLINK: Compact LSTM inference kernel for energy efficient neurofeedback devices. In Proceedings of the International Symposium on Low Power Electronics and Design. 1\u20136. [14] Yujeong Choi and Minsoo Rhu. 2020. PREMA: A predictive multi-task scheduling algorithm for preemptible neu- ral processing units. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201920). IEEE, 220\u2013233. [15] Fran\u00e7ois Chollet et al. 2015. Keras: Deep Learning Library for theano and tensorflow. https://keras.io/k. [16] R. Dimond, O. Mencer, and W. Luk. 2006. Application-specific customisation of multi-threaded soft processors. IEE Proc. Comput. Digit. Techn. 153, 3 (2006), 173\u2013180. [17] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2625\u20132634. [18] Martin Ferianc, Zhiqiang Que, Hongxiang Fan, Wayne Luk, and Miguel Rodrigues. 2021. Optimizing Bayesian re- current neural networks on an FPGA-based accelerator. In Proceedings of the International Conference on FieldProgrammable Technology (ICFPT\u201921). IEEE, 1\u201310. [19] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. 2018. A configurable cloud-scale DNN processor for real-time AI. In Proceedings of the 45th Annual International Symposium on Computer Architecture. IEEE Press, 1\u201314. [20] Chang Gao, Tobi Delbruck, and Shih-Chii Liu. 2021. Spartus: A 9.4 TOp/s FPGA-based LSTM accelerator exploiting spatio-temporal sparsity. IEEE Transactions on Neural Networks and Learning Systems. (Early Access) [21] Chang Gao, Daniel Neil, Enea Ceolini, Shih-Chii Liu, and Tobi Delbruck. 2018. DeltaRNN: A power-efficient recurrent neural network accelerator. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 21\u201330. [22] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency RNN inference with cellular batching. In Pro- ceedings of the 13th European Conference on Computer Systems Conference (EuroSys\u201918). 1\u201315. [23] Seyed Abolfazl Ghasemzadeh, Erfan Bank Tavakoli, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2021. BRDS: An FPGA-based LSTM accelerator with row-balanced dual-ratio sparsification. arXiv:2101.02667. Retrieved from https: //arxiv.org/abs/2101.02667. [24] Soroush Ghodrati, Byung Hoon Ahn, Joon Kyung Kim, Sean Kinzer, Brahmendra Reddy Yatham, Navateja Alla, Hardik Sharma, Mohammad Alian, Eiman Ebrahimi, Nam Sung Kim, et al. 2020. Planaria: Dynamic architecture fission for spatial multi-tenant acceleration of deep neural networks. In Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO\u201920). IEEE, 681\u2013697. [25] Yoav Goldberg. 2016. A primer on neural network models for natural language processing. J. Artif. Intell. Res. 57 (2016), 345\u2013420. [26] Yijin Guan, Hao Liang, Ningyi Xu, Wenqiang Wang, Shaoshuai Shi, Xi Chen, Guangyu Sun, Wei Zhang, and Jason Cong. 2017. FP-DNN: An automated framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates. In Proceedings of the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201917). IEEE, 152\u2013159. [27] Yijin Guan, Zhihang Yuan, Guangyu Sun, and Jason Cong. 2017. FPGA-based accelerator for long short-term memory recurrent neural networks. In Proceedings of the 22nd Asia and South Pacific Design Automation Conference (ASPDAC\u201917). IEEE, 629\u2013634. [28] Song Han, Junlong Kang, Huizi Mao, Yiming Hu, Xin Li, Yubin Li, Dongliang Xie, Hong Luo, Song Yao, Yu Wang, et al. 2017. ESE: Efficient speech recognition engine with sparse LSTM on FPGA. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 75\u201384. [29] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv:1412.5567. Retrieved from https://arxiv.org/abs/1412.5567. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [30] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Comput. 9, 8 (1997), 1735\u20131780. [31] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. 2018. Detecting spacecraft anomalies using LSTM and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 387\u2013395. [32] Intel. [n.d.]. Understanding How Hyperflex Architecture Enables High Performance Systems. White Paper 01231. [33] Intel. 2020. Intel Agilex Variable Precision DSP Blocks User Guide. [34] Jingfei Jiang, Tao Xiao, Jinwei Xu, Dong Wen, Lei Gao, and Yong Dou. 2022. A low-latency LSTM accelerator using balanced sparsity based on FPGA. Microprocess. Microsyst. 89 (2022), 104417. [35] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture. 1\u201312. [36] Sheng-Chun Kao and Tushar Krishna. 2022. MAGMA: An optimization framework for mapping multiple DNNs on multiple accelerator cores. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201922). IEEE. [37] Kasem Khalil, Bappaditya Dey, Ashok Kumar, and Magdy Bayoumi. 2021. A reversible-logic based architecture for long short-term memory (LSTM) network. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201921). IEEE, 1\u20135. [38] Jinwon Kim, Jiho Kim, and Tae-Hwan Kim. 2021. AERO: A 1.28 MOP/s/LUT reconfigurable inference processor for recurrent neural networks in a resource-limited FPGA. Electronics 10, 11 (2021), 1249. [39] Dongup Kwon, Suyeon Hur, Hamin Jang, Eriko Nurvitadhi, and Jangwoo Kim. 2020. Scalable multi-FPGA accelera- tion for large RNNs with full parallelism levels. In Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC\u201920). IEEE, 1\u20136. [40] Hyoukjun Kwon, Liangzhen Lai, Michael Pellauer, Tushar Krishna, Yu-Hsin Chen, and Vikas Chandra. 2021. Het- erogeneous dataflow accelerators for multi-DNN workloads. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 71\u201383. [41] Martin Langhammer, Bogdan Pasca, Gregg Baeckler, and Sergey Gribok. 2019. Extracting INT8 multipliers from INT18 multipliers. In Proceedings of the International Conference on Field Programmable Logic and Applications (FPL\u201919). IEEE. [42] Zhe Li, Caiwen Ding, Siyue Wang, Wujie Wen, Youwei Zhuo, Chang Liu, Qinru Qiu, Wenyao Xu, Xue Lin, Xuehai Qian, et al. 2019. E-RNN: Design optimization for efficient recurrent neural networks in FPGAs. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA\u201919). IEEE, 69\u201380. [43] Yidong Liu, Leibo Liu, Fabrizio Lombardi, and Jie Han. 2019. An energy-efficient and noise-tolerant recurrent neural network using stochastic computing. IEEE Trans. VLSI Syst. 27, 9 (2019), 2213\u20132221. [44] Rui Ma, Jia-Ching Hsu, Tian Tan, Eriko Nurvitadhi, David Sheffield, Rob Pelt, Martin Langhammer, Jaewoong Sim, Aravind Dasu, and Derek Chiou. 2021. Specializing FGPU for persistent deep learning. ACM Trans. Reconfig. Technol. Syst. 14, 2 (2021), 1\u201323. [45] Andrew Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 142\u2013150. [46] Cameron McNairy and Rohit Bhatia. 2005. Montecito: A dual-core, dual-thread itanium processor. IEEE Micro 25, 2 (2020), 10\u201320. [47] Sparsh Mittal and Sumanth Umesh. 2021. A survey on hardware accelerators and optimization techniques for RNNs. J. Syst. Arch. 112 (2021), 101839. [48] Guocai Nan, Chenghua Wang, Weiqiang Liu, and Fabrizio Lombardi. 2020. DC-LSTM: Deep compressed LSTM with low bit-width and structured matrices. In Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS\u201920). IEEE, 1\u20135. [49] Eriko Nurvitadhi, Andrew Boutros, Prerna Budhkar, Ali Jafari, Dongup Kwon, David Sheffield, Abirami Prabhakaran, Karthik Gururaj, Pranavi Appana, and Mishali Naik. 2019. Scalable low-latency persistent neural machine translation on CPU server with multiple FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 307\u2013310. [50] Eriko Nurvitadhi, James C. Hoe, Shih-Lien L. Lu, and Timothy Kam. 2010. Automatic multithreaded pipeline synthesis from transactional datapath specifications. In Proceedings of the Design Automation Conference. IEEE, 314\u2013319. [51] Eriko Nurvitadhi, Dongup Kwon, Ali Jafari, Andrew Boutros, Jaewoong Sim, Phillip Tomson, Huseyin Sumbul, Gregory Chen, Phil Knag, Raghavan Kumar, et al. 2019. Why compete when you can work together: Fpga-asic integration for persistent rnns. In Proceedings of the IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201919). IEEE, 199\u2013207. [52] Eriko Nurvitadhi, Jaewoong Sim, David Sheffield, Asit Mishra, Srivatsan Krishnan, and Debbie Marr. 2016. Accelerat- ing recurrent neural networks in analytics servers: Comparison of FPGA, CPU, GPU, and ASIC. In Proceedings of the 26th International Conference on Field Programmable Logic and Applications (FPL\u201916). IEEE, 1\u20134. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [53] Young H. Oh, Seonghak Kim, Yunho Jin, Sam Son, Jonghyun Bae, Jongsung Lee, Yeonhong Park, Dong Uk Kim, Tae Jun Ham, and Jae W. Lee. 2021. Layerweaver: Maximizing resource utilization of neural processing units via layer-wise scheduling. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA\u201921). IEEE, 584\u2013597. [54] Naebeom Park, Yulhwa Kim, Daehyun Ahn, Taesu Kim, and Jae-Joon Kim. 2020. Time-step interleaved weight reuse for LSTM neural network computing. In Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design. 13\u201318. [55] Lu Peng, Wentao Shi, Jian Zhang, and Samuel Irving. 2019. Exploiting model-level parallelism in recurrent neural net- work accelerators. In Proceedings of the IEEE 13th International Symposium on Embedded Multicore/Many-core Systemson-Chip (MCSoC\u201919). IEEE, 241\u2013248. [56] Raghu Prabhakar, Yaqi Zhang, David Koeplinger, Matt Feldman, Tian Zhao, Stefan Hadjis, Ardavan Pedram, Christos Kozyrakis, and Kunle Olukotun. 2017. Plasticine: A reconfigurable architecture for parallel patterns. In Proceedings of the ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA\u201917). IEEE, 389\u2013402. [57] Zhiqiang Que et al. 2020. Optimizing reconfigurable recurrent neural networks. In Proceedings of the IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM\u201920). IEEE. [58] Zhiqiang Que, Yanyang Liu, Ce Guo, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Real-time anomaly detection for flight testing using AutoEncoder and LSTM. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201919). IEEE, 379\u2013382. [59] Zhiqiang Que, Hiroki Nakahara, Hongxiang Fan, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, Eriko Nurvitadhi, and Wayne Luk. 2020. A reconfigurable multithreaded accelerator for recurrent neural networks. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 20\u201328. [60] Zhiqiang Que, Hiroki Nakahara, Eriko Nurvitadhi, Andrew Boutros, Hongxiang Fan, Chenglong Zeng, Jiuxi Meng, Kuen Hung Tsoi, Xinyu Niu, and Wayne Luk. 2022. Recurrent neural networks with column-wise matrix-vector multiplication on FPGAs. IEEE Trans. VLSI Syst. (2022). [61] Zhiqiang Que, Thomas Nugent, Shuanglong Liu, Li Tian, Xinyu Niu, Yongxin Zhu, and Wayne Luk. 2019. Efficient weight reuse for large LSTMs. In Proceedings of the IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201919), Vol. 2160. IEEE, 17\u201324. [62] Zhiqiang Que, Erwei Wang, Umar Marikar, Eric Moreno, Jennifer Ngadiuba, Hamza Javed, Bart\u0142omiej Borzyszkowski, Thea Aarrestad, Vladimir Loncar, Sioni Summers, Maurizio Pierini, Peter Y. Cheung, and Wayne Luk. 2021. Accelerating recurrent neural networks for gravitational wave experiments. In Proceedings of the 32th International Conference on Application-specific Systems, Architectures and Processors (ASAP\u201921). IEEE. [63] Zhiqiang Que, Yongxin Zhu, Hongxiang Fan, Jiuxi Meng, Xinyu Niu, and Wayne Luk. 2020. Mapping large LSTMs to FPGAs with weight reuse. J. Sign. Process. Syst. 92, 9 (2020), 965\u2013979. [64] Stefano Ribes, Pedro Trancoso, Ioannis Sourdis, and Christos-Savvas Bouganis. 2020. Mapping multiple LSTM models on FPGAs. In Proceedings of the International Conference on Field-Programmable Technology (ICFPT\u201920). IEEE, 1\u20139. [65] Michalis Rizakis, Stylianos I. Venieris, Alexandros Kouris, and Christos-Savvas Bouganis. 2018. Approximate FPGA- based LSTMs under computation time constraints. In Proceedings of the International Symposium on Applied Reconfigurable Computing. Springer, 3\u201315. [66] Vladimir Rybalkin, Alessandro Pappalardo, Muhammad Mohsin Ghaffar, Giulio Gambardella, Norbert Wehn, and Michaela Blott. 2018. FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE. [67] Vladimir Rybalkin, Chirag Sudarshan, Christian Weis, Jan Lappas, Norbert Wehn, and Li Cheng. 2020. Efficient hard- ware architectures for 1D-and MD-LSTM networks. J. Sign. Process. Syst. 92, 11 (2020), 1219\u20131245. [68] Vladimir Rybalkin and Norbert Wehn. 2020. When massive GPU parallelism ain\u2019t enough: A novel hardware architec- ture of 2D-LSTM neural network. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. [69] Vladimir Rybalkin, Norbert Wehn, Mohammad Reza Yousefi, and Didier Stricker. 2017. Hardware architecture of bidi- rectional long short-term memory neural network for optical character recognition. In Proceedings of the Conference on Design, Automation & Test in Europe. 1394\u20131399. [70] Runbin Shi, Junjie Liu, K.-H. Hayden So, Shuo Wang, and Yun Liang. 2019. E-LSTM: Efficient inference of sparse LSTM on embedded heterogeneous system. In Proceedings of the 56th ACM/IEEE Design Automation Conference (DAC\u201919). IEEE, 1\u20136. [71] Gil Shomron, Tal Horowitz, and Uri Weiser. 2019. SMT-SA: Simultaneous multithreading in systolic arrays. IEEE Comput. Arch. Lett. 18, 2 (2019), 99\u2013102. [72] Franyell Silfa, Jose Maria Arnau, and Antonio Gonzalez. 2020. E-BATCH: Energy-efficient and high-throughput RNN batching. ACM Transactions on Architecture and Code Optimization (TACO) 19, 1 (2020), 1\u201323. ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022. [73] Burton J. Smith. 1986. A pipelined, shared resource MIMD computer. In Advanced Computer Architecture. 39\u201341. [74] Yuxi Sun, Akram Ben Ahmed, and Hideharu Amano. 2019. Acceleration of deep recurrent neural networks with an FPGA cluster. In Proceedings of the 10th International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies. 1\u20134. [75] Zhanrui Sun, Yongxin Zhu, Yu Zheng, Hao Wu, Zihao Cao, Peng Xiong, Junjie Hou, Tian Huang, and Zhiqiang Que. 2018. FPGA acceleration of LSTM based on data for test flight. In Proceedings of the IEEE International Conference on Smart Cloud (SmartCloud\u201918). IEEE, 1\u20136. [76] Tian Tan, Eriko Nurvitadhi, David Shih, and Derek Chiou. 2018. Evaluating the highly-pipelined intel stratix 10 FPGA architecture using open-source benchmarks. In Proceedings of the International Conference on Field-Programmable Technology (FPT\u201918). IEEE, 206\u2013213. [77] James E. Thornton. 1964. Parallel operation in the control data 6600. In Proceedings of the Fall Joint Computer Confer- ence, Part II: Very High Speed Computer Systems. 33\u201340. [78] Stylianos I. Venieris and Christos-Savvas Bouganis. 2018. f-CNNx: A toolflow for mapping multiple convolutional neural networks on FPGAs. In Proceedings of the 28th International Conference on Field Programmable Logic and Applications (FPL\u201918). IEEE, 381\u20133817. [79] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3156\u20133164. [80] Shuo Wang, Zhe Li, Caiwen Ding, Bo Yuan, Qinru Qiu, Yanzhi Wang, and Yun Liang. 2018. C-LSTM: Enabling efficient LSTM using structured compression techniques on FPGAs. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 11\u201320. [81] Zhisheng Wang, Jun Lin, and Zhongfeng Wang. 2017. Accelerating recurrent neural networks: A memory-efficient approach. IEEE Trans. VLSI Syst. 25, 10 (2017), 2763\u20132775. [82] Zhao Wang, Guangyu Sun, Jingchen Zhu, Zhe Zhou, Yijiang Guo, and Zhihang Yuan. 2021. METRO: A software- hardware co-design of interconnections for spatial DNN accelerators. arXiv:2108.10570 [cs.AR]. Retrieved from https: //arxiv.org/abs/2108.10570. [83] Jiaquan Wu, Feiteng Li, Zhijian Chen, and Xiaoyan Xiang. 2019. A 3.89-GOPS/mW scalable recurrent neural network processor with improved efficiency on memory and computation. IEEE Trans. VLSI Syst. 27, 12 (2019), 2939\u20132943. [84] Xilinx. 2017. Deep Learning with INT8 Optimization on Xilinx Devices. Retrieved from https://www.xilinx.com/ support/documentation/whitepapers/wp486-deep-learning-int8.pdf. [85] Krishna Praveen Yalamarthy et al. 2019. Low-complexity distributed-arithmetic-based pipelined architecture for an LSTM network. IEEE Trans. VLSI Syst. 28, 2 (2019), 329\u2013338. [86] Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria Arnau, and Antonio Gonz\u00e1lez. 2019. LSTM- sharp: An adaptable, energy-efficient hardware accelerator for long short-term memory. arXiv:1911.01258. Retrieved from https://arxiv.org/abs/1911.01258. [87] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, and George Toderici. 2015. Beyond short snippets: Deep networks for video classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4694\u20134702. [88] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv:1409.2329. Retrieved from https://arxiv.org/abs/1409.2329. [89] Tian Zhao, Yaqi Zhang, and Kunle Olukotun. 2019. Serving recurrent neural networks efficiently with a spatial accel- erator. Proc. Mach. Learn. Syst. 1 (2019), 166\u2013177. [90] Yong Zheng, Haigang Yang, Yiping Jia, and Zhihong Huang. 2021. PermLSTM: A high energy-efficiency LSTM accel- erator architecture. Electronics 10, 8 (2021), 882. Received 2 September 2021; revised 18 February 2022; accepted 1 May 2022 ACM Transactions on Reconfigurable Technology and Systems, Vol. 16, No. 1, Article 4. Publication date: December 2022.",
    "sections_mapping": {
        "background": [
            "background and preliminaries"
        ],
        "methodology": [
            "design and optimization methodology",
            "hardware architecture"
        ],
        "evaluation": [
            "experimental results and analysis"
        ],
        "related work": [
            "related work"
        ],
        "conclusion": [
            "conclusion"
        ],
        "abstract": [
            "abstract"
        ],
        "introduction": [
            "introduction"
        ],
        "references": [
            "references"
        ]
    }
}